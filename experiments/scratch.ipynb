{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5388fd59-e4e9-458f-bed7-2a9e6e042147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellitaire.game.game import Game\n",
    "from cellitaire.game.card import Card\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import itertools\n",
    "import time\n",
    "%matplotlib inline\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb3e319-e5f1-46a4-92a3-6b276d0da043",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cd062e1-7c84-406c-8c86-0b484fa76d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoardState(board):\n",
    "    return torch.tensor([[slot.card.card_id if slot.card != None else 0 for slot in row] for row in board.slots], device=device)\n",
    "\n",
    "def getStockpileState(stockpile):\n",
    "    top_card = stockpile.top_card()\n",
    "    return torch.concat(\n",
    "        (torch.tensor([top_card.card_id if top_card != None else 0], dtype=torch.float, device=device),\n",
    "        torch.tensor([stockpile.count()], dtype=torch.float, device=device))\n",
    "    )\n",
    "\n",
    "def getFoundationState(foundation):\n",
    "    return torch.tensor([Card.RANKS.index(card.rank) + 1 if card != None else 0 for suit, card in foundation.foundation.items()], dtype=torch.float, device=device)\n",
    "\n",
    "def getFlattenedState(game):\n",
    "    board_state = getBoardState(game.board)\n",
    "    stockpile_state = getStockpileState(game.stockpile)\n",
    "    foundation_state = getFoundationState(game.foundation)\n",
    "    return torch.cat((board_state.view(1, -1), stockpile_state.view(1, -1), foundation_state.view(1, -1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4da539dd-95e2-462c-8632-0b19fa8344b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols):\n",
    "        self.card_embedding_layer = torch.nn.Embedding(num_cards, card_emb_dim, device=device)\n",
    "        self.board_cnn_1 = torch.nn.Conv2d(in_channels=card_emb_dim, out_channels=cnn_1_out_channels, kernel_size=3, stride=1, padding=1, device=device)\n",
    "        self.hidden_layer1 = torch.nn.Linear(cnn_1_out_channels * board_rows * board_cols + 6, hidden_size, device=device)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.output_layer = torch.nn.Linear(hidden_size, board_rows * board_cols, device=device)\n",
    "        self.board_rows = board_rows\n",
    "        self.board_cols = board_cols\n",
    "\n",
    "    '''\n",
    "    \n",
    "    def __call__(self, game):\n",
    "        board_state = getBoardState(game.board)\n",
    "        stockpile_state = getStockpileState(game.stockpile)\n",
    "        foundation_state = getFoundationState(game.foundation)\n",
    "\n",
    "        board_state_emb = self.card_embedding_layer(board_state)\n",
    "        board_state_emb = board_state_emb.permute(2, 0, 1)\n",
    "        \n",
    "        cnn_1_otpt = self.board_cnn_1(board_state_emb)\n",
    "\n",
    "        all_inputs = torch.cat((foundation_state.view(1, 4), stockpile_state.view(1, 2), cnn_1_otpt.view(1, -1)), dim=1)\n",
    "\n",
    "        x = self.hidden_layer1(all_inputs)\n",
    "        x = self.relu1(x)\n",
    "        x = self.output_layer(x)\n",
    "        self.out = x\n",
    "        return x\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __call__(self, flat_input):\n",
    "        \"\"\"\n",
    "        Expects flat_input to be a tensor of shape \n",
    "          (batch_size, board_rows * board_cols + 6),\n",
    "        where the first board_rows * board_cols values are the board card IDs,\n",
    "        and the remaining 6 values are the concatenated foundation (4) and stockpile (2) features.\n",
    "        \"\"\"\n",
    "        batch_size = flat_input.shape[0]\n",
    "        \n",
    "        # Extract and reshape the board part (make sure they are of type long for the embedding)\n",
    "        board_flat = flat_input[:, :self.board_rows * self.board_cols].long()\n",
    "        board_state = board_flat.view(batch_size, self.board_rows, self.board_cols)\n",
    "        \n",
    "        # The extra features (foundation and stockpile) remain as floats.\n",
    "        extra_features = flat_input[:, self.board_rows * self.board_cols:]\n",
    "        \n",
    "        # Process the board state: embed and run through CNN\n",
    "        board_state_emb = self.card_embedding_layer(board_state)  # (batch_size, board_rows, board_cols, card_emb_dim)\n",
    "        board_state_emb = board_state_emb.permute(0, 3, 1, 2)        # (batch_size, card_emb_dim, board_rows, board_cols)\n",
    "        cnn_1_otpt = self.board_cnn_1(board_state_emb)               # (batch_size, cnn_1_out_channels, board_rows, board_cols)\n",
    "        cnn_1_otpt_flat = cnn_1_otpt.reshape(batch_size, -1)            # (batch_size, cnn_1_out_channels * board_rows * board_cols)\n",
    "        \n",
    "        # Concatenate extra features with the CNN output\n",
    "        all_inputs = torch.cat((extra_features, cnn_1_otpt_flat), dim=1)\n",
    "        \n",
    "        # Forward pass through the rest of the network\n",
    "        x = self.hidden_layer1(all_inputs)\n",
    "        x = self.relu1(x)\n",
    "        x = self.output_layer(x)\n",
    "        self.out = x\n",
    "        return x\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        return itertools.chain(\n",
    "            self.card_embedding_layer.parameters(),\n",
    "            self.board_cnn_1.parameters(),\n",
    "            self.hidden_layer1.parameters(),\n",
    "            self.output_layer.parameters()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "728a9a7d-6a03-4b94-8a8c-a978feb8a5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 12, 10])\n",
      "torch.Size([1, 10, 7, 12])\n",
      "torch.Size([1, 3, 7, 12])\n"
     ]
    }
   ],
   "source": [
    "m = Model(card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols)\n",
    "\n",
    "g = Game()\n",
    "g.new_game(board_rows, board_cols, num_reserved)\n",
    "fs = getFlattenedState(g)\n",
    "fs.shape\n",
    "otpt = m(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75e2c39e-7855-475f-aba3-0d1696da1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "31e9d534-a050-4d92-9fe2-68712368305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingEnvironment:\n",
    "    def __init__(self, card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols, num_reserved, replay_buffer_size,\n",
    "                num_episodes, batch_size, gamma, learning_rate, epsilon_start, epsilon_final, epsilon_decay, max_moves_per_episode):\n",
    "        self.game = Game()\n",
    "        self.prev_foundation_count = 0\n",
    "\n",
    "        self.board_rows = board_rows\n",
    "        self.board_cols = board_cols\n",
    "        self.num_reserved = num_reserved\n",
    "\n",
    "        self.model = Model(card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols)\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "        self.calculate_score_by_foundation_count()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.num_episodes = num_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_final = epsilon_final\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.max_moves_per_episode = max_moves_per_episode\n",
    "\n",
    "        self.time_track_map = {}\n",
    "\n",
    "        self.initialize_time_metrics('getting_moves')\n",
    "        self.initialize_time_metrics('getting_action')\n",
    "        self.initialize_time_metrics('stepping')\n",
    "        self.initialize_time_metrics('replay')\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.game.new_game(self.board_rows, self.board_cols, self.num_reserved)\n",
    "\n",
    "        self.prev_foundation_count = self.game.foundation.total_cards()\n",
    "\n",
    "    def initialize_time_metrics(self, tag):\n",
    "        self.time_track_map[tag + '_sum'] = 0\n",
    "        self.time_track_map[tag + '_count'] = 0\n",
    "    \n",
    "    def print_time_metrics(self, tag):\n",
    "        if self.time_track_map[tag + '_count'] > 0:\n",
    "            print(\n",
    "                tag + '_average:', self.time_track_map[tag + '_sum'] / self.time_track_map[tag + '_count'],\n",
    "                tag + '_time:', self.time_track_map[tag + '_sum'])\n",
    "\n",
    "    def add_time(self, tag, time):\n",
    "        self.time_track_map[tag + '_count'] += 1\n",
    "        self.time_track_map[tag + '_sum'] += time\n",
    "\n",
    "    def train(self):\n",
    "        frame_idx = 0\n",
    "\n",
    "        for i in range(self.num_episodes):\n",
    "            frame_idx = self.run_episode(frame_idx)\n",
    "            print('------')\n",
    "            self.print_time_metrics('getting_moves')\n",
    "            self.print_time_metrics('getting_action')\n",
    "            self.print_time_metrics('stepping')\n",
    "            self.print_time_metrics('replay')\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(len(self.losses))\n",
    "\n",
    "    def run_episode(self, frame_idx):\n",
    "        self.reset_episode()\n",
    "        done = False\n",
    "\n",
    "        move_count = 0\n",
    "        while not done:\n",
    "            frame_idx += 1\n",
    "            epsilon = self.epsilon_by_frame(frame_idx)\n",
    "\n",
    "            get_moves_start = time.time()\n",
    "            special_coords = self.game.get_possible_lonely_suffocated_coords()\n",
    "            placeable_coords = self.game.get_possible_placeable_coords()\n",
    "            legal_moves = list(set(special_coords + placeable_coords))\n",
    "            get_moves_end = time.time()\n",
    "            self.add_time('getting_moves', get_moves_end - get_moves_start)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(legal_moves)\n",
    "            else:\n",
    "                get_action_start = time.time()\n",
    "                q = self.model(getFlattenedState(self.game))\n",
    "                action = self.get_action(q, legal_moves)\n",
    "                get_action_end = time.time()\n",
    "                self.add_time('getting_action', get_action_end - get_action_start)\n",
    "\n",
    "            step_start = time.time()\n",
    "            current_state, next_state, done, reward = self.step(action)\n",
    "            step_end = time.time()\n",
    "            self.add_time('stepping', step_end - step_start)\n",
    "            \n",
    "            self.replay_buffer.push(current_state, action, reward, next_state, done)\n",
    "            if len(self.replay_buffer) > self.batch_size:\n",
    "                replay_start = time.time()\n",
    "                self.replay()\n",
    "                replay_end = time.time()\n",
    "                self.add_time('replay', replay_end - replay_start)\n",
    "            move_count += 1\n",
    "            if move_count >= self.max_moves_per_episode:\n",
    "                break\n",
    "        return frame_idx\n",
    "    '''\n",
    "\n",
    "    def replay(self):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.int, device=self.device)\n",
    "\n",
    "        current_qs = torch.tensor([], device=self.device)\n",
    "        next_qs = torch.tensor([], device=self.device)\n",
    "        for state in states:\n",
    "            current_qs = torch.cat((current_qs, self.model(state)))\n",
    "        with torch.no_grad():\n",
    "            for state in next_states:\n",
    "                next_qs = torch.cat((next_qs, self.model(state)))\n",
    "            next_qs_max, _ = next_qs.max(dim=1)\n",
    "            target = rewards_tensor + self.gamma * next_qs_max * (1 - dones_tensor)\n",
    "        loss = nn.MSELoss()(current_qs, target.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "    '''\n",
    "\n",
    "    def replay(self):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.int, device=self.device)\n",
    "    \n",
    "        # Assuming states and next_states can be converted to tensors:\n",
    "        # If needed, preprocess or convert state objects to tensors here.\n",
    "        state_batch = torch.stack(states)       # Now shape is (batch_size, ...)\n",
    "        next_state_batch = torch.stack(next_states)\n",
    "    \n",
    "        # Get current Q-values for all states in one forward pass.\n",
    "        current_qs = self.model(state_batch)\n",
    "        \n",
    "        # Get next Q-values in one forward pass.\n",
    "        with torch.no_grad():\n",
    "            next_qs = self.model(next_state_batch)\n",
    "            next_qs_max, _ = next_qs.max(dim=1)\n",
    "            target = rewards_tensor + self.gamma * next_qs_max * (1 - dones_tensor)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_qs, target.unsqueeze(1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        current_state = getFlattenedState(self.game)\n",
    "        self.game.make_move(action)\n",
    "        done, reward = self.compute_step_components()\n",
    "        next_state = getFlattenedState(self.game)\n",
    "        return current_state, next_state, done, reward\n",
    "\n",
    "    def get_action(self, network_output, legal_moves):\n",
    "        legal_moves_as_otpt_idx = [r * self.board_cols + c for r, c in legal_moves]\n",
    "        moves_with_mask = torch.full_like(network_output, -1e9)\n",
    "        for idx in legal_moves_as_otpt_idx:\n",
    "            moves_with_mask[0, idx] = 0\n",
    "        with torch.no_grad():\n",
    "            moves_with_mask = moves_with_mask + network_output\n",
    "        move_idx = torch.max(moves_with_mask, 1).indices[0]\n",
    "        move_row = move_idx // self.board_cols\n",
    "        move_col = move_idx % self.board_cols\n",
    "        return (move_row.item(), move_col.item())\n",
    "            \n",
    "    def compute_step_components(self):\n",
    "        done = not self.game.possible_moves_remaining()\n",
    "\n",
    "        current_foundation_count = self.game.foundation.total_cards()\n",
    "\n",
    "        if done:\n",
    "            self.prev_foundation_count = current_foundation_count\n",
    "            return done, self.get_score_from_foundation_count(current_foundation_count)\n",
    "\n",
    "        reward = 0\n",
    "        if current_foundation_count > self.prev_foundation_count:\n",
    "            reward = 0.1\n",
    "\n",
    "        self.prev_foundation_count = current_foundation_count\n",
    "\n",
    "        return done, reward\n",
    "\n",
    "    def epsilon_by_frame(self, frame_idx):\n",
    "        return self.epsilon_final + (self.epsilon_start - epsilon_final) * math.exp(-1.0 * frame_idx / epsilon_decay)\n",
    "\n",
    "    def get_score_from_foundation_count(self, foundation_count):\n",
    "        if foundation_count < 40:\n",
    "            return -1\n",
    "        return self.foundation_count_to_score[foundation_count]\n",
    "\n",
    "    def calculate_score_by_foundation_count(self):\n",
    "        self.foundation_count_to_score = {foundation_count: ((foundation_count - 40.0) / 12.0) ** (math.log(0.5) / math.log(5.0 / 6.0)) for foundation_count in range(40, 53)}\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d719a9fa-7028-44e3-91d9-6382428e877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingEnvironment:\n",
    "    def __init__(self, card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols, num_reserved, replay_buffer_size,\n",
    "                 num_episodes, batch_size, gamma, learning_rate, epsilon_start, epsilon_final, epsilon_decay, max_moves_per_episode):\n",
    "        self.game = Game()\n",
    "        self.prev_foundation_count = 0\n",
    "\n",
    "        self.board_rows = board_rows\n",
    "        self.board_cols = board_cols\n",
    "        self.num_reserved = num_reserved\n",
    "\n",
    "        self.model = Model(card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols)\n",
    "        self.replay_buffer = ReplayBuffer(replay_buffer_size)\n",
    "        self.calculate_score_by_foundation_count()\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.num_episodes = num_episodes\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_final = epsilon_final\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.max_moves_per_episode = max_moves_per_episode\n",
    "\n",
    "        self.time_track_map = {}\n",
    "        self.initialize_time_metrics('getting_moves')\n",
    "        self.initialize_time_metrics('getting_action')\n",
    "        self.initialize_time_metrics('stepping')\n",
    "        self.initialize_time_metrics('replay')\n",
    "\n",
    "        self.losses = []\n",
    "        self.cards_saved = []\n",
    "        self.illegal_move_count = 0\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def reset_episode(self):\n",
    "        self.game.new_game(self.board_rows, self.board_cols, self.num_reserved)\n",
    "        self.prev_foundation_count = self.game.foundation.total_cards()\n",
    "\n",
    "    def initialize_time_metrics(self, tag):\n",
    "        self.time_track_map[tag + '_sum'] = 0\n",
    "        self.time_track_map[tag + '_count'] = 0\n",
    "    \n",
    "    def print_time_metrics(self, tag):\n",
    "        if self.time_track_map[tag + '_count'] > 0:\n",
    "            print(\n",
    "                tag + '_average:', self.time_track_map[tag + '_sum'] / self.time_track_map[tag + '_count'],\n",
    "                tag + '_time:', self.time_track_map[tag + '_sum'])\n",
    "\n",
    "    def add_time(self, tag, t):\n",
    "        self.time_track_map[tag + '_count'] += 1\n",
    "        self.time_track_map[tag + '_sum'] += t\n",
    "\n",
    "    def print_all_time_metrics(self):\n",
    "        print('------')\n",
    "        self.print_time_metrics('getting_moves')\n",
    "        self.print_time_metrics('getting_action')\n",
    "        self.print_time_metrics('stepping')\n",
    "        self.print_time_metrics('replay')\n",
    "\n",
    "    def train(self):\n",
    "        frame_idx = 0\n",
    "        for i in range(self.num_episodes):\n",
    "            frame_idx = self.run_episode(frame_idx)\n",
    "            if i % 100 == 0 and len(self.losses) > 0:\n",
    "                print(\"average loss:\", sum(self.losses[-100:]) / len(self.losses[-100:]))\n",
    "                print(\"average cards saved:\", sum(self.cards_saved[-100:]) / len(self.cards_saved[-100:]))\n",
    "                print(\"max cards saved:\", max(self.cards_saved[-100:]))\n",
    "                print(\"percentage of games ended in illegal move:\", (self.illegal_move_count / (i + 1)) * 100)\n",
    "\n",
    "    def run_episode(self, frame_idx):\n",
    "        self.reset_episode()\n",
    "        done = False\n",
    "        move_count = 0\n",
    "        while not done:\n",
    "            frame_idx += 1\n",
    "            epsilon = self.epsilon_by_frame(frame_idx)\n",
    "\n",
    "            get_moves_start = time.time()\n",
    "            special_coords = self.game.get_possible_lonely_suffocated_coords()\n",
    "            placeable_coords = self.game.get_possible_placeable_coords()\n",
    "            legal_moves = list(set(special_coords + placeable_coords))\n",
    "            get_moves_end = time.time()\n",
    "            self.add_time('getting_moves', get_moves_end - get_moves_start)\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = random.choice(legal_moves)\n",
    "            else:\n",
    "                get_action_start = time.time()\n",
    "                # Use the flattened state representation here\n",
    "                state_flat = getFlattenedState(self.game)\n",
    "                q = self.model(state_flat)\n",
    "                action = self.get_action(q, legal_moves)\n",
    "                get_action_end = time.time()\n",
    "                self.add_time('getting_action', get_action_end - get_action_start)\n",
    "\n",
    "            step_start = time.time()\n",
    "            current_state, next_state, done, reward = self.step(action)\n",
    "            step_end = time.time()\n",
    "            self.add_time('stepping', step_end - step_start)\n",
    "            \n",
    "            # Push the flattened state tensors into the replay buffer\n",
    "            self.replay_buffer.push(current_state, action, reward, next_state, done)\n",
    "            \n",
    "            if len(self.replay_buffer) > self.batch_size:\n",
    "                replay_start = time.time()\n",
    "                self.replay()\n",
    "                replay_end = time.time()\n",
    "                self.add_time('replay', replay_end - replay_start)\n",
    "                \n",
    "            move_count += 1\n",
    "            if move_count >= self.max_moves_per_episode:\n",
    "                break\n",
    "        self.cards_saved.append(self.game.foundation.total_cards())\n",
    "        return frame_idx\n",
    "\n",
    "    def replay(self):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        rewards_tensor = torch.tensor(rewards, device=self.device)\n",
    "        dones_tensor = torch.tensor(dones, dtype=torch.int, device=self.device)\n",
    "        \n",
    "        # Each state is already flattened (shape: (1, flat_dim)); stack them into a batch\n",
    "        state_batch = torch.cat(states, dim=0)\n",
    "        next_state_batch = torch.cat(next_states, dim=0)\n",
    "        \n",
    "        current_qs = self.model(state_batch)  # shape: [batch_size, num_actions]\n",
    "        actions_tensor = torch.tensor(actions, device=self.device).unsqueeze(1)  # shape: [batch_size, 1]\n",
    "        current_q_for_actions = current_qs.gather(1, actions_tensor).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_qs = self.model(next_state_batch)\n",
    "            next_qs_max, _ = next_qs.max(dim=1)\n",
    "            target = rewards_tensor + self.gamma * next_qs_max * (1 - dones_tensor)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q_for_actions, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.losses.append(loss.item())\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get the current flattened state before taking the action\n",
    "        current_state = getFlattenedState(self.game)\n",
    "        move_made = self.game.make_move(action)\n",
    "        done, reward = self.compute_step_components(move_made)\n",
    "        # Get the next flattened state after the move\n",
    "        next_state = getFlattenedState(self.game)\n",
    "        return current_state, next_state, done, reward\n",
    "\n",
    "    def get_action(self, network_output, legal_moves):\n",
    "        #legal_moves_as_otpt_idx = [r * self.board_cols + c for r, c in legal_moves]\n",
    "        #moves_with_mask = torch.full_like(network_output, -1e9)\n",
    "        #for idx in legal_moves_as_otpt_idx:\n",
    "        #    moves_with_mask[0, idx] = 0\n",
    "        #with torch.no_grad():\n",
    "        #    moves_with_mask = moves_with_mask + network_output\n",
    "        #move_idx = torch.max(moves_with_mask, 1).indices[0]\n",
    "        move_idx = torch.max(network_output, 1).indices[0]\n",
    "        move_row = move_idx // self.board_cols\n",
    "        move_col = move_idx % self.board_cols\n",
    "        return (move_row.item(), move_col.item())\n",
    "            \n",
    "    def compute_step_components(self, move_made):\n",
    "        if not move_made:\n",
    "            self.illegal_move_count += 1\n",
    "            done = True\n",
    "            reward = -1\n",
    "            return done, reward\n",
    "        done = not self.game.possible_moves_remaining()\n",
    "        current_foundation_count = self.game.foundation.total_cards()\n",
    "        if done:\n",
    "            self.prev_foundation_count = current_foundation_count\n",
    "            return done, self.get_score_from_foundation_count(current_foundation_count)\n",
    "        reward = -0.1\n",
    "        if current_foundation_count > self.prev_foundation_count:\n",
    "            reward = 0.1\n",
    "        self.prev_foundation_count = current_foundation_count\n",
    "        return done, reward\n",
    "\n",
    "    def epsilon_by_frame(self, frame_idx):\n",
    "        return self.epsilon_final + (self.epsilon_start - self.epsilon_final) * math.exp(-1.0 * frame_idx / self.epsilon_decay)\n",
    "\n",
    "    def get_score_from_foundation_count(self, foundation_count):\n",
    "        if foundation_count < 40:\n",
    "            return -1\n",
    "        return self.foundation_count_to_score[foundation_count]\n",
    "\n",
    "    def calculate_score_by_foundation_count(self):\n",
    "        self.foundation_count_to_score = {foundation_count: ((foundation_count - 40.0) / 12.0) ** (math.log(0.5) / math.log(5.0 / 6.0)) for foundation_count in range(40, 53)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "51e08b8e-4bd0-4467-b8ef-223f4e4a2445",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m max_moves_per_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m     18\u001b[0m env \u001b[38;5;241m=\u001b[39m TestingEnvironment(card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols, num_reserved, replay_buffer_size, num_episodes, batch_size, gamma, learning_rate, epsilon_start, epsilon_final, epsilon_decay, max_moves_per_episode)\n\u001b[1;32m---> 19\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[144], line 65\u001b[0m, in \u001b[0;36mTestingEnvironment.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     63\u001b[0m frame_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_episodes):\n\u001b[1;32m---> 65\u001b[0m     frame_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m:]))\n",
      "Cell \u001b[1;32mIn[144], line 108\u001b[0m, in \u001b[0;36mTestingEnvironment.run_episode\u001b[1;34m(self, frame_idx)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m    107\u001b[0m     replay_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     replay_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_time(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplay\u001b[39m\u001b[38;5;124m'\u001b[39m, replay_end \u001b[38;5;241m-\u001b[39m replay_start)\n",
      "Cell \u001b[1;32mIn[144], line 129\u001b[0m, in \u001b[0;36mTestingEnvironment.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m current_qs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(state_batch)  \u001b[38;5;66;03m# shape: [batch_size, num_actions]\u001b[39;00m\n\u001b[0;32m    128\u001b[0m actions_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(actions, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape: [batch_size, 1]\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m current_q_for_actions \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent_qs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    132\u001b[0m     next_qs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(next_state_batch)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "board_rows = 7\n",
    "board_cols = 12\n",
    "num_reserved = 6\n",
    "num_cards = 53\n",
    "card_emb_dim = 30\n",
    "cnn_1_out_channels = 12\n",
    "hidden_size = 30\n",
    "replay_buffer_size = 10000\n",
    "num_episodes = 10000       \n",
    "batch_size = 1000        \n",
    "gamma = 0.99              \n",
    "learning_rate = 1e-1\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.1\n",
    "epsilon_decay = 50000\n",
    "max_moves_per_episode = 300\n",
    "\n",
    "env = TestingEnvironment(card_emb_dim, cnn_1_out_channels, device, hidden_size, board_rows, board_cols, num_reserved, replay_buffer_size, num_episodes, batch_size, gamma, learning_rate, epsilon_start, epsilon_final, epsilon_decay, max_moves_per_episode)\n",
    "env.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c3dc14fa-b076-4ae3-ba61-8fa001942b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e906d7d1c0>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(env.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a9f0788d-121d-401d-be0e-70ff5a3f96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumPossibleMoves(game):\n",
    "    special_coords = game.get_possible_lonely_suffocated_coords()\n",
    "    placeable_coords = game.get_possible_placeable_coords()\n",
    "    return len(set(special_coords + placeable_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e4d2d660-7c6f-4cdd-8e6e-066a7ee08b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0\n",
      "20 0\n",
      "30 0\n",
      "32 0\n"
     ]
    }
   ],
   "source": [
    "test_game = Game()\n",
    "test_game.new_game(board_rows, board_cols, num_reserved)\n",
    "\n",
    "num_moves = 0\n",
    "while getNumPossibleMoves(test_game) > 0:\n",
    "    network_output = env.model(getFlattenedState(test_game))\n",
    "    special_coords = test_game.get_possible_lonely_suffocated_coords()\n",
    "    placeable_coords = test_game.get_possible_placeable_coords()\n",
    "    legal_moves = list(set(special_coords + placeable_coords))\n",
    "    legal_moves_as_otpt_idx = [r * board_cols + c for r, c in legal_moves]\n",
    "    moves_with_mask = torch.full_like(network_output, -1e9)\n",
    "    for idx in legal_moves_as_otpt_idx:\n",
    "        moves_with_mask[0, idx] = 0\n",
    "    with torch.no_grad():\n",
    "        moves_with_mask = moves_with_mask + network_output\n",
    "    move_idx = torch.max(moves_with_mask, 1).indices[0]\n",
    "    move_row = move_idx // board_cols\n",
    "    move_col = move_idx % board_cols\n",
    "    test_game.make_move((move_row.item(), move_col.item()))\n",
    "    num_moves += 1\n",
    "    if num_moves % 10 == 0:\n",
    "        print(num_moves, test_game.foundation.total_cards())\n",
    "    if num_moves > 1000:\n",
    "        break\n",
    "\n",
    "saved = test_game.foundation.total_cards()\n",
    "print(num_moves, saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8e127-e107-4239-b6a4-117962df0799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellitaire-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
