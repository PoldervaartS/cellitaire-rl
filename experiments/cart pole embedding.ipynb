{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb804a91-6ebc-4b2c-a410-b0bb5ca11ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from cellitaire.environment.cellitaire_env import CellitaireEnv\n",
    "from cellitaire.environment.rewards.reward import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16e4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "        \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3c5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        alpha,\n",
    "        chkpt_dir='tmp/ppo', \n",
    "        num_embeddings=53, \n",
    "        embedding_dim=30, \n",
    "        embeddings_in_state=85,\n",
    "        num_hidden_layers=1,\n",
    "        hidden_dim=256\n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.embeddings_in_state = embeddings_in_state\n",
    "\n",
    "        # Create embedding layer only if embeddings_in_state > 0.\n",
    "        if embeddings_in_state > 0:\n",
    "            self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim).to(self.device)\n",
    "            input_layer_dim = input_dims[0] - embeddings_in_state + (embeddings_in_state * embedding_dim)\n",
    "        else:\n",
    "            self.embedding_layer = None\n",
    "            input_layer_dim = input_dims[0]\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_layer_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.actor.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.actor.append(nn.ReLU())\n",
    "        \n",
    "        self.actor.append(nn.Linear(hidden_dim, n_actions))\n",
    "        self.actor.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.to(self.device)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state):\n",
    "        if self.embeddings_in_state > 0:\n",
    "            embeddings = self.embedding_layer(state[:, :self.embeddings_in_state].int()).view(state.shape[0], 1, -1)\n",
    "            state = state.view(state.shape[0], 1, -1)[:, :, self.embeddings_in_state:]\n",
    "\n",
    "            \n",
    "            # Concatenate the remaining state features with the flattened embeddings.\n",
    "            new_state = torch.cat((state, embeddings), dim=2)\n",
    "        else:\n",
    "            new_state = state.view(state.shape[0], 1, -1)\n",
    "        \n",
    "        # Pass the processed state through the actor network.\n",
    "        logits = self.actor(new_state)\n",
    "        return logits\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89c790f-8870-4894-ac88-9a9cafbe684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dims, \n",
    "        alpha, \n",
    "        num_hidden_layers=1, \n",
    "        hidden_dim=256, \n",
    "        chkpt_dir='tmp/ppo'\n",
    "    ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(*input_dims, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.critic.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.critic.append(nn.ReLU())\n",
    "\n",
    "        self.critic.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ae2fc7-893a-4c88-a9eb-95d2fbfa027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        gamma=0.99, \n",
    "        alpha=0.0003, \n",
    "        gae_lambda=0.95,\n",
    "        policy_clip=0.2, \n",
    "        batch_size=64, \n",
    "        n_epochs=10,\n",
    "        num_hidden_layers_actor=1,\n",
    "        hidden_dim_actor=256,\n",
    "        num_hidden_layers_critic=1,\n",
    "        hidden_dim_critic=256,\n",
    "        embeddings_in_state_actor=85,\n",
    "        embedding_dim_actor=30\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            n_actions, \n",
    "            input_dims, \n",
    "            alpha, \n",
    "            num_hidden_layers=num_hidden_layers_actor, \n",
    "            hidden_dim=hidden_dim_actor,\n",
    "            embeddings_in_state=embeddings_in_state_actor,\n",
    "            embedding_dim=embedding_dim_actor\n",
    "        )\n",
    "        self.critic = CriticNetwork(\n",
    "            input_dims, \n",
    "            alpha,\n",
    "            num_hidden_layers=num_hidden_layers_critic,\n",
    "            hidden_dim=hidden_dim_critic\n",
    "        )\n",
    "        actor_param_count = sum(p.numel() for p in self.actor.parameters())\n",
    "        critic_param_count = sum(p.numel() for p in self.critic.parameters())\n",
    "        print(f'NUM - PARAMS {actor_param_count + critic_param_count}')\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        #print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        #print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "        dist = Categorical(dist)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        # Convert observation to tensor and send it to the actor's device.\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (batch, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # Convert legal_actions to a tensor on the same device.\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "\n",
    "        # Mask logits so that only legal actions remain.\n",
    "        # If logits has shape (n, 1, k), we index into the third dimension.\n",
    "        masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "\n",
    "        # Build a Categorical using the masked logits.\n",
    "        dist = Categorical(logits=masked_logits)\n",
    "\n",
    "        # Get the value estimate from the critic.\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        # This gives a relative index in the filtered (legal) logits.\n",
    "        relative_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map the relative index back to the original action space.\n",
    "        action = legal_actions[relative_index]\n",
    "\n",
    "        # Compute the log probability for the sampled action.\n",
    "        probs = torch.squeeze(dist.log_prob(relative_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    def choose_action(self, observation, legal_actions=None):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (n, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # If legal_actions is provided, mask the logits to include only those actions.\n",
    "        if legal_actions is not None:\n",
    "            legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "            masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "            dist = Categorical(logits=masked_logits)\n",
    "        else:\n",
    "            # Use all logits if no legal actions mask is provided.\n",
    "            full_logits = logits.squeeze(1)  # shape: (n, k)\n",
    "            dist = Categorical(logits=full_logits)\n",
    "\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        sampled_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map back to the original action if a mask was applied.\n",
    "        if legal_actions is not None:\n",
    "            action = legal_actions[sampled_index]\n",
    "        else:\n",
    "            action = sampled_index\n",
    "\n",
    "        # Get the log probability of the sampled action.\n",
    "        log_prob = torch.squeeze(dist.log_prob(sampled_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, log_prob, value\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        device = self.actor.device  # Assuming this is a CUDA device\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Retrieve batch data\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, batches = \\\n",
    "                self.memory.generate_batches()\n",
    "            \n",
    "            # Convert arrays to torch tensors on GPU\n",
    "            rewards = torch.tensor(reward_arr, dtype=torch.float32, device=device)\n",
    "            values = torch.tensor(vals_arr, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones_arr, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Compute deltas for GAE: delta_t = r_t + gamma * V(t+1) * (1-done) - V(t)\n",
    "            deltas = rewards[:-1] + self.gamma * values[1:] * (1 - dones[:-1]) - values[:-1]\n",
    "            \n",
    "            # Compute advantage vector using the vectorized discounted cumulative sum.\n",
    "            advantage = torch.zeros_like(rewards, device=device)\n",
    "            advantage[:-1] = discount_cumsum(deltas, self.gamma * self.gae_lambda)\n",
    "            \n",
    "            # Loop over minibatches\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float, device=device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch], device=device)\n",
    "                actions = torch.tensor(action_arr[batch], device=device)\n",
    "    \n",
    "                # Forward pass through the actor and critic networks\n",
    "                dist = self.actor(states)\n",
    "                dist = Categorical(dist)\n",
    "                critic_value = self.critic(states).squeeze()\n",
    "                \n",
    "                # Calculate probability ratio and losses\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1 - self.policy_clip, 1 + self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "    \n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value).pow(2).mean()\n",
    "    \n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                \n",
    "                # Update the networks\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "    \n",
    "        self.memory.clear_memory()             \n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    Compute discounted cumulative sums of vector x with discount factor.\n",
    "    For each index t, returns sum_{l=0}^{T-t-1} discount^l * x[t+l].\n",
    "    \"\"\"\n",
    "    T = x.size(0)\n",
    "    # Create a vector of discount factors\n",
    "    discount_factors = discount ** torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "    # Multiply elementwise and compute cumulative sums in reverse order\n",
    "    x_discounted = x * discount_factors\n",
    "    reversed_x = torch.flip(x_discounted, dims=[0])\n",
    "    cumsum_reversed = torch.cumsum(reversed_x, dim=0)\n",
    "    discounted_cumsum = torch.flip(cumsum_reversed, dims=[0])\n",
    "    # Divide by discount factors to get the proper values\n",
    "    return discounted_cumsum / discount_factors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67f3518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3712543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1000000, 2, 0.0001]\n"
     ]
    }
   ],
   "source": [
    "board_rows = 7\n",
    "board_cols = 12\n",
    "num_reserved = 6\n",
    "test_reward = CombinedReward([\n",
    "    PlacedCardInFoundationReward(weight=2),\n",
    "    WinReward(),\n",
    "    #ConstantReward(weight=-0.01),\n",
    "    #PlayedLegalMoveReward(weight=1),\n",
    "    PeriodicPlacedCardInFoundationReward(weight=2, reward_period=3),\n",
    "    CreatedMovesReward(weight=0.0001, num_reserved=num_reserved, foundation_count_dropoff=30)\n",
    "])\n",
    "print([reward.weight for reward in test_reward.rewards_list])\n",
    "env = CellitaireEnv(test_reward, rows=board_rows, cols=board_cols, num_reserved=num_reserved, max_moves=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e261d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM - PARAMS 111888011\n",
      "episode     1 | score    0.0 | recent avg    0.0 | avg moves   8.0 | avg saved  0.0 | learning steps      0 | done True *\n",
      "episode     2 | score   38.0 | recent avg   19.0 | avg moves 112.0 | avg saved  5.5 | learning steps      0 | done True *\n",
      "episode     9 | score  150.0 | recent avg   24.9 | avg moves 111.0 | avg saved  7.3 | learning steps      0 | done True *\n",
      "episode    58 | score  128.0 | recent avg   26.2 | avg moves 106.8 | avg saved  7.7 | learning steps      6 | done True *\n",
      "episode   127 | score   88.0 | recent avg   26.6 | avg moves 115.1 | avg saved  7.8 | learning steps     13 | done True *\n",
      "episode   128 | score    4.0 | recent avg   26.7 | avg moves 115.8 | avg saved  7.8 | learning steps     13 | done True *\n",
      "episode   182 | score   14.0 | recent avg   26.7 | avg moves 121.9 | avg saved  7.8 | learning steps     20 | done True *\n",
      "episode   223 | score  134.0 | recent avg   26.8 | avg moves 123.8 | avg saved  7.9 | learning steps     25 | done True *\n",
      "episode   229 | score  128.0 | recent avg   27.2 | avg moves 124.8 | avg saved  8.0 | learning steps     25 | done True *\n",
      "episode   234 | score    8.0 | recent avg   27.2 | avg moves 124.8 | avg saved  8.0 | learning steps     26 | done True *\n",
      "episode   260 | score  114.0 | recent avg   27.6 | avg moves 121.5 | avg saved  8.1 | learning steps     28 | done True *\n",
      "episode   268 | score   64.0 | recent avg   27.9 | avg moves 121.6 | avg saved  8.2 | learning steps     29 | done True *\n",
      "episode   269 | score   20.0 | recent avg   27.9 | avg moves 121.9 | avg saved  8.2 | learning steps     29 | done True *\n",
      "episode   271 | score  140.0 | recent avg   28.0 | avg moves 122.8 | avg saved  8.2 | learning steps     30 | done True *\n",
      "episode   371 | score    4.0 | recent avg   22.6 | avg moves 104.4 | avg saved  6.6 | learning steps     40 | done True\n",
      "episode   471 | score    4.0 | recent avg   21.9 | avg moves  98.6 | avg saved  6.4 | learning steps     50 | done True\n",
      "episode   571 | score  128.0 | recent avg   20.1 | avg moves  84.3 | avg saved  5.8 | learning steps     59 | done True\n",
      "episode   671 | score    4.0 | recent avg   23.1 | avg moves 110.5 | avg saved  6.8 | learning steps     70 | done True\n",
      "episode   771 | score    0.0 | recent avg   20.3 | avg moves  90.4 | avg saved  5.9 | learning steps     79 | done True\n",
      "episode   871 | score    4.0 | recent avg   25.2 | avg moves 112.6 | avg saved  7.4 | learning steps     90 | done True\n",
      "episode   925 | score   94.0 | recent avg   28.0 | avg moves 124.8 | avg saved  8.2 | learning steps     96 | done True *\n",
      "episode   926 | score    4.0 | recent avg   28.1 | avg moves 124.9 | avg saved  8.2 | learning steps     96 | done True *\n",
      "episode  1026 | score  104.0 | recent avg   22.5 | avg moves 102.0 | avg saved  6.6 | learning steps    106 | done True\n",
      "episode  1126 | score    0.0 | recent avg   24.6 | avg moves 113.7 | avg saved  7.2 | learning steps    118 | done True\n",
      "episode  1163 | score  100.0 | recent avg   28.3 | avg moves 130.0 | avg saved  8.3 | learning steps    123 | done True *\n",
      "episode  1164 | score   68.0 | recent avg   29.0 | avg moves 132.9 | avg saved  8.5 | learning steps    123 | done True *\n",
      "episode  1165 | score  128.0 | recent avg   29.7 | avg moves 134.5 | avg saved  8.8 | learning steps    124 | done True *\n",
      "episode  1166 | score    8.0 | recent avg   29.8 | avg moves 134.8 | avg saved  8.8 | learning steps    124 | done True *\n",
      "episode  1168 | score   10.0 | recent avg   29.8 | avg moves 134.3 | avg saved  8.8 | learning steps    124 | done True *\n",
      "episode  1169 | score   30.0 | recent avg   30.1 | avg moves 135.5 | avg saved  8.9 | learning steps    124 | done True *\n",
      "episode  1171 | score    8.0 | recent avg   30.1 | avg moves 135.5 | avg saved  8.9 | learning steps    124 | done True *\n",
      "episode  1174 | score   10.0 | recent avg   30.1 | avg moves 135.7 | avg saved  8.9 | learning steps    124 | done True *\n",
      "episode  1175 | score    4.0 | recent avg   30.2 | avg moves 135.6 | avg saved  8.9 | learning steps    124 | done True *\n",
      "episode  1176 | score    8.0 | recent avg   30.2 | avg moves 135.9 | avg saved  8.9 | learning steps    124 | done True *\n",
      "episode  1185 | score  138.0 | recent avg   30.7 | avg moves 137.3 | avg saved  9.0 | learning steps    125 | done False *\n",
      "episode  1186 | score   74.0 | recent avg   31.4 | avg moves 143.2 | avg saved  9.3 | learning steps    126 | done True *\n",
      "episode  1187 | score   64.0 | recent avg   32.1 | avg moves 146.2 | avg saved  9.4 | learning steps    126 | done True *\n",
      "episode  1189 | score   34.0 | recent avg   32.1 | avg moves 147.5 | avg saved  9.5 | learning steps    126 | done True *\n",
      "episode  1190 | score   10.0 | recent avg   32.2 | avg moves 147.3 | avg saved  9.5 | learning steps    126 | done True *\n",
      "episode  1191 | score   14.0 | recent avg   32.3 | avg moves 147.6 | avg saved  9.5 | learning steps    127 | done True *\n",
      "episode  1192 | score   44.0 | recent avg   32.3 | avg moves 148.0 | avg saved  9.5 | learning steps    127 | done True *\n",
      "episode  1193 | score  120.0 | recent avg   32.4 | avg moves 147.9 | avg saved  9.6 | learning steps    127 | done True *\n",
      "episode  1196 | score   28.0 | recent avg   32.7 | avg moves 148.0 | avg saved  9.6 | learning steps    127 | done True *\n",
      "episode  1197 | score  104.0 | recent avg   33.6 | avg moves 153.5 | avg saved  9.9 | learning steps    128 | done True *\n",
      "episode  1297 | score    0.0 | recent avg   28.9 | avg moves 119.6 | avg saved  8.5 | learning steps    140 | done True\n",
      "episode  1397 | score   18.0 | recent avg   22.5 | avg moves  99.1 | avg saved  6.6 | learning steps    150 | done True\n",
      "episode  1497 | score    4.0 | recent avg   20.2 | avg moves  95.4 | avg saved  5.9 | learning steps    159 | done True\n",
      "episode  1597 | score    4.0 | recent avg   28.6 | avg moves 122.8 | avg saved  8.4 | learning steps    171 | done True\n",
      "episode  1635 | score  114.0 | recent avg   34.3 | avg moves 142.3 | avg saved 10.1 | learning steps    177 | done False *\n",
      "episode  1636 | score  138.0 | recent avg   35.6 | avg moves 147.3 | avg saved 10.5 | learning steps    178 | done True *\n",
      "episode  1639 | score  134.0 | recent avg   36.6 | avg moves 151.7 | avg saved 10.8 | learning steps    179 | done True *\n",
      "episode  1640 | score   38.0 | recent avg   37.0 | avg moves 152.9 | avg saved 10.9 | learning steps    179 | done True *\n",
      "episode  1641 | score   74.0 | recent avg   37.6 | avg moves 155.9 | avg saved 11.1 | learning steps    179 | done True *\n",
      "episode  1741 | score    0.0 | recent avg   26.2 | avg moves 114.6 | avg saved  7.7 | learning steps    191 | done True\n",
      "episode  1841 | score   24.0 | recent avg   25.2 | avg moves 111.6 | avg saved  7.4 | learning steps    202 | done True\n",
      "episode  1941 | score   10.0 | recent avg   27.8 | avg moves 120.2 | avg saved  8.2 | learning steps    214 | done True\n",
      "episode  2041 | score    4.0 | recent avg   32.7 | avg moves 137.2 | avg saved  9.6 | learning steps    227 | done True\n",
      "episode  2141 | score    0.0 | recent avg   30.2 | avg moves 135.0 | avg saved  8.9 | learning steps    241 | done True\n",
      "episode  2241 | score   30.0 | recent avg   24.6 | avg moves 106.4 | avg saved  7.2 | learning steps    252 | done True\n",
      "episode  2341 | score    0.0 | recent avg   27.5 | avg moves 116.1 | avg saved  8.1 | learning steps    263 | done True\n",
      "episode  2441 | score    0.0 | recent avg   25.2 | avg moves 108.9 | avg saved  7.4 | learning steps    274 | done True\n",
      "episode  2541 | score    8.0 | recent avg   24.4 | avg moves 110.5 | avg saved  7.2 | learning steps    285 | done True\n",
      "episode  2641 | score   10.0 | recent avg   26.8 | avg moves 121.8 | avg saved  7.9 | learning steps    297 | done True\n",
      "episode  2741 | score    0.0 | recent avg   27.3 | avg moves 113.6 | avg saved  8.0 | learning steps    309 | done True\n",
      "episode  2841 | score   64.0 | recent avg   27.3 | avg moves 115.5 | avg saved  8.0 | learning steps    320 | done True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m done) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m truncated):\n\u001b[0;32m     51\u001b[0m     legal_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_legal_actions_as_int()\n\u001b[1;32m---> 52\u001b[0m     action, prob, val \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(observation, legal_actions)\n\u001b[0;32m     53\u001b[0m     observation_, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     54\u001b[0m     n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 117\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, observation, legal_actions)\u001b[0m\n\u001b[0;32m    114\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([observation]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Get logits from the actor; assumed shape: (n, 1, k)\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# If legal_actions is provided, mask the logits to include only those actions.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m legal_actions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 55\u001b[0m, in \u001b[0;36mActorNetwork.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     52\u001b[0m     new_state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mview(state\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Pass the processed state through the actor network.\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(new_state)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "batch_size = 1000\n",
    "n_epochs = 6\n",
    "alpha = 1e-6\n",
    "num_hidden_layers_actor=2\n",
    "hidden_dim_actor=4096\n",
    "num_hidden_layers_critic=4\n",
    "hidden_dim_critic=4096\n",
    "#embeddings_in_state_actor=1\n",
    "embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "env.reset()\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=(board_rows * board_cols + 7,), \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim,\n",
    "    num_hidden_layers_critic=num_hidden_layers_critic,\n",
    "    hidden_dim_critic=hidden_dim_critic,\n",
    ")\n",
    "\n",
    "n_games = 1\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "moves_history = []\n",
    "cards_saved_history = []\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "avg_moves = 0\n",
    "avg_cards_saved = 0\n",
    "n_steps = 0\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    observation, reward, done, truncated, info = env.reset()\n",
    "    observation = env.get_state()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while (not done) and (not truncated):\n",
    "        legal_actions = env.get_legal_actions_as_int()\n",
    "        action, prob, val = agent.choose_action(observation, legal_actions)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    moves_history.append(env.num_moves)\n",
    "    cards_saved_history.append(env.game.foundation.total_cards())\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    avg_moves = np.mean(moves_history[-100:])\n",
    "    avg_cards_saved = np.mean(cards_saved_history[-100:])\n",
    "    i += 1\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done} *') \n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e08b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(score_history, dtype=torch.float).view(-1, 100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(moves_history, dtype=torch.float).view(-1, 100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(cards_saved_history, dtype=torch.float).view(-1, 100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cards_saved_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b04beb-bd28-4c5a-987e-9c8c28b033c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00121022,  0.02562538,  0.01463803, -0.02002225], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05d40bf7-3d64-496d-b842-340670553b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode     0 | score   37.0 | avg   37.0 | std   0.00 | max score  37.0 | learning steps     3 | done True *\n",
      "episode   100 | score   10.0 | avg   22.4 | std  11.91 | max score  73.0 | learning steps   228 | done True\n",
      "episode   200 | score   31.0 | avg   21.9 | std   9.94 | max score  73.0 | learning steps   446 | done True\n",
      "episode   300 | score   49.0 | avg   23.5 | std  12.01 | max score  73.0 | learning steps   682 | done True\n",
      "episode   400 | score   17.0 | avg   26.3 | std  11.91 | max score  73.0 | learning steps   944 | done True\n",
      "episode   500 | score   23.0 | avg   27.2 | std  16.38 | max score 110.0 | learning steps  1216 | done True\n",
      "episode   600 | score   14.0 | avg   29.3 | std  15.42 | max score 110.0 | learning steps  1509 | done True\n",
      "episode   700 | score   49.0 | avg   33.6 | std  22.05 | max score 112.0 | learning steps  1845 | done True\n",
      "episode   800 | score   34.0 | avg   31.9 | std  18.61 | max score 112.0 | learning steps  2163 | done True\n",
      "episode   900 | score   31.0 | avg   31.6 | std  18.12 | max score 112.0 | learning steps  2479 | done True\n",
      "episode  1000 | score   13.0 | avg   31.0 | std  19.99 | max score 112.0 | learning steps  2789 | done True\n",
      "episode  1100 | score   24.0 | avg   30.9 | std  18.79 | max score 112.0 | learning steps  3099 | done True\n",
      "episode  1200 | score   14.0 | avg   30.4 | std  19.71 | max score 138.0 | learning steps  3402 | done True\n",
      "episode  1300 | score   35.0 | avg   33.9 | std  24.07 | max score 171.0 | learning steps  3742 | done True\n",
      "episode  1400 | score   10.0 | avg   30.4 | std  18.27 | max score 171.0 | learning steps  4045 | done True\n",
      "episode  1500 | score   15.0 | avg   34.0 | std  24.10 | max score 171.0 | learning steps  4385 | done True\n",
      "episode  1600 | score   22.0 | avg   34.9 | std  24.66 | max score 171.0 | learning steps  4734 | done True\n",
      "episode  1628 | score   36.0 | avg   37.1 | std  24.38 | max score 171.0 | learning steps  4837 | done True *\n",
      "episode  1629 | score   25.0 | avg   37.2 | std  24.28 | max score 171.0 | learning steps  4840 | done True *\n",
      "episode  1635 | score   18.0 | avg   37.2 | std  24.03 | max score 171.0 | learning steps  4856 | done True *\n",
      "episode  1636 | score   62.0 | avg   37.5 | std  24.16 | max score 171.0 | learning steps  4862 | done True *\n",
      "episode  1642 | score   30.0 | avg   37.6 | std  23.96 | max score 171.0 | learning steps  4887 | done True *\n",
      "episode  1643 | score  105.0 | avg   37.9 | std  24.57 | max score 171.0 | learning steps  4897 | done True *\n",
      "episode  1644 | score   71.0 | avg   38.4 | std  24.67 | max score 171.0 | learning steps  4904 | done True *\n",
      "episode  1645 | score   31.0 | avg   38.6 | std  24.59 | max score 171.0 | learning steps  4908 | done True *\n",
      "episode  1647 | score   54.0 | avg   38.9 | std  24.58 | max score 171.0 | learning steps  4915 | done True *\n",
      "episode  1651 | score   87.0 | avg   39.2 | std  25.06 | max score 171.0 | learning steps  4932 | done True *\n",
      "episode  1652 | score   84.0 | avg   39.9 | std  25.33 | max score 171.0 | learning steps  4940 | done True *\n",
      "episode  1653 | score   46.0 | avg   39.9 | std  25.34 | max score 171.0 | learning steps  4945 | done True *\n",
      "episode  1654 | score   49.0 | avg   40.2 | std  25.30 | max score 171.0 | learning steps  4950 | done True *\n",
      "episode  1754 | score   12.0 | avg   33.4 | std  19.30 | max score 171.0 | learning steps  5284 | done True\n",
      "episode  1854 | score   68.0 | avg   37.7 | std  23.27 | max score 171.0 | learning steps  5661 | done True\n",
      "episode  1954 | score   75.0 | avg   35.2 | std  23.74 | max score 171.0 | learning steps  6013 | done True\n",
      "episode  2054 | score   52.0 | avg   34.8 | std  23.85 | max score 171.0 | learning steps  6361 | done True\n",
      "episode  2123 | score  129.0 | avg   40.2 | std  27.57 | max score 171.0 | learning steps  6634 | done True *\n",
      "episode  2124 | score   32.0 | avg   40.4 | std  27.51 | max score 171.0 | learning steps  6637 | done True *\n",
      "episode  2125 | score   24.0 | avg   40.5 | std  27.43 | max score 171.0 | learning steps  6640 | done True *\n",
      "episode  2128 | score   96.0 | avg   40.9 | std  27.94 | max score 171.0 | learning steps  6656 | done True *\n",
      "episode  2129 | score   48.0 | avg   41.2 | std  27.85 | max score 171.0 | learning steps  6660 | done True *\n",
      "episode  2130 | score   22.0 | avg   41.3 | std  27.76 | max score 171.0 | learning steps  6663 | done True *\n",
      "episode  2131 | score   29.0 | avg   41.4 | std  27.68 | max score 171.0 | learning steps  6666 | done True *\n",
      "episode  2231 | score   21.0 | avg   36.5 | std  22.33 | max score 171.0 | learning steps  7031 | done True\n",
      "episode  2331 | score   23.0 | avg   37.4 | std  27.37 | max score 199.0 | learning steps  7405 | done True\n",
      "episode  2431 | score   52.0 | avg   37.4 | std  24.58 | max score 199.0 | learning steps  7778 | done True\n",
      "episode  2531 | score   17.0 | avg   37.8 | std  28.68 | max score 199.0 | learning steps  8156 | done True\n",
      "episode  2568 | score   29.0 | avg   41.4 | std  28.89 | max score 199.0 | learning steps  8313 | done True *\n",
      "episode  2569 | score   70.0 | avg   41.8 | std  29.01 | max score 199.0 | learning steps  8320 | done True *\n",
      "episode  2570 | score   65.0 | avg   42.3 | std  29.00 | max score 199.0 | learning steps  8327 | done True *\n",
      "episode  2571 | score   21.0 | avg   42.4 | std  28.93 | max score 199.0 | learning steps  8329 | done True *\n",
      "episode  2611 | score   59.0 | avg   42.4 | std  27.90 | max score 199.0 | learning steps  8487 | done True *\n",
      "episode  2614 | score   28.0 | avg   42.4 | std  27.84 | max score 199.0 | learning steps  8496 | done True *\n",
      "episode  2615 | score   61.0 | avg   42.7 | std  27.87 | max score 199.0 | learning steps  8502 | done True *\n",
      "episode  2715 | score   12.0 | avg   34.5 | std  22.78 | max score 199.0 | learning steps  8847 | done True\n",
      "episode  2815 | score   21.0 | avg   36.3 | std  23.36 | max score 199.0 | learning steps  9210 | done True\n",
      "episode  2915 | score   89.0 | avg   36.8 | std  32.41 | max score 205.0 | learning steps  9578 | done True\n",
      "episode  3015 | score   18.0 | avg   38.5 | std  24.88 | max score 205.0 | learning steps  9962 | done True\n",
      "episode  3115 | score   30.0 | avg   39.4 | std  26.31 | max score 205.0 | learning steps 10356 | done True\n",
      "episode  3215 | score   12.0 | avg   35.8 | std  25.60 | max score 205.0 | learning steps 10715 | done True\n",
      "episode  3315 | score   54.0 | avg   40.9 | std  29.30 | max score 205.0 | learning steps 11123 | done True\n",
      "episode  3415 | score   46.0 | avg   38.9 | std  33.44 | max score 229.0 | learning steps 11512 | done True\n",
      "episode  3452 | score  129.0 | avg   43.3 | std  29.68 | max score 229.0 | learning steps 11686 | done True *\n",
      "episode  3453 | score  128.0 | avg   44.0 | std  30.83 | max score 229.0 | learning steps 11699 | done True *\n",
      "episode  3454 | score   57.0 | avg   44.4 | std  30.76 | max score 229.0 | learning steps 11704 | done True *\n",
      "episode  3455 | score   27.0 | avg   44.5 | std  30.67 | max score 229.0 | learning steps 11707 | done True *\n",
      "episode  3456 | score   74.0 | avg   45.0 | std  30.71 | max score 229.0 | learning steps 11714 | done True *\n",
      "episode  3463 | score   64.0 | avg   45.3 | std  30.50 | max score 229.0 | learning steps 11736 | done True *\n",
      "episode  3494 | score   62.0 | avg   45.4 | std  27.19 | max score 229.0 | learning steps 11863 | done True *\n",
      "episode  3495 | score   32.0 | avg   45.5 | std  27.14 | max score 229.0 | learning steps 11867 | done True *\n",
      "episode  3496 | score   22.0 | avg   45.5 | std  27.13 | max score 229.0 | learning steps 11869 | done True *\n",
      "episode  3497 | score   85.0 | avg   46.1 | std  27.31 | max score 229.0 | learning steps 11877 | done True *\n",
      "episode  3498 | score   77.0 | avg   46.6 | std  27.43 | max score 229.0 | learning steps 11885 | done True *\n",
      "episode  3505 | score  197.0 | avg   46.8 | std  31.56 | max score 229.0 | learning steps 11920 | done True *\n",
      "episode  3537 | score  182.0 | avg   46.9 | std  32.78 | max score 229.0 | learning steps 12068 | done True *\n",
      "episode  3538 | score   75.0 | avg   47.1 | std  32.89 | max score 229.0 | learning steps 12075 | done True *\n",
      "episode  3638 | score   22.0 | avg   39.5 | std  27.84 | max score 229.0 | learning steps 12470 | done True\n",
      "episode  3738 | score  114.0 | avg   40.2 | std  25.55 | max score 229.0 | learning steps 12873 | done True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(observation, action, prob, val, reward, done)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn()\n\u001b[0;32m     69\u001b[0m     learn_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     70\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation_\n",
      "Cell \u001b[1;32mIn[5], line 161\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    159\u001b[0m values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(values)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[1;32m--> 161\u001b[0m     states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state_arr[batch], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    162\u001b[0m     old_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(old_prob_arr[batch])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    163\u001b[0m     actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(action_arr[batch])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(observation, action, prob, val, reward, done)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn()\n\u001b[0;32m     69\u001b[0m     learn_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     70\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation_\n",
      "Cell \u001b[1;32mIn[84], line 186\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 186\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#N = 20\n",
    "#batch_size = 5\n",
    "#n_epochs = 4\n",
    "#alpha = 0.0003\n",
    "#embeddings_in_state_actor=1\n",
    "\n",
    "N = 10\n",
    "batch_size = 3\n",
    "n_epochs = 2\n",
    "alpha = 0.0003\n",
    "num_hidden_layers_actor=3\n",
    "hidden_dim_actor=2048\n",
    "embeddings_in_state_actor=1\n",
    "#embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "'''\n",
    "agent = Agent(\n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor\n",
    ")\n",
    "'''\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim\n",
    ")\n",
    "\n",
    "n_games = 150000\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "max_score = 0\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    (observation, _) = env.reset()\n",
    "    observation[0] += 5.0\n",
    "    observation[0] *= 5.2\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    while not done and not truncated:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_[0] += 5.0\n",
    "        observation_[0] *= 5.2\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    max_score = max(max_score, score)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done} *')\n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done}')\n",
    "    \n",
    "\n",
    "plt.plot(x, score_history)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "603fba1d-212d-4624-985d-82473afbdf4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612c7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///P:/repos/cellitaire-rlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cellitaire==0.0.1) (1.26.2)\n",
      "Building wheels for collected packages: cellitaire\n",
      "  Building editable for cellitaire (pyproject.toml): started\n",
      "  Building editable for cellitaire (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for cellitaire: filename=cellitaire-0.0.1-0.editable-py3-none-any.whl size=1308 sha256=b378391a3c216160d1234ca202eae929e697782b029366f7620acdb700ddbf9d\n",
      "  Stored in directory: C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x9pvpc0w\\wheels\\94\\28\\43\\7b210dfe894f4cf7a94090468ee7dfeadc8e241120461ab4b9\n",
      "Successfully built cellitaire\n",
      "Installing collected packages: cellitaire\n",
      "Successfully installed cellitaire-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Noe\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b132a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (0.28.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [86 lines of output]\n",
      "  \n",
      "  \n",
      "  WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
      "  Using WINDOWS configuration...\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m2\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mexec\u001b[0m\u001b[1;31m(compile('''\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;31m# This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "      ...<31 lines>...\n",
      "      \u001b[1;31mexec(compile(setup_py_code, filename, \"exec\"))\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;31m''' % ('C:\\\\Users\\\\Noe\\\\AppData\\\\Local\\\\Temp\\\\pip-install-eoyw7vpr\\\\pygame_967dc80c3fe74667b8820019c1335dad\\\\setup.py',), \"<pip-setuptools-caller>\", \"exec\"))\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"<pip-setuptools-caller>\"\u001b[0m, line \u001b[35m34\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\setup.py\"\u001b[0m, line \u001b[35m400\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mbuildconfig.config.main\u001b[0m\u001b[1;31m(AUTO_CONFIG)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config.py\"\u001b[0m, line \u001b[35m231\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      deps = CFG.main(**kwds, auto_config=auto)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m493\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      return setup_prebuilt_sdl2(prebuilt_dir)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m453\u001b[0m, in \u001b[35msetup_prebuilt_sdl2\u001b[0m\n",
      "      \u001b[31mDEPS.configure\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m336\u001b[0m, in \u001b[35mconfigure\u001b[0m\n",
      "      from . import vstools\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\vstools.py\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mcompiler.initialize\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\"\u001b[0m, line \u001b[35m400\u001b[0m, in \u001b[35minitialize\u001b[0m\n",
      "      vc_env = query_vcvarsall(VERSION, plat_spec)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\"\u001b[0m, line \u001b[35m280\u001b[0m, in \u001b[35mquery_vcvarsall\u001b[0m\n",
      "      raise DistutilsPlatformError(\"Unable to find vcvarsall.bat\")\n",
      "  \u001b[1;35mdistutils.errors.DistutilsPlatformError\u001b[0m: \u001b[35mUnable to find vcvarsall.bat\u001b[0m\n",
      "  Making dir :prebuilt_downloads:\n",
      "  Downloading... https://www.libsdl.org/release/SDL2-devel-2.0.22-VC.zip efa040633c4faf8b006c0c1e552456ca4e5a3a53\n",
      "  Unzipping :prebuilt_downloads\\SDL2-devel-2.0.22-VC.zip:\n",
      "  Downloading... https://www.libsdl.org/projects/SDL_image/release/SDL2_image-devel-2.0.5-VC.zip 137f86474691f4e12e76e07d58d5920c8d844d5b\n",
      "  Unzipping :prebuilt_downloads\\SDL2_image-devel-2.0.5-VC.zip:\n",
      "  Downloading... https://github.com/libsdl-org/SDL_ttf/releases/download/release-2.20.1/SDL2_ttf-devel-2.20.1-VC.zip 371606aceba450384428fd2852f73d2f6290b136\n",
      "  Unzipping :prebuilt_downloads\\SDL2_ttf-devel-2.20.1-VC.zip:\n",
      "  Downloading... https://github.com/libsdl-org/SDL_mixer/releases/download/release-2.6.2/SDL2_mixer-devel-2.6.2-VC.zip 000e3ea8a50261d46dbd200fb450b93c59ed4482\n",
      "  Unzipping :prebuilt_downloads\\SDL2_mixer-devel-2.6.2-VC.zip:\n",
      "  Downloading... https://github.com/pygame/pygame/releases/download/2.1.3.dev4/prebuilt-x64-pygame-2.1.4-20220319.zip 16b46596744ce9ef80e7e40fa72ddbafef1cf586\n",
      "  Unzipping :prebuilt_downloads\\prebuilt-x64-pygame-2.1.4-20220319.zip:\n",
      "  copying into .\\prebuilt-x64\n",
      "  Path for SDL: prebuilt-x64\\SDL2-2.0.22\n",
      "  ...Library directory for SDL: prebuilt-x64/SDL2-2.0.22/lib/x64\n",
      "  ...Include directory for SDL: prebuilt-x64/SDL2-2.0.22/include\n",
      "  Path for FONT: prebuilt-x64\\SDL2_ttf-2.20.1\n",
      "  ...Library directory for FONT: prebuilt-x64/SDL2_ttf-2.20.1/lib/x64\n",
      "  ...Include directory for FONT: prebuilt-x64/SDL2_ttf-2.20.1/include\n",
      "  Path for IMAGE: prebuilt-x64\\SDL2_image-2.0.5\n",
      "  ...Library directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/lib/x64\n",
      "  ...Include directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/include\n",
      "  Path for MIXER: prebuilt-x64\\SDL2_mixer-2.6.2\n",
      "  ...Library directory for MIXER: prebuilt-x64/SDL2_mixer-2.6.2/lib/x64\n",
      "  ...Include directory for MIXER: prebuilt-x64/SDL2_mixer-2.6.2/include\n",
      "  Path for PORTMIDI: prebuilt-x64\n",
      "  ...Library directory for PORTMIDI: prebuilt-x64/lib\n",
      "  ...Include directory for PORTMIDI: prebuilt-x64/include\n",
      "  DLL for SDL2: prebuilt-x64/SDL2-2.0.22/lib/x64/SDL2.dll\n",
      "  DLL for SDL2_ttf: prebuilt-x64/SDL2_ttf-2.20.1/lib/x64/SDL2_ttf.dll\n",
      "  DLL for SDL2_image: prebuilt-x64/SDL2_image-2.0.5/lib/x64/SDL2_image.dll\n",
      "  DLL for SDL2_mixer: prebuilt-x64/SDL2_mixer-2.6.2/lib/x64/SDL2_mixer.dll\n",
      "  DLL for portmidi: prebuilt-x64/lib/portmidi.dll\n",
      "  Path for FREETYPE: prebuilt-x64\n",
      "  ...Library directory for FREETYPE: prebuilt-x64/lib\n",
      "  ...Include directory for FREETYPE: prebuilt-x64/include\n",
      "  Path for PNG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  Path for JPEG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  DLL for freetype: prebuilt-x64/lib/freetype.dll\n",
      "  DLL for png: prebuilt-x64/SDL2_image-2.0.5/lib/x64/libpng16-16.dll\n",
      "  \n",
      "  ---\n",
      "  For help with compilation see:\n",
      "      https://www.pygame.org/wiki/CompileWindows\n",
      "  To contribute to pygame development see:\n",
      "      https://www.pygame.org/contribute.html\n",
      "  ---\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (2.2.2)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Collecting pygame==2.1.3 (from gymnasium[classic-control])\n",
      "  Using cached pygame-2.1.3.tar.gz (12.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce4d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 42.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 37.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b18d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%jupyter` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd2b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
