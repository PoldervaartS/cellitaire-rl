{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb804a91-6ebc-4b2c-a410-b0bb5ca11ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from cellitaire.environment.cellitaire_env import CellitaireEnv\n",
    "from cellitaire.environment.rewards.reward import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "980a32a0-d5c3-4e1c-94f2-8434f51c9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "        \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7260d2b0-e733-4f74-81b8-69c8c3a50fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        alpha,\n",
    "        chkpt_dir='tmp/ppo', \n",
    "        num_embeddings=53, \n",
    "        embedding_dim=30, \n",
    "        embeddings_in_state=85,\n",
    "        num_hidden_layers=1,\n",
    "        hidden_dim=256\n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim).to(self.device)\n",
    "        self.embeddings_in_state = embeddings_in_state\n",
    "        \n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dims[0] - self.embeddings_in_state + (self.embeddings_in_state * embedding_dim), hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.actor.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.actor.append(nn.ReLU())\n",
    "        \n",
    "        self.actor.append(nn.Linear(hidden_dim, n_actions))\n",
    "        self.actor.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.to(self.device)\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state):\n",
    "        embeddings = self.embedding_layer(state[:, :self.embeddings_in_state].int()).view(state.shape[0], 1, -1)\n",
    "        state = state.view(state.shape[0], 1, -1)[:, :, self.embeddings_in_state:]\n",
    "        state = torch.concatenate((state, embeddings), dim=2)\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        return dist\n",
    "    \n",
    "    '''\n",
    "    def forward_with_mask(self, state, legal_actions):\n",
    "        embeddings = self.embedding_layer(state[:, :self.embeddings_in_state].int()).view(state.shape[0], 1, -1)\n",
    "        state = state.view(state.shape[0], 1, -1)[:, :, self.embeddings_in_state:]\n",
    "        state = torch.concatenate((state, embeddings), dim=2)\n",
    "        dist = self.actor(state)\n",
    "        legal_actions_mask = torch.full((state.shape[0], 1, self.n_actions), -1e9)\n",
    "        for action in legal_actions:\n",
    "            legal_actions_mask[0, 0, action] = 1\n",
    "        print(legal_actions_mask.shape)\n",
    "        print(dist.shape)\n",
    "        dist = dist * legal_actions_mask\n",
    "        dist = Categorical(dist)\n",
    "        \n",
    "        return dist\n",
    "    '''\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89c790f-8870-4894-ac88-9a9cafbe684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dims, \n",
    "        alpha, \n",
    "        fc1_dims=256, \n",
    "        fc2_dims=256, \n",
    "        chkpt_dir='tmp/ppo'\n",
    "    ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(*input_dims, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6338dcc-9c50-4744-8c64-cc5da74e5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        gamma=0.99, \n",
    "        alpha=0.0003, \n",
    "        gae_lambda=0.95,\n",
    "        policy_clip=0.2, \n",
    "        batch_size=64, \n",
    "        n_epochs=10,\n",
    "        num_hidden_layers_actor=1,\n",
    "        hidden_dim_actor=256\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            n_actions, \n",
    "            input_dims, \n",
    "            alpha, \n",
    "            num_hidden_layers=num_hidden_layers_actor, \n",
    "            hidden_dim=hidden_dim_actor\n",
    "        )\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = torch.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor.device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = torch.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = torch.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c67f3518",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reward = CombinedReward([\n",
    "    PlacedCardInFoundationReward(weight=2),\n",
    "    WinReward(),\n",
    "    ConstantReward(weight=-0.1),\n",
    "    PlayedLegalMoveReward(weight=1),\n",
    "    PeriodicPlacedCardInFoundationReward(weight=2, reward_period=3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dabe2019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1000000, -0.1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "print([reward.weight for reward in test_reward.rewards_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3712543",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CellitaireEnv(test_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e261d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving models ...\n",
      "episode     0 score  -10.8 recent avg  -10.8 legal moves  17 cards saved  1\n",
      "episode     1 score  -13.9 recent avg  -12.3 legal moves  18 cards saved  0\n",
      "episode     2 score  -16.6 recent avg  -13.8 legal moves  15 cards saved  0\n",
      "episode     3 score   -5.7 recent avg  -11.7 legal moves  16 cards saved  3\n",
      "episode     4 score  -16.2 recent avg  -12.6 legal moves  11 cards saved  1\n",
      "episode     5 score   -8.0 recent avg  -11.9 legal moves  12 cards saved  0\n",
      "episode     6 score   -5.9 recent avg  -11.0 legal moves  18 cards saved  2\n",
      "... saving models ...\n",
      "episode     7 score   -0.0 recent avg   -9.6 legal moves   6 cards saved  0\n",
      "... saving models ...\n",
      "episode     8 score   -1.1 recent avg   -8.7 legal moves  12 cards saved  0\n",
      "episode     9 score  -12.8 recent avg   -9.1 legal moves  14 cards saved  0\n",
      "... saving models ...\n",
      "episode    10 score   -0.5 recent avg   -8.3 legal moves  24 cards saved  2\n",
      "episode    11 score  -15.6 recent avg   -8.9 legal moves   6 cards saved  0\n",
      "episode    12 score  -18.0 recent avg   -9.6 legal moves  10 cards saved  0\n",
      "episode    13 score  -17.1 recent avg  -10.2 legal moves  10 cards saved  1\n",
      "episode    14 score  -19.3 recent avg  -10.8 legal moves  12 cards saved  0\n",
      "episode    15 score  -14.8 recent avg  -11.0 legal moves  17 cards saved  0\n",
      "episode    16 score   -6.3 recent avg  -10.7 legal moves  22 cards saved  1\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "batch_size = 3\n",
    "n_epochs = 5\n",
    "alpha = 0.0003\n",
    "num_hidden_layers_actor=1\n",
    "hidden_dim_actor=256\n",
    "agent = Agent(\n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=(7 * 12 + 6,), \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor\n",
    ")\n",
    "\n",
    "n_games = 15000\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    observation, reward, done, truncated, info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while (not done) and (not truncated):\n",
    "        #legal_actions = env.get_legal_actions_as_int()\n",
    "        #print(legal_actions)\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "    \n",
    "    print(f'episode {i:>5} score {score:>6.1f} recent avg {avg_score:>6.1f} legal moves {env.num_moves:>3} cards saved {env.game.foundation.total_cards():>2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e08b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(score_history, dtype=torch.float).view(-1, 100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7b04beb-bd28-4c5a-987e-9c8c28b033c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05d40bf7-3d64-496d-b842-340670553b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noe\\AppData\\Local\\Temp\\ipykernel_9188\\690304986.py:27: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\Users\\dev-admin\\Desktop\\c_full\\libtorch_1738968566392\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  state = torch.tensor([observation], dtype=torch.float).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving models ...\n",
      "episode 0 score 19.0 avg score 19.0 time_steps 19 learning_steps 0\n",
      "episode 1 score 19.0 avg score 19.0 time_steps 38 learning_steps 1\n",
      "episode 2 score 19.0 avg score 19.0 time_steps 57 learning_steps 2\n",
      "... saving models ...\n",
      "episode 3 score 44.0 avg score 25.2 time_steps 101 learning_steps 5\n",
      "... saving models ...\n",
      "episode 4 score 38.0 avg score 27.8 time_steps 139 learning_steps 6\n",
      "episode 5 score 21.0 avg score 26.7 time_steps 160 learning_steps 8\n",
      "episode 6 score 14.0 avg score 24.9 time_steps 174 learning_steps 8\n",
      "episode 7 score 12.0 avg score 23.2 time_steps 186 learning_steps 9\n",
      "episode 8 score 10.0 avg score 21.8 time_steps 196 learning_steps 9\n",
      "episode 9 score 15.0 avg score 21.1 time_steps 211 learning_steps 10\n",
      "episode 10 score 25.0 avg score 21.5 time_steps 236 learning_steps 11\n",
      "episode 11 score 15.0 avg score 20.9 time_steps 251 learning_steps 12\n",
      "episode 12 score 18.0 avg score 20.7 time_steps 269 learning_steps 13\n",
      "episode 13 score 21.0 avg score 20.7 time_steps 290 learning_steps 14\n",
      "episode 14 score 18.0 avg score 20.5 time_steps 308 learning_steps 15\n",
      "episode 15 score 30.0 avg score 21.1 time_steps 338 learning_steps 16\n",
      "episode 16 score 16.0 avg score 20.8 time_steps 354 learning_steps 17\n",
      "episode 17 score 15.0 avg score 20.5 time_steps 369 learning_steps 18\n",
      "episode 18 score 14.0 avg score 20.2 time_steps 383 learning_steps 19\n",
      "episode 19 score 10.0 avg score 19.6 time_steps 393 learning_steps 19\n",
      "episode 20 score 12.0 avg score 19.3 time_steps 405 learning_steps 20\n",
      "episode 21 score 22.0 avg score 19.4 time_steps 427 learning_steps 21\n",
      "episode 22 score 22.0 avg score 19.5 time_steps 449 learning_steps 22\n",
      "episode 23 score 54.0 avg score 21.0 time_steps 503 learning_steps 25\n",
      "episode 24 score 37.0 avg score 21.6 time_steps 540 learning_steps 27\n",
      "episode 25 score 36.0 avg score 22.2 time_steps 576 learning_steps 28\n",
      "episode 26 score 33.0 avg score 22.6 time_steps 609 learning_steps 30\n",
      "episode 27 score 34.0 avg score 23.0 time_steps 643 learning_steps 32\n",
      "episode 28 score 31.0 avg score 23.2 time_steps 674 learning_steps 33\n",
      "episode 29 score 26.0 avg score 23.3 time_steps 700 learning_steps 35\n",
      "episode 30 score 26.0 avg score 23.4 time_steps 726 learning_steps 36\n",
      "episode 31 score 21.0 avg score 23.3 time_steps 747 learning_steps 37\n",
      "episode 32 score 25.0 avg score 23.4 time_steps 772 learning_steps 38\n",
      "episode 33 score 23.0 avg score 23.4 time_steps 795 learning_steps 39\n",
      "episode 34 score 37.0 avg score 23.8 time_steps 832 learning_steps 41\n",
      "episode 35 score 26.0 avg score 23.8 time_steps 858 learning_steps 42\n",
      "episode 36 score 13.0 avg score 23.5 time_steps 871 learning_steps 43\n",
      "episode 37 score 79.0 avg score 25.0 time_steps 950 learning_steps 47\n",
      "episode 38 score 74.0 avg score 26.3 time_steps 1024 learning_steps 51\n",
      "... saving models ...\n",
      "episode 39 score 100.0 avg score 28.1 time_steps 1124 learning_steps 56\n",
      "episode 40 score 17.0 avg score 27.8 time_steps 1141 learning_steps 57\n",
      "... saving models ...\n",
      "episode 41 score 43.0 avg score 28.2 time_steps 1184 learning_steps 59\n",
      "... saving models ...\n",
      "episode 42 score 56.0 avg score 28.8 time_steps 1240 learning_steps 62\n",
      "... saving models ...\n",
      "episode 43 score 140.0 avg score 31.4 time_steps 1380 learning_steps 69\n",
      "... saving models ...\n",
      "episode 44 score 78.0 avg score 32.4 time_steps 1458 learning_steps 72\n",
      "... saving models ...\n",
      "episode 45 score 35.0 avg score 32.5 time_steps 1493 learning_steps 74\n",
      "... saving models ...\n",
      "episode 46 score 64.0 avg score 33.1 time_steps 1557 learning_steps 77\n",
      "... saving models ...\n",
      "episode 47 score 208.0 avg score 36.8 time_steps 1765 learning_steps 88\n",
      "... saving models ...\n",
      "episode 48 score 157.0 avg score 39.2 time_steps 1922 learning_steps 96\n",
      "... saving models ...\n",
      "episode 49 score 58.0 avg score 39.6 time_steps 1980 learning_steps 99\n",
      "... saving models ...\n",
      "episode 50 score 65.0 avg score 40.1 time_steps 2045 learning_steps 102\n",
      "episode 51 score 14.0 avg score 39.6 time_steps 2059 learning_steps 102\n",
      "episode 52 score 50.0 avg score 39.8 time_steps 2109 learning_steps 105\n",
      "... saving models ...\n",
      "episode 53 score 196.0 avg score 42.7 time_steps 2305 learning_steps 115\n",
      "... saving models ...\n",
      "episode 54 score 49.0 avg score 42.8 time_steps 2354 learning_steps 117\n",
      "episode 55 score 22.0 avg score 42.4 time_steps 2376 learning_steps 118\n",
      "... saving models ...\n",
      "episode 56 score 115.0 avg score 43.7 time_steps 2491 learning_steps 124\n",
      "... saving models ...\n",
      "episode 57 score 67.0 avg score 44.1 time_steps 2558 learning_steps 127\n",
      "... saving models ...\n",
      "episode 58 score 71.0 avg score 44.6 time_steps 2629 learning_steps 131\n",
      "... saving models ...\n",
      "episode 59 score 75.0 avg score 45.1 time_steps 2704 learning_steps 135\n",
      "... saving models ...\n",
      "episode 60 score 111.0 avg score 46.1 time_steps 2815 learning_steps 140\n",
      "... saving models ...\n",
      "episode 61 score 60.0 avg score 46.4 time_steps 2875 learning_steps 143\n",
      "... saving models ...\n",
      "episode 62 score 122.0 avg score 47.6 time_steps 2997 learning_steps 149\n",
      "episode 63 score 44.0 avg score 47.5 time_steps 3041 learning_steps 152\n",
      "episode 64 score 25.0 avg score 47.2 time_steps 3066 learning_steps 153\n",
      "... saving models ...\n",
      "episode 65 score 117.0 avg score 48.2 time_steps 3183 learning_steps 159\n",
      "... saving models ...\n",
      "episode 66 score 291.0 avg score 51.9 time_steps 3474 learning_steps 173\n",
      "... saving models ...\n",
      "episode 67 score 106.0 avg score 52.6 time_steps 3580 learning_steps 179\n",
      "... saving models ...\n",
      "episode 68 score 86.0 avg score 53.1 time_steps 3666 learning_steps 183\n",
      "... saving models ...\n",
      "episode 69 score 287.0 avg score 56.5 time_steps 3953 learning_steps 197\n",
      "... saving models ...\n",
      "episode 70 score 323.0 avg score 60.2 time_steps 4276 learning_steps 213\n",
      "... saving models ...\n",
      "episode 71 score 265.0 avg score 63.1 time_steps 4541 learning_steps 227\n",
      "... saving models ...\n",
      "episode 72 score 71.0 avg score 63.2 time_steps 4612 learning_steps 230\n",
      "... saving models ...\n",
      "episode 73 score 209.0 avg score 65.1 time_steps 4821 learning_steps 241\n",
      "... saving models ...\n",
      "episode 74 score 172.0 avg score 66.6 time_steps 4993 learning_steps 249\n",
      "... saving models ...\n",
      "episode 75 score 118.0 avg score 67.2 time_steps 5111 learning_steps 255\n",
      "... saving models ...\n",
      "episode 76 score 120.0 avg score 67.9 time_steps 5231 learning_steps 261\n",
      "... saving models ...\n",
      "episode 77 score 98.0 avg score 68.3 time_steps 5329 learning_steps 266\n",
      "... saving models ...\n",
      "episode 78 score 146.0 avg score 69.3 time_steps 5475 learning_steps 273\n",
      "... saving models ...\n",
      "episode 79 score 155.0 avg score 70.4 time_steps 5630 learning_steps 281\n",
      "... saving models ...\n",
      "episode 80 score 100.0 avg score 70.7 time_steps 5730 learning_steps 286\n",
      "... saving models ...\n",
      "episode 81 score 74.0 avg score 70.8 time_steps 5804 learning_steps 290\n",
      "... saving models ...\n",
      "episode 82 score 71.0 avg score 70.8 time_steps 5875 learning_steps 293\n",
      "... saving models ...\n",
      "episode 83 score 96.0 avg score 71.1 time_steps 5971 learning_steps 298\n",
      "episode 84 score 52.0 avg score 70.9 time_steps 6023 learning_steps 301\n",
      "... saving models ...\n",
      "episode 85 score 156.0 avg score 71.8 time_steps 6179 learning_steps 308\n",
      "... saving models ...\n",
      "episode 86 score 82.0 avg score 72.0 time_steps 6261 learning_steps 313\n",
      "... saving models ...\n",
      "episode 87 score 98.0 avg score 72.3 time_steps 6359 learning_steps 317\n",
      "... saving models ...\n",
      "episode 88 score 83.0 avg score 72.4 time_steps 6442 learning_steps 322\n",
      "episode 89 score 43.0 avg score 72.1 time_steps 6485 learning_steps 324\n",
      "episode 90 score 84.0 avg score 72.2 time_steps 6569 learning_steps 328\n",
      "episode 91 score 61.0 avg score 72.1 time_steps 6630 learning_steps 331\n",
      "episode 92 score 58.0 avg score 71.9 time_steps 6688 learning_steps 334\n",
      "... saving models ...\n",
      "episode 93 score 125.0 avg score 72.5 time_steps 6813 learning_steps 340\n",
      "... saving models ...\n",
      "episode 94 score 113.0 avg score 72.9 time_steps 6926 learning_steps 346\n",
      "... saving models ...\n",
      "episode 95 score 82.0 avg score 73.0 time_steps 7008 learning_steps 350\n",
      "... saving models ...\n",
      "episode 96 score 74.0 avg score 73.0 time_steps 7082 learning_steps 354\n",
      "... saving models ...\n",
      "episode 97 score 221.0 avg score 74.5 time_steps 7303 learning_steps 365\n",
      "... saving models ...\n",
      "episode 98 score 79.0 avg score 74.6 time_steps 7382 learning_steps 369\n",
      "... saving models ...\n",
      "episode 99 score 134.0 avg score 75.2 time_steps 7516 learning_steps 375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving models ...\n",
      "episode 100 score 135.0 avg score 76.3 time_steps 7651 learning_steps 382\n",
      "... saving models ...\n",
      "episode 101 score 77.0 avg score 76.9 time_steps 7728 learning_steps 386\n",
      "... saving models ...\n",
      "episode 102 score 104.0 avg score 77.8 time_steps 7832 learning_steps 391\n",
      "... saving models ...\n",
      "episode 103 score 162.0 avg score 78.9 time_steps 7994 learning_steps 399\n",
      "... saving models ...\n",
      "episode 104 score 142.0 avg score 80.0 time_steps 8136 learning_steps 406\n",
      "... saving models ...\n",
      "episode 105 score 171.0 avg score 81.5 time_steps 8307 learning_steps 415\n",
      "... saving models ...\n",
      "episode 106 score 238.0 avg score 83.7 time_steps 8545 learning_steps 427\n",
      "... saving models ...\n",
      "episode 107 score 161.0 avg score 85.2 time_steps 8706 learning_steps 435\n",
      "... saving models ...\n",
      "episode 108 score 142.0 avg score 86.5 time_steps 8848 learning_steps 442\n",
      "... saving models ...\n",
      "episode 109 score 730.0 avg score 93.7 time_steps 9578 learning_steps 478\n",
      "... saving models ...\n",
      "episode 110 score 220.0 avg score 95.6 time_steps 9798 learning_steps 489\n",
      "... saving models ...\n",
      "episode 111 score 165.0 avg score 97.1 time_steps 9963 learning_steps 498\n",
      "... saving models ...\n",
      "episode 112 score 434.0 avg score 101.3 time_steps 10397 learning_steps 519\n",
      "... saving models ...\n",
      "episode 113 score 383.0 avg score 104.9 time_steps 10780 learning_steps 539\n",
      "... saving models ...\n",
      "episode 114 score 990.0 avg score 114.6 time_steps 11770 learning_steps 588\n",
      "... saving models ...\n",
      "episode 115 score 346.0 avg score 117.8 time_steps 12116 learning_steps 605\n",
      "... saving models ...\n",
      "episode 116 score 432.0 avg score 121.9 time_steps 12548 learning_steps 627\n",
      "... saving models ...\n",
      "episode 117 score 293.0 avg score 124.7 time_steps 12841 learning_steps 642\n",
      "... saving models ...\n",
      "episode 118 score 655.0 avg score 131.1 time_steps 13496 learning_steps 674\n",
      "... saving models ...\n",
      "episode 119 score 1845.0 avg score 149.5 time_steps 15341 learning_steps 767\n",
      "... saving models ...\n",
      "episode 120 score 175.0 avg score 151.1 time_steps 15516 learning_steps 775\n",
      "... saving models ...\n",
      "episode 121 score 94.0 avg score 151.8 time_steps 15610 learning_steps 780\n",
      "... saving models ...\n",
      "episode 122 score 130.0 avg score 152.9 time_steps 15740 learning_steps 787\n",
      "... saving models ...\n",
      "episode 123 score 152.0 avg score 153.9 time_steps 15892 learning_steps 794\n",
      "... saving models ...\n",
      "episode 124 score 1727.0 avg score 170.8 time_steps 17619 learning_steps 880\n",
      "... saving models ...\n",
      "episode 125 score 1496.0 avg score 185.4 time_steps 19115 learning_steps 955\n",
      "... saving models ...\n",
      "episode 126 score 360.0 avg score 188.7 time_steps 19475 learning_steps 973\n",
      "... saving models ...\n",
      "episode 127 score 897.0 avg score 197.3 time_steps 20372 learning_steps 1018\n",
      "... saving models ...\n",
      "episode 128 score 187.0 avg score 198.8 time_steps 20559 learning_steps 1027\n",
      "... saving models ...\n",
      "episode 129 score 276.0 avg score 201.3 time_steps 20835 learning_steps 1041\n",
      "... saving models ...\n",
      "episode 130 score 1083.0 avg score 211.9 time_steps 21918 learning_steps 1095\n",
      "... saving models ...\n",
      "episode 131 score 255.0 avg score 214.3 time_steps 22173 learning_steps 1108\n",
      "... saving models ...\n",
      "episode 132 score 718.0 avg score 221.2 time_steps 22891 learning_steps 1144\n",
      "... saving models ...\n",
      "episode 133 score 760.0 avg score 228.6 time_steps 23651 learning_steps 1182\n",
      "... saving models ...\n",
      "episode 134 score 2176.0 avg score 249.9 time_steps 25827 learning_steps 1291\n",
      "... saving models ...\n",
      "episode 135 score 443.0 avg score 254.1 time_steps 26270 learning_steps 1313\n",
      "... saving models ...\n",
      "episode 136 score 404.0 avg score 258.0 time_steps 26674 learning_steps 1333\n",
      "... saving models ...\n",
      "episode 137 score 220.0 avg score 259.4 time_steps 26894 learning_steps 1344\n",
      "... saving models ...\n",
      "episode 138 score 228.0 avg score 261.0 time_steps 27122 learning_steps 1356\n",
      "... saving models ...\n",
      "episode 139 score 150.0 avg score 261.5 time_steps 27272 learning_steps 1363\n",
      "... saving models ...\n",
      "episode 140 score 180.0 avg score 263.1 time_steps 27452 learning_steps 1372\n",
      "... saving models ...\n",
      "episode 141 score 193.0 avg score 264.6 time_steps 27645 learning_steps 1382\n",
      "... saving models ...\n",
      "episode 142 score 24446.0 avg score 508.5 time_steps 52091 learning_steps 2604\n",
      "... saving models ...\n",
      "episode 143 score 283.0 avg score 509.9 time_steps 52374 learning_steps 2618\n",
      "... saving models ...\n",
      "episode 144 score 363.0 avg score 512.8 time_steps 52737 learning_steps 2636\n",
      "... saving models ...\n",
      "episode 145 score 395.0 avg score 516.4 time_steps 53132 learning_steps 2656\n",
      "... saving models ...\n",
      "episode 146 score 297.0 avg score 518.7 time_steps 53429 learning_steps 2671\n",
      "... saving models ...\n",
      "episode 147 score 305.0 avg score 519.7 time_steps 53734 learning_steps 2686\n",
      "... saving models ...\n",
      "episode 148 score 266.0 avg score 520.8 time_steps 54000 learning_steps 2700\n",
      "... saving models ...\n",
      "episode 149 score 276.0 avg score 523.0 time_steps 54276 learning_steps 2713\n",
      "... saving models ...\n",
      "episode 150 score 293.0 avg score 525.2 time_steps 54569 learning_steps 2728\n",
      "... saving models ...\n",
      "episode 151 score 276.0 avg score 527.9 time_steps 54845 learning_steps 2742\n",
      "... saving models ...\n",
      "episode 152 score 323.0 avg score 530.6 time_steps 55168 learning_steps 2758\n",
      "... saving models ...\n",
      "episode 153 score 374.0 avg score 532.4 time_steps 55542 learning_steps 2777\n",
      "... saving models ...\n",
      "episode 154 score 380.0 avg score 535.7 time_steps 55922 learning_steps 2796\n",
      "... saving models ...\n",
      "episode 155 score 503.0 avg score 540.5 time_steps 56425 learning_steps 2821\n",
      "... saving models ...\n",
      "episode 156 score 569.0 avg score 545.0 time_steps 56994 learning_steps 2849\n",
      "... saving models ...\n",
      "episode 157 score 1888.0 avg score 563.2 time_steps 58882 learning_steps 2944\n",
      "... saving models ...\n",
      "episode 158 score 1650.0 avg score 579.0 time_steps 60532 learning_steps 3026\n",
      "... saving models ...\n",
      "episode 159 score 638.0 avg score 584.7 time_steps 61170 learning_steps 3058\n",
      "... saving models ...\n",
      "episode 160 score 580.0 avg score 589.4 time_steps 61750 learning_steps 3087\n",
      "... saving models ...\n",
      "episode 161 score 1111.0 avg score 599.9 time_steps 62861 learning_steps 3143\n",
      "... saving models ...\n",
      "episode 162 score 1089.0 avg score 609.5 time_steps 63950 learning_steps 3197\n",
      "... saving models ...\n",
      "episode 163 score 830.0 avg score 617.4 time_steps 64780 learning_steps 3239\n",
      "... saving models ...\n",
      "episode 164 score 860.0 avg score 625.7 time_steps 65640 learning_steps 3282\n",
      "episode 165 score 45.0 avg score 625.0 time_steps 65685 learning_steps 3284\n",
      "episode 166 score 20.0 avg score 622.3 time_steps 65705 learning_steps 3285\n",
      "episode 167 score 22.0 avg score 621.5 time_steps 65727 learning_steps 3286\n",
      "episode 168 score 21.0 avg score 620.8 time_steps 65748 learning_steps 3287\n",
      "... saving models ...\n",
      "episode 169 score 2174.0 avg score 639.7 time_steps 67922 learning_steps 3396\n",
      "... saving models ...\n",
      "episode 170 score 421.0 avg score 640.7 time_steps 68343 learning_steps 3417\n",
      "... saving models ...\n",
      "episode 171 score 381.0 avg score 641.8 time_steps 68724 learning_steps 3436\n",
      "... saving models ...\n",
      "episode 172 score 393.0 avg score 645.0 time_steps 69117 learning_steps 3455\n",
      "... saving models ...\n",
      "episode 173 score 419.0 avg score 647.1 time_steps 69536 learning_steps 3476\n",
      "... saving models ...\n",
      "episode 174 score 769.0 avg score 653.1 time_steps 70305 learning_steps 3515\n",
      "... saving models ...\n",
      "episode 175 score 845.0 avg score 660.4 time_steps 71150 learning_steps 3557\n",
      "... saving models ...\n",
      "episode 176 score 20405.0 avg score 863.2 time_steps 91555 learning_steps 4577\n",
      "episode 177 score 80.0 avg score 863.1 time_steps 91635 learning_steps 4581\n",
      "episode 178 score 8.0 avg score 861.7 time_steps 91643 learning_steps 4582\n",
      "episode 179 score 9.0 avg score 860.2 time_steps 91652 learning_steps 4582\n",
      "episode 180 score 10.0 avg score 859.3 time_steps 91662 learning_steps 4583\n",
      "episode 181 score 10.0 avg score 858.7 time_steps 91672 learning_steps 4583\n",
      "episode 182 score 9.0 avg score 858.1 time_steps 91681 learning_steps 4584\n",
      "episode 183 score 59.0 avg score 857.7 time_steps 91740 learning_steps 4587\n",
      "episode 184 score 11.0 avg score 857.3 time_steps 91751 learning_steps 4587\n",
      "episode 185 score 12.0 avg score 855.8 time_steps 91763 learning_steps 4588\n",
      "episode 186 score 120.0 avg score 856.2 time_steps 91883 learning_steps 4594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 187 score 104.0 avg score 856.3 time_steps 91987 learning_steps 4599\n",
      "episode 188 score 110.0 avg score 856.5 time_steps 92097 learning_steps 4604\n",
      "episode 189 score 100.0 avg score 857.1 time_steps 92197 learning_steps 4609\n",
      "episode 190 score 12.0 avg score 856.4 time_steps 92209 learning_steps 4610\n",
      "episode 191 score 131.0 avg score 857.1 time_steps 92340 learning_steps 4617\n",
      "episode 192 score 127.0 avg score 857.8 time_steps 92467 learning_steps 4623\n",
      "episode 193 score 78.0 avg score 857.3 time_steps 92545 learning_steps 4627\n",
      "episode 194 score 12.0 avg score 856.3 time_steps 92557 learning_steps 4627\n",
      "episode 195 score 10.0 avg score 855.6 time_steps 92567 learning_steps 4628\n",
      "episode 196 score 11.0 avg score 855.0 time_steps 92578 learning_steps 4628\n",
      "episode 197 score 11.0 avg score 852.9 time_steps 92589 learning_steps 4629\n",
      "episode 198 score 12.0 avg score 852.2 time_steps 92601 learning_steps 4630\n",
      "episode 199 score 10.0 avg score 851.0 time_steps 92611 learning_steps 4630\n",
      "episode 200 score 10.0 avg score 849.7 time_steps 92621 learning_steps 4631\n",
      "episode 201 score 10.0 avg score 849.0 time_steps 92631 learning_steps 4631\n",
      "episode 202 score 95.0 avg score 848.9 time_steps 92726 learning_steps 4636\n",
      "episode 203 score 12.0 avg score 847.4 time_steps 92738 learning_steps 4636\n",
      "episode 204 score 10.0 avg score 846.1 time_steps 92748 learning_steps 4637\n",
      "episode 205 score 10.0 avg score 844.5 time_steps 92758 learning_steps 4637\n",
      "episode 206 score 11.0 avg score 842.2 time_steps 92769 learning_steps 4638\n",
      "episode 207 score 11.0 avg score 840.7 time_steps 92780 learning_steps 4639\n",
      "episode 208 score 10.0 avg score 839.4 time_steps 92790 learning_steps 4639\n",
      "episode 209 score 10.0 avg score 832.2 time_steps 92800 learning_steps 4640\n",
      "episode 210 score 10.0 avg score 830.1 time_steps 92810 learning_steps 4640\n",
      "episode 211 score 11.0 avg score 828.6 time_steps 92821 learning_steps 4641\n",
      "episode 212 score 83.0 avg score 825.1 time_steps 92904 learning_steps 4645\n",
      "episode 213 score 11.0 avg score 821.4 time_steps 92915 learning_steps 4645\n",
      "episode 214 score 10.0 avg score 811.5 time_steps 92925 learning_steps 4646\n",
      "episode 215 score 11.0 avg score 808.2 time_steps 92936 learning_steps 4646\n",
      "episode 216 score 12.0 avg score 804.0 time_steps 92948 learning_steps 4647\n",
      "episode 217 score 135.0 avg score 802.4 time_steps 93083 learning_steps 4654\n",
      "episode 218 score 175.0 avg score 797.6 time_steps 93258 learning_steps 4662\n",
      "episode 219 score 188.0 avg score 781.0 time_steps 93446 learning_steps 4672\n",
      "episode 220 score 200.0 avg score 781.3 time_steps 93646 learning_steps 4682\n",
      "episode 221 score 201.0 avg score 782.4 time_steps 93847 learning_steps 4692\n",
      "episode 222 score 198.0 avg score 783.0 time_steps 94045 learning_steps 4702\n",
      "episode 223 score 179.0 avg score 783.3 time_steps 94224 learning_steps 4711\n",
      "episode 224 score 169.0 avg score 767.7 time_steps 94393 learning_steps 4719\n",
      "episode 225 score 12.0 avg score 752.9 time_steps 94405 learning_steps 4720\n",
      "episode 226 score 11.0 avg score 749.4 time_steps 94416 learning_steps 4720\n",
      "episode 227 score 194.0 avg score 742.4 time_steps 94610 learning_steps 4730\n",
      "episode 228 score 229.0 avg score 742.8 time_steps 94839 learning_steps 4741\n",
      "episode 229 score 268.0 avg score 742.7 time_steps 95107 learning_steps 4755\n",
      "episode 230 score 378.0 avg score 735.7 time_steps 95485 learning_steps 4774\n",
      "episode 231 score 360.0 avg score 736.7 time_steps 95845 learning_steps 4792\n",
      "episode 232 score 353.0 avg score 733.1 time_steps 96198 learning_steps 4809\n",
      "episode 233 score 498.0 avg score 730.5 time_steps 96696 learning_steps 4834\n",
      "episode 234 score 1096.0 avg score 719.6 time_steps 97792 learning_steps 4889\n",
      "episode 235 score 2284.0 avg score 738.1 time_steps 100076 learning_steps 5003\n",
      "episode 236 score 1560.0 avg score 749.6 time_steps 101636 learning_steps 5081\n",
      "episode 237 score 305.0 avg score 750.5 time_steps 101941 learning_steps 5097\n",
      "episode 238 score 310.0 avg score 751.3 time_steps 102251 learning_steps 5112\n",
      "episode 239 score 304.0 avg score 752.8 time_steps 102555 learning_steps 5127\n",
      "episode 240 score 316.0 avg score 754.2 time_steps 102871 learning_steps 5143\n",
      "episode 241 score 291.0 avg score 755.2 time_steps 103162 learning_steps 5158\n",
      "episode 242 score 287.0 avg score 513.6 time_steps 103449 learning_steps 5172\n",
      "episode 243 score 307.0 avg score 513.8 time_steps 103756 learning_steps 5187\n",
      "episode 244 score 347.0 avg score 513.7 time_steps 104103 learning_steps 5205\n",
      "episode 245 score 330.0 avg score 513.0 time_steps 104433 learning_steps 5221\n",
      "episode 246 score 346.0 avg score 513.5 time_steps 104779 learning_steps 5238\n",
      "episode 247 score 356.0 avg score 514.0 time_steps 105135 learning_steps 5256\n",
      "episode 248 score 27278.0 avg score 784.1 time_steps 132413 learning_steps 6620\n",
      "episode 249 score 434.0 avg score 785.7 time_steps 132847 learning_steps 6642\n",
      "episode 250 score 817.0 avg score 791.0 time_steps 133664 learning_steps 6683\n",
      "episode 251 score 643.0 avg score 794.6 time_steps 134307 learning_steps 6715\n",
      "episode 252 score 589.0 avg score 797.3 time_steps 134896 learning_steps 6744\n",
      "episode 253 score 579.0 avg score 799.3 time_steps 135475 learning_steps 6773\n",
      "episode 254 score 748.0 avg score 803.0 time_steps 136223 learning_steps 6811\n",
      "episode 255 score 1289.0 avg score 810.9 time_steps 137512 learning_steps 6875\n",
      "... saving models ...\n",
      "episode 256 score 51038.0 avg score 1315.6 time_steps 188550 learning_steps 9427\n",
      "episode 257 score 12.0 avg score 1296.8 time_steps 188562 learning_steps 9428\n",
      "episode 258 score 63.0 avg score 1280.9 time_steps 188625 learning_steps 9431\n",
      "episode 259 score 9.0 avg score 1274.6 time_steps 188634 learning_steps 9431\n",
      "episode 260 score 9.0 avg score 1268.9 time_steps 188643 learning_steps 9432\n",
      "episode 261 score 10.0 avg score 1257.9 time_steps 188653 learning_steps 9432\n",
      "episode 262 score 12.0 avg score 1247.2 time_steps 188665 learning_steps 9433\n",
      "episode 263 score 239.0 avg score 1241.2 time_steps 188904 learning_steps 9445\n",
      "episode 264 score 339.0 avg score 1236.0 time_steps 189243 learning_steps 9462\n",
      "episode 265 score 10.0 avg score 1235.7 time_steps 189253 learning_steps 9462\n",
      "episode 266 score 12.0 avg score 1235.6 time_steps 189265 learning_steps 9463\n",
      "... saving models ...\n",
      "episode 267 score 24796.0 avg score 1483.3 time_steps 214061 learning_steps 10703\n",
      "... saving models ...\n",
      "episode 268 score 320.0 avg score 1486.3 time_steps 214381 learning_steps 10719\n",
      "episode 269 score 259.0 avg score 1467.2 time_steps 214640 learning_steps 10732\n",
      "episode 270 score 270.0 avg score 1465.7 time_steps 214910 learning_steps 10745\n",
      "episode 271 score 306.0 avg score 1464.9 time_steps 215216 learning_steps 10760\n",
      "episode 272 score 305.0 avg score 1464.0 time_steps 215521 learning_steps 10776\n",
      "episode 273 score 437.0 avg score 1464.2 time_steps 215958 learning_steps 10797\n",
      "... saving models ...\n",
      "episode 274 score 23901.0 avg score 1695.5 time_steps 239859 learning_steps 11992\n",
      "... saving models ...\n",
      "episode 275 score 12826.0 avg score 1815.3 time_steps 252685 learning_steps 12634\n",
      "episode 276 score 11.0 avg score 1611.4 time_steps 252696 learning_steps 12634\n",
      "episode 277 score 9.0 avg score 1610.7 time_steps 252705 learning_steps 12635\n",
      "episode 278 score 10.0 avg score 1610.7 time_steps 252715 learning_steps 12635\n",
      "episode 279 score 9.0 avg score 1610.7 time_steps 252724 learning_steps 12636\n",
      "episode 280 score 11.0 avg score 1610.7 time_steps 252735 learning_steps 12636\n",
      "episode 281 score 9.0 avg score 1610.7 time_steps 252744 learning_steps 12637\n",
      "episode 282 score 9.0 avg score 1610.7 time_steps 252753 learning_steps 12637\n",
      "episode 283 score 10.0 avg score 1610.2 time_steps 252763 learning_steps 12638\n",
      "episode 284 score 9.0 avg score 1610.2 time_steps 252772 learning_steps 12638\n",
      "episode 285 score 9.0 avg score 1610.2 time_steps 252781 learning_steps 12639\n",
      "episode 286 score 10.0 avg score 1609.1 time_steps 252791 learning_steps 12639\n",
      "episode 287 score 9.0 avg score 1608.1 time_steps 252800 learning_steps 12640\n",
      "episode 288 score 8.0 avg score 1607.1 time_steps 252808 learning_steps 12640\n",
      "episode 289 score 10.0 avg score 1606.2 time_steps 252818 learning_steps 12640\n",
      "episode 290 score 10.0 avg score 1606.2 time_steps 252828 learning_steps 12641\n",
      "episode 291 score 9.0 avg score 1605.0 time_steps 252837 learning_steps 12641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 292 score 8.0 avg score 1603.8 time_steps 252845 learning_steps 12642\n",
      "episode 293 score 10.0 avg score 1603.1 time_steps 252855 learning_steps 12642\n",
      "episode 294 score 9.0 avg score 1603.1 time_steps 252864 learning_steps 12643\n",
      "episode 295 score 10.0 avg score 1603.1 time_steps 252874 learning_steps 12643\n",
      "episode 296 score 9.0 avg score 1603.0 time_steps 252883 learning_steps 12644\n",
      "episode 297 score 11.0 avg score 1603.0 time_steps 252894 learning_steps 12644\n",
      "episode 298 score 10.0 avg score 1603.0 time_steps 252904 learning_steps 12645\n",
      "episode 299 score 10.0 avg score 1603.0 time_steps 252914 learning_steps 12645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1837afd0f50>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 20\n",
    "batch_size = 5\n",
    "n_epochs = 4\n",
    "alpha = 0.0003\n",
    "agent = Agent(n_actions=env.action_space.n, input_dims=env.observation_space.shape, batch_size=batch_size, alpha=alpha, n_epochs=n_epochs)\n",
    "\n",
    "n_games = 300\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    (observation, _) = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, _, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "\n",
    "    print('episode', i, 'score %.1f' % score,'avg score %.1f' % avg_score, 'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "x = [i + 1 for i in range(len(score_history))]\n",
    "plt.plot(x, score_history)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "603fba1d-212d-4624-985d-82473afbdf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "612c7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///P:/repos/cellitaire-rl\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from cellitaire==0.0.1) (2.2.2)\n",
      "Building wheels for collected packages: cellitaire\n",
      "  Building editable for cellitaire (pyproject.toml): started\n",
      "  Building editable for cellitaire (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for cellitaire: filename=cellitaire-0.0.1-0.editable-py3-none-any.whl size=1307 sha256=abd36cab9a29c4bae2fb4bc6203232d252c22704dd97daeba65e075a317e3d22\n",
      "  Stored in directory: C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-14ynjh2i\\wheels\\76\\88\\96\\576254946bf13aa1d012dfa539e7e5e1fddd6ac7e334da0df2\n",
      "Successfully built cellitaire\n",
      "Installing collected packages: cellitaire\n",
      "  Attempting uninstall: cellitaire\n",
      "    Found existing installation: cellitaire 0.0.1\n",
      "    Uninstalling cellitaire-0.0.1:\n",
      "      Successfully uninstalled cellitaire-0.0.1\n",
      "Successfully installed cellitaire-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b132a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (0.28.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [86 lines of output]\n",
      "  \n",
      "  \n",
      "  WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
      "  Using WINDOWS configuration...\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m2\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mexec\u001b[0m\u001b[1;31m(compile('''\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;31m# This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "      ...<31 lines>...\n",
      "      \u001b[1;31mexec(compile(setup_py_code, filename, \"exec\"))\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;31m''' % ('C:\\\\Users\\\\Noe\\\\AppData\\\\Local\\\\Temp\\\\pip-install-eoyw7vpr\\\\pygame_967dc80c3fe74667b8820019c1335dad\\\\setup.py',), \"<pip-setuptools-caller>\", \"exec\"))\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"<pip-setuptools-caller>\"\u001b[0m, line \u001b[35m34\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\setup.py\"\u001b[0m, line \u001b[35m400\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mbuildconfig.config.main\u001b[0m\u001b[1;31m(AUTO_CONFIG)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config.py\"\u001b[0m, line \u001b[35m231\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      deps = CFG.main(**kwds, auto_config=auto)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m493\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      return setup_prebuilt_sdl2(prebuilt_dir)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m453\u001b[0m, in \u001b[35msetup_prebuilt_sdl2\u001b[0m\n",
      "      \u001b[31mDEPS.configure\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m336\u001b[0m, in \u001b[35mconfigure\u001b[0m\n",
      "      from . import vstools\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\vstools.py\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mcompiler.initialize\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\"\u001b[0m, line \u001b[35m400\u001b[0m, in \u001b[35minitialize\u001b[0m\n",
      "      vc_env = query_vcvarsall(VERSION, plat_spec)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\"\u001b[0m, line \u001b[35m280\u001b[0m, in \u001b[35mquery_vcvarsall\u001b[0m\n",
      "      raise DistutilsPlatformError(\"Unable to find vcvarsall.bat\")\n",
      "  \u001b[1;35mdistutils.errors.DistutilsPlatformError\u001b[0m: \u001b[35mUnable to find vcvarsall.bat\u001b[0m\n",
      "  Making dir :prebuilt_downloads:\n",
      "  Downloading... https://www.libsdl.org/release/SDL2-devel-2.0.22-VC.zip efa040633c4faf8b006c0c1e552456ca4e5a3a53\n",
      "  Unzipping :prebuilt_downloads\\SDL2-devel-2.0.22-VC.zip:\n",
      "  Downloading... https://www.libsdl.org/projects/SDL_image/release/SDL2_image-devel-2.0.5-VC.zip 137f86474691f4e12e76e07d58d5920c8d844d5b\n",
      "  Unzipping :prebuilt_downloads\\SDL2_image-devel-2.0.5-VC.zip:\n",
      "  Downloading... https://github.com/libsdl-org/SDL_ttf/releases/download/release-2.20.1/SDL2_ttf-devel-2.20.1-VC.zip 371606aceba450384428fd2852f73d2f6290b136\n",
      "  Unzipping :prebuilt_downloads\\SDL2_ttf-devel-2.20.1-VC.zip:\n",
      "  Downloading... https://github.com/libsdl-org/SDL_mixer/releases/download/release-2.6.2/SDL2_mixer-devel-2.6.2-VC.zip 000e3ea8a50261d46dbd200fb450b93c59ed4482\n",
      "  Unzipping :prebuilt_downloads\\SDL2_mixer-devel-2.6.2-VC.zip:\n",
      "  Downloading... https://github.com/pygame/pygame/releases/download/2.1.3.dev4/prebuilt-x64-pygame-2.1.4-20220319.zip 16b46596744ce9ef80e7e40fa72ddbafef1cf586\n",
      "  Unzipping :prebuilt_downloads\\prebuilt-x64-pygame-2.1.4-20220319.zip:\n",
      "  copying into .\\prebuilt-x64\n",
      "  Path for SDL: prebuilt-x64\\SDL2-2.0.22\n",
      "  ...Library directory for SDL: prebuilt-x64/SDL2-2.0.22/lib/x64\n",
      "  ...Include directory for SDL: prebuilt-x64/SDL2-2.0.22/include\n",
      "  Path for FONT: prebuilt-x64\\SDL2_ttf-2.20.1\n",
      "  ...Library directory for FONT: prebuilt-x64/SDL2_ttf-2.20.1/lib/x64\n",
      "  ...Include directory for FONT: prebuilt-x64/SDL2_ttf-2.20.1/include\n",
      "  Path for IMAGE: prebuilt-x64\\SDL2_image-2.0.5\n",
      "  ...Library directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/lib/x64\n",
      "  ...Include directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/include\n",
      "  Path for MIXER: prebuilt-x64\\SDL2_mixer-2.6.2\n",
      "  ...Library directory for MIXER: prebuilt-x64/SDL2_mixer-2.6.2/lib/x64\n",
      "  ...Include directory for MIXER: prebuilt-x64/SDL2_mixer-2.6.2/include\n",
      "  Path for PORTMIDI: prebuilt-x64\n",
      "  ...Library directory for PORTMIDI: prebuilt-x64/lib\n",
      "  ...Include directory for PORTMIDI: prebuilt-x64/include\n",
      "  DLL for SDL2: prebuilt-x64/SDL2-2.0.22/lib/x64/SDL2.dll\n",
      "  DLL for SDL2_ttf: prebuilt-x64/SDL2_ttf-2.20.1/lib/x64/SDL2_ttf.dll\n",
      "  DLL for SDL2_image: prebuilt-x64/SDL2_image-2.0.5/lib/x64/SDL2_image.dll\n",
      "  DLL for SDL2_mixer: prebuilt-x64/SDL2_mixer-2.6.2/lib/x64/SDL2_mixer.dll\n",
      "  DLL for portmidi: prebuilt-x64/lib/portmidi.dll\n",
      "  Path for FREETYPE: prebuilt-x64\n",
      "  ...Library directory for FREETYPE: prebuilt-x64/lib\n",
      "  ...Include directory for FREETYPE: prebuilt-x64/include\n",
      "  Path for PNG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  Path for JPEG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  DLL for freetype: prebuilt-x64/lib/freetype.dll\n",
      "  DLL for png: prebuilt-x64/SDL2_image-2.0.5/lib/x64/libpng16-16.dll\n",
      "  \n",
      "  ---\n",
      "  For help with compilation see:\n",
      "      https://www.pygame.org/wiki/CompileWindows\n",
      "  To contribute to pygame development see:\n",
      "      https://www.pygame.org/contribute.html\n",
      "  ---\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (2.2.2)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Collecting pygame==2.1.3 (from gymnasium[classic-control])\n",
      "  Using cached pygame-2.1.3.tar.gz (12.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce4d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 42.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 37.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b18d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
