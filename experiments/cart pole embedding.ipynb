{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb804a91-6ebc-4b2c-a410-b0bb5ca11ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from cellitaire.environment.cellitaire_env import CellitaireEnv\n",
    "from cellitaire.environment.rewards.reward import *\n",
    "from cellitaire.environment.rewards.foundation_rewards import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16e4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "        \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48832150-4a0e-4e32-ab19-1b324ff80055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        alpha,\n",
    "        chkpt_dir='tmp/ppo', \n",
    "        num_embeddings=53, \n",
    "        embedding_dim=30, \n",
    "        embeddings_in_state=85,\n",
    "        num_hidden_layers=1,\n",
    "        hidden_dim=256\n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.embeddings_in_state = embeddings_in_state\n",
    "\n",
    "        # Create embedding layer only if embeddings_in_state > 0.\n",
    "        if embeddings_in_state > 0:\n",
    "            self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim).to(self.device)\n",
    "            input_layer_dim = input_dims[0] - embeddings_in_state + (embeddings_in_state * embedding_dim)\n",
    "        else:\n",
    "            self.embedding_layer = None\n",
    "            input_layer_dim = input_dims[0]\n",
    "\n",
    "        print(self.embedding_layer.parameters())\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # First layer with layer normalization and activation.\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_layer_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Append hidden layers with layer normalization between linear and activation.\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.actor.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.actor.append(nn.LayerNorm(hidden_dim))\n",
    "            self.actor.append(nn.ReLU())\n",
    "        \n",
    "        # Final output layer.\n",
    "        self.actor.append(nn.Linear(hidden_dim, n_actions))\n",
    "        #self.actor.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.to(self.device)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state):\n",
    "        if self.embeddings_in_state > 0:\n",
    "            embeddings = self.embedding_layer(state[:, :self.embeddings_in_state].int()).view(state.shape[0], 1, -1)\n",
    "            s = state.view(state.shape[0], 1, -1)[:, :, self.embeddings_in_state:]\n",
    "            \n",
    "            # Concatenate the remaining state features with the flattened embeddings.\n",
    "            new_state = torch.cat((s, embeddings), dim=2)\n",
    "        else:\n",
    "            new_state = state.view(state.shape[0], 1, -1)\n",
    "        \n",
    "        x = new_state\n",
    "        if torch.isnan(x).any():\n",
    "            print(f\"NaN detected after embedding\")\n",
    "        logits = self.actor(new_state)\n",
    "        return logits\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89c790f-8870-4894-ac88-9a9cafbe684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dims, \n",
    "        alpha, \n",
    "        num_hidden_layers=1, \n",
    "        hidden_dim=256, \n",
    "        chkpt_dir='tmp/ppo'\n",
    "    ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(*input_dims, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.critic.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.critic.append(nn.ReLU())\n",
    "\n",
    "        self.critic.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27ae2fc7-893a-4c88-a9eb-95d2fbfa027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        gamma=0.99, \n",
    "        alpha=0.0003, \n",
    "        gae_lambda=0.95,\n",
    "        policy_clip=0.2, \n",
    "        batch_size=64, \n",
    "        n_epochs=10,\n",
    "        num_hidden_layers_actor=1,\n",
    "        hidden_dim_actor=256,\n",
    "        num_hidden_layers_critic=1,\n",
    "        hidden_dim_critic=256,\n",
    "        embeddings_in_state_actor=85,\n",
    "        embedding_dim_actor=30\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            n_actions, \n",
    "            input_dims, \n",
    "            alpha, \n",
    "            num_hidden_layers=num_hidden_layers_actor, \n",
    "            hidden_dim=hidden_dim_actor,\n",
    "            embeddings_in_state=embeddings_in_state_actor,\n",
    "            embedding_dim=embedding_dim_actor\n",
    "        )\n",
    "        self.critic = CriticNetwork(\n",
    "            input_dims, \n",
    "            alpha,\n",
    "            num_hidden_layers=num_hidden_layers_critic,\n",
    "            hidden_dim=hidden_dim_critic\n",
    "        )\n",
    "        actor_param_count = sum(p.numel() for p in self.actor.parameters())\n",
    "        critic_param_count = sum(p.numel() for p in self.critic.parameters())\n",
    "        print(f'NUM - PARAMS {actor_param_count + critic_param_count}')\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        #print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        #print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "        dist = Categorical(dist)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        # Convert observation to tensor and send it to the actor's device.\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (batch, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # Convert legal_actions to a tensor on the same device.\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "\n",
    "        # Mask logits so that only legal actions remain.\n",
    "        # If logits has shape (n, 1, k), we index into the third dimension.\n",
    "        masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "\n",
    "        # Build a Categorical using the masked logits.\n",
    "        dist = Categorical(logits=masked_logits)\n",
    "\n",
    "        # Get the value estimate from the critic.\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        # This gives a relative index in the filtered (legal) logits.\n",
    "        relative_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map the relative index back to the original action space.\n",
    "        action = legal_actions[relative_index]\n",
    "\n",
    "        # Compute the log probability for the sampled action.\n",
    "        probs = torch.squeeze(dist.log_prob(relative_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    def choose_action(self, observation, legal_actions=None):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (n, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # If legal_actions is provided, mask the logits to include only those actions.\n",
    "        if legal_actions is not None:\n",
    "            legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "            masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "            dist = Categorical(logits=masked_logits)\n",
    "        else:\n",
    "            # Use all logits if no legal actions mask is provided.\n",
    "            full_logits = logits.squeeze(1)  # shape: (n, k)\n",
    "            dist = Categorical(logits=full_logits)\n",
    "\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        sampled_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map back to the original action if a mask was applied.\n",
    "        if legal_actions is not None:\n",
    "            action = legal_actions[sampled_index]\n",
    "        else:\n",
    "            action = sampled_index\n",
    "\n",
    "        # Get the log probability of the sampled action.\n",
    "        log_prob = torch.squeeze(dist.log_prob(sampled_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, log_prob, value\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        device = self.actor.device  # Assuming this is a CUDA device\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Retrieve batch data\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, batches = \\\n",
    "                self.memory.generate_batches()\n",
    "            \n",
    "            # Convert arrays to torch tensors on GPU\n",
    "            rewards = torch.tensor(reward_arr, dtype=torch.float32, device=device)\n",
    "            values = torch.tensor(vals_arr, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones_arr, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Compute deltas for GAE: delta_t = r_t + gamma * V(t+1) * (1-done) - V(t)\n",
    "            deltas = rewards[:-1] + self.gamma * values[1:] * (1 - dones[:-1]) - values[:-1]\n",
    "            \n",
    "            # Compute advantage vector using the vectorized discounted cumulative sum.\n",
    "            advantage = torch.zeros_like(rewards, device=device)\n",
    "            advantage[:-1] = discount_cumsum(deltas, self.gamma * self.gae_lambda)\n",
    "            \n",
    "            # Loop over minibatches\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float, device=device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch], device=device)\n",
    "                actions = torch.tensor(action_arr[batch], device=device)\n",
    "    \n",
    "                # Forward pass through the actor and critic networks\n",
    "                dist = self.actor(states)\n",
    "                dist = Categorical(logits=dist)\n",
    "                critic_value = self.critic(states).squeeze()\n",
    "                \n",
    "                # Calculate probability ratio and losses\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1 - self.policy_clip, 1 + self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "    \n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value).pow(2).mean()\n",
    "    \n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                \n",
    "                # Update the networks\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1)\n",
    "                #torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1)\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "    \n",
    "        self.memory.clear_memory()             \n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    T = x.size(0)\n",
    "    discount_factors = discount ** torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "    # Clamp discount_factors to avoid zero values\n",
    "    discount_factors = torch.clamp(discount_factors, min=1e-10)\n",
    "    x_discounted = x * discount_factors\n",
    "    reversed_x = torch.flip(x_discounted, dims=[0])\n",
    "    cumsum_reversed = torch.cumsum(reversed_x, dim=0)\n",
    "    discounted_cumsum = torch.flip(cumsum_reversed, dims=[0])\n",
    "    return discounted_cumsum / discount_factors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67f3518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3712543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000000, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "board_rows = 7\n",
    "board_cols = 12\n",
    "num_reserved = 6\n",
    "test_reward = CombinedReward([\n",
    "    #PlacedCardInFoundationReward(weight=6),\n",
    "    WinReward(rows=board_rows, cols=board_cols),\n",
    "    ConstantReward(weight=0.5),\n",
    "    ScalingPlacedCardInFoundationReward(weight=1, rows=board_rows, cols=board_cols)\n",
    "    #PlayedLegalMoveReward(weight=1),\n",
    "    #PeriodicPlacedCardInFoundationReward(weight=4, reward_period=3),\n",
    "    #CreatedMovesReward(weight=1, num_reserved=num_reserved, foundation_count_dropoff=30)\n",
    "])\n",
    "print([reward.weight for reward in test_reward.rewards_list])\n",
    "env = CellitaireEnv(test_reward, rows=board_rows, cols=board_cols, num_reserved=num_reserved, max_moves=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e261d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001B1E960ECE0>\n",
      "NUM - PARAMS 113968779\n",
      "episode    40 | score  132.0 | recent avg   64.5 | avg moves  53.6 | avg saved  3.6 | learning steps      1 | done True *\n",
      "episode    44 | score  617.0 | recent avg   73.3 | avg moves  59.5 | avg saved  4.0 | learning steps      1 | done True *\n",
      "episode    45 | score  309.0 | recent avg   78.5 | avg moves  65.1 | avg saved  4.3 | learning steps      1 | done True *\n",
      "episode    82 | score  732.5 | recent avg   83.2 | avg moves  69.8 | avg saved  4.5 | learning steps      2 | done True *\n",
      "episode    87 | score  793.0 | recent avg   89.4 | avg moves  73.5 | avg saved  4.8 | learning steps      3 | done True *\n",
      "episode    88 | score  430.0 | recent avg   93.2 | avg moves  76.6 | avg saved  5.0 | learning steps      3 | done True *\n",
      "episode    99 | score 1142.5 | recent avg   98.1 | avg moves  77.3 | avg saved  5.1 | learning steps      3 | done True *\n",
      "episode   103 | score  164.0 | recent avg   98.3 | avg moves  77.9 | avg saved  5.1 | learning steps      3 | done True *\n",
      "episode   104 | score   17.5 | recent avg   98.3 | avg moves  78.0 | avg saved  5.1 | learning steps      3 | done True *\n",
      "episode   105 | score    9.0 | recent avg   98.3 | avg moves  78.1 | avg saved  5.1 | learning steps      3 | done True *\n",
      "episode   106 | score    8.0 | recent avg   98.3 | avg moves  78.1 | avg saved  5.1 | learning steps      3 | done True *\n",
      "episode   107 | score    7.0 | recent avg   98.3 | avg moves  78.1 | avg saved  5.1 | learning steps      4 | done True *\n",
      "episode   135 | score  923.0 | recent avg  105.3 | avg moves  83.8 | avg saved  5.5 | learning steps      5 | done True *\n",
      "episode   137 | score   13.0 | recent avg  105.4 | avg moves  83.9 | avg saved  5.5 | learning steps      5 | done True *\n",
      "episode   138 | score  860.0 | recent avg  113.6 | avg moves  88.6 | avg saved  5.8 | learning steps      5 | done True *\n",
      "episode   139 | score  978.0 | recent avg  123.2 | avg moves  93.1 | avg saved  6.2 | learning steps      5 | done True *\n",
      "episode   142 | score  564.5 | recent avg  127.6 | avg moves  96.1 | avg saved  6.4 | learning steps      5 | done True *\n",
      "episode   143 | score   24.0 | recent avg  127.8 | avg moves  96.4 | avg saved  6.4 | learning steps      5 | done True *\n",
      "episode   144 | score 1058.0 | recent avg  132.2 | avg moves  96.9 | avg saved  6.5 | learning steps      6 | done True *\n",
      "episode   146 | score  796.5 | recent avg  137.1 | avg moves  99.8 | avg saved  6.6 | learning steps      6 | done False *\n",
      "episode   147 | score   22.5 | recent avg  137.3 | avg moves 100.2 | avg saved  6.6 | learning steps      6 | done True *\n",
      "episode   149 | score  103.5 | recent avg  138.1 | avg moves 101.3 | avg saved  6.7 | learning steps      6 | done True *\n",
      "episode   150 | score   42.0 | recent avg  138.5 | avg moves 102.0 | avg saved  6.7 | learning steps      6 | done True *\n",
      "episode   152 | score  389.5 | recent avg  141.8 | avg moves 105.2 | avg saved  6.8 | learning steps      6 | done True *\n",
      "episode   157 | score  345.0 | recent avg  143.6 | avg moves 105.5 | avg saved  6.9 | learning steps      6 | done True *\n",
      "episode   158 | score  508.5 | recent avg  148.6 | avg moves 108.4 | avg saved  7.2 | learning steps      7 | done True *\n",
      "episode   159 | score  731.0 | recent avg  155.8 | avg moves 113.6 | avg saved  7.5 | learning steps      7 | done True *\n",
      "episode   160 | score   26.0 | recent avg  155.9 | avg moves 113.9 | avg saved  7.5 | learning steps      7 | done True *\n",
      "episode   161 | score  354.0 | recent avg  159.4 | avg moves 116.6 | avg saved  7.7 | learning steps      7 | done True *\n",
      "episode   163 | score   32.0 | recent avg  159.6 | avg moves 117.0 | avg saved  7.7 | learning steps      7 | done True *\n",
      "episode   164 | score   34.5 | recent avg  159.8 | avg moves 117.3 | avg saved  7.7 | learning steps      7 | done True *\n",
      "episode   165 | score   14.0 | recent avg  159.9 | avg moves 117.5 | avg saved  7.7 | learning steps      7 | done True *\n",
      "episode   173 | score  907.0 | recent avg  168.4 | avg moves 121.5 | avg saved  8.0 | learning steps      7 | done True *\n",
      "episode   175 | score  447.0 | recent avg  169.5 | avg moves 125.0 | avg saved  8.0 | learning steps      8 | done True *\n",
      "episode   176 | score  734.5 | recent avg  172.1 | avg moves 127.1 | avg saved  8.0 | learning steps      8 | done True *\n",
      "episode   229 | score  630.5 | recent avg  172.2 | avg moves 127.2 | avg saved  8.0 | learning steps     11 | done True *\n",
      "episode   311 | score  786.5 | recent avg  178.2 | avg moves 129.5 | avg saved  8.7 | learning steps     16 | done True *\n",
      "episode   312 | score   27.0 | recent avg  178.4 | avg moves 129.9 | avg saved  8.7 | learning steps     16 | done True *\n",
      "episode   336 | score 1264.5 | recent avg  182.4 | avg moves 123.5 | avg saved  8.6 | learning steps     17 | done True *\n",
      "episode   337 | score  228.5 | recent avg  184.7 | avg moves 125.6 | avg saved  8.8 | learning steps     17 | done True *\n",
      "episode   338 | score   19.0 | recent avg  184.8 | avg moves 125.9 | avg saved  8.7 | learning steps     17 | done True *\n",
      "episode   339 | score   22.0 | recent avg  184.9 | avg moves 126.0 | avg saved  8.8 | learning steps     17 | done True *\n",
      "episode   340 | score   38.0 | recent avg  185.2 | avg moves 126.2 | avg saved  8.8 | learning steps     17 | done True *\n",
      "episode   342 | score   83.0 | recent avg  185.6 | avg moves 126.8 | avg saved  8.8 | learning steps     17 | done True *\n",
      "episode   343 | score   10.0 | recent avg  185.6 | avg moves 126.8 | avg saved  8.9 | learning steps     17 | done True *\n",
      "episode   355 | score 1195.0 | recent avg  189.5 | avg moves 126.3 | avg saved  8.9 | learning steps     18 | done True *\n",
      "episode   360 | score  320.0 | recent avg  191.2 | avg moves 127.6 | avg saved  9.0 | learning steps     18 | done True *\n",
      "episode   361 | score   19.0 | recent avg  191.4 | avg moves 127.8 | avg saved  9.0 | learning steps     18 | done True *\n",
      "episode   362 | score  860.0 | recent avg  199.9 | avg moves 133.1 | avg saved  9.3 | learning steps     19 | done True *\n",
      "episode   364 | score 1263.0 | recent avg  212.1 | avg moves 136.8 | avg saved  9.7 | learning steps     19 | done True *\n",
      "episode   464 | score   12.5 | recent avg   84.0 | avg moves  73.6 | avg saved  4.3 | learning steps     22 | done True\n",
      "episode   564 | score    5.0 | recent avg  148.7 | avg moves 107.8 | avg saved  7.1 | learning steps     28 | done True\n",
      "episode   664 | score   86.0 | recent avg  105.7 | avg moves  87.7 | avg saved  5.4 | learning steps     32 | done True\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "N = 2000\n",
    "batch_size = 2000\n",
    "n_epochs = 1\n",
    "alpha = 1e-6\n",
    "num_hidden_layers_actor=2\n",
    "hidden_dim_actor=4096\n",
    "num_hidden_layers_critic=4\n",
    "hidden_dim_critic=4096\n",
    "#embeddings_in_state_actor=1\n",
    "embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "env.reset()\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=(board_rows * board_cols * 4 + 6,), \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim,\n",
    "    num_hidden_layers_critic=num_hidden_layers_critic,\n",
    "    hidden_dim_critic=hidden_dim_critic,\n",
    ")\n",
    "\n",
    "n_games = 1\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "moves_history = []\n",
    "cards_saved_history = []\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "avg_moves = 0\n",
    "avg_cards_saved = 0\n",
    "n_steps = 0\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    observation, reward, done, truncated, info = env.reset()\n",
    "    observation = env.get_state()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while (not done) and (not truncated):\n",
    "        legal_actions = env.get_legal_actions_as_int()\n",
    "        action, prob, val = agent.choose_action(observation, legal_actions)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    moves_history.append(env.num_moves)\n",
    "    cards_saved_history.append(env.game.foundation.total_cards())\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    avg_moves = np.mean(moves_history[-100:])\n",
    "    avg_cards_saved = np.mean(cards_saved_history[-100:])\n",
    "    i += 1\n",
    "\n",
    "    if avg_score > best_score and n_steps > N:\n",
    "        best_score = avg_score\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done} *') \n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e08b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(score_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(moves_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(cards_saved_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cards_saved_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b04beb-bd28-4c5a-987e-9c8c28b033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d40bf7-3d64-496d-b842-340670553b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#N = 20\n",
    "#batch_size = 5\n",
    "#n_epochs = 4\n",
    "#alpha = 0.0003\n",
    "#embeddings_in_state_actor=1\n",
    "\n",
    "N = 10\n",
    "batch_size = 3\n",
    "n_epochs = 2\n",
    "alpha = 0.0003\n",
    "num_hidden_layers_actor=3\n",
    "hidden_dim_actor=2048\n",
    "embeddings_in_state_actor=1\n",
    "#embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "'''\n",
    "agent = Agent(\n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor\n",
    ")\n",
    "'''\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim\n",
    ")\n",
    "\n",
    "n_games = 150000\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "max_score = 0\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    (observation, _) = env.reset()\n",
    "    observation[0] += 5.0\n",
    "    observation[0] *= 5.2\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    while not done and not truncated:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_[0] += 5.0\n",
    "        observation_[0] *= 5.2\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    max_score = max(max_score, score)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done} *')\n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done}')\n",
    "    \n",
    "\n",
    "plt.plot(x, score_history)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fba1d-212d-4624-985d-82473afbdf4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b132a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b18d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd2b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
