{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb804a91-6ebc-4b2c-a410-b0bb5ca11ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from cellitaire.environment.cellitaire_env import CellitaireEnv\n",
    "from cellitaire.environment.rewards.reward import *\n",
    "from cellitaire.environment.rewards.foundation_rewards import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16e4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "        \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48832150-4a0e-4e32-ab19-1b324ff80055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        alpha,\n",
    "        chkpt_dir='tmp/ppo', \n",
    "        num_embeddings=53, \n",
    "        embedding_dim=30, \n",
    "        embeddings_in_state=85,\n",
    "        num_hidden_layers=1,\n",
    "        hidden_dim=256\n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.embeddings_in_state = embeddings_in_state\n",
    "\n",
    "        # Create embedding layer only if embeddings_in_state > 0.\n",
    "        if embeddings_in_state > 0:\n",
    "            self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim).to(self.device)\n",
    "            input_layer_dim = input_dims[0] - embeddings_in_state + (embeddings_in_state * embedding_dim)\n",
    "        else:\n",
    "            self.embedding_layer = None\n",
    "            input_layer_dim = input_dims[0]\n",
    "\n",
    "        print(self.embedding_layer.parameters())\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # First layer with layer normalization and activation.\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_layer_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Append hidden layers with layer normalization between linear and activation.\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.actor.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.actor.append(nn.LayerNorm(hidden_dim))\n",
    "            self.actor.append(nn.ReLU())\n",
    "        \n",
    "        # Final output layer.\n",
    "        self.actor.append(nn.Linear(hidden_dim, n_actions))\n",
    "        #self.actor.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.to(self.device)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state):\n",
    "        if self.embeddings_in_state > 0:\n",
    "            embeddings = self.embedding_layer(state[:, :self.embeddings_in_state].int()).view(state.shape[0], 1, -1)\n",
    "            s = state.view(state.shape[0], 1, -1)[:, :, self.embeddings_in_state:]\n",
    "            \n",
    "            # Concatenate the remaining state features with the flattened embeddings.\n",
    "            new_state = torch.cat((s, embeddings), dim=2)\n",
    "        else:\n",
    "            new_state = state.view(state.shape[0], 1, -1)\n",
    "        \n",
    "        x = new_state\n",
    "        if torch.isnan(x).any():\n",
    "            print(f\"NaN detected after embedding\")\n",
    "        logits = self.actor(new_state)\n",
    "        return logits\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89c790f-8870-4894-ac88-9a9cafbe684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dims, \n",
    "        alpha, \n",
    "        num_hidden_layers=1, \n",
    "        hidden_dim=256, \n",
    "        chkpt_dir='tmp/ppo'\n",
    "    ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(*input_dims, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.critic.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.critic.append(nn.ReLU())\n",
    "\n",
    "        self.critic.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ae2fc7-893a-4c88-a9eb-95d2fbfa027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        gamma=0.99, \n",
    "        alpha=0.0003, \n",
    "        gae_lambda=0.95,\n",
    "        policy_clip=0.2, \n",
    "        batch_size=64, \n",
    "        n_epochs=10,\n",
    "        num_hidden_layers_actor=1,\n",
    "        hidden_dim_actor=256,\n",
    "        num_hidden_layers_critic=1,\n",
    "        hidden_dim_critic=256,\n",
    "        embeddings_in_state_actor=85,\n",
    "        embedding_dim_actor=30\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            n_actions, \n",
    "            input_dims, \n",
    "            alpha, \n",
    "            num_hidden_layers=num_hidden_layers_actor, \n",
    "            hidden_dim=hidden_dim_actor,\n",
    "            embeddings_in_state=embeddings_in_state_actor,\n",
    "            embedding_dim=embedding_dim_actor\n",
    "        )\n",
    "        self.critic = CriticNetwork(\n",
    "            input_dims, \n",
    "            alpha,\n",
    "            num_hidden_layers=num_hidden_layers_critic,\n",
    "            hidden_dim=hidden_dim_critic\n",
    "        )\n",
    "        actor_param_count = sum(p.numel() for p in self.actor.parameters())\n",
    "        critic_param_count = sum(p.numel() for p in self.critic.parameters())\n",
    "        print(f'NUM - PARAMS {actor_param_count + critic_param_count}')\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        #print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        #print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "        dist = Categorical(dist)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        # Convert observation to tensor and send it to the actor's device.\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (batch, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # Convert legal_actions to a tensor on the same device.\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "\n",
    "        # Mask logits so that only legal actions remain.\n",
    "        # If logits has shape (n, 1, k), we index into the third dimension.\n",
    "        masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "\n",
    "        # Build a Categorical using the masked logits.\n",
    "        dist = Categorical(logits=masked_logits)\n",
    "\n",
    "        # Get the value estimate from the critic.\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        # This gives a relative index in the filtered (legal) logits.\n",
    "        relative_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map the relative index back to the original action space.\n",
    "        action = legal_actions[relative_index]\n",
    "\n",
    "        # Compute the log probability for the sampled action.\n",
    "        probs = torch.squeeze(dist.log_prob(relative_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    def choose_action(self, observation, legal_actions=None):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (n, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # If legal_actions is provided, mask the logits to include only those actions.\n",
    "        if legal_actions is not None:\n",
    "            legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "            masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "            dist = Categorical(logits=masked_logits)\n",
    "        else:\n",
    "            # Use all logits if no legal actions mask is provided.\n",
    "            full_logits = logits.squeeze(1)  # shape: (n, k)\n",
    "            dist = Categorical(logits=full_logits)\n",
    "\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        sampled_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map back to the original action if a mask was applied.\n",
    "        if legal_actions is not None:\n",
    "            action = legal_actions[sampled_index]\n",
    "        else:\n",
    "            action = sampled_index\n",
    "\n",
    "        # Get the log probability of the sampled action.\n",
    "        log_prob = torch.squeeze(dist.log_prob(sampled_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, log_prob, value\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        device = self.actor.device  # Assuming this is a CUDA device\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Retrieve batch data\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, batches = \\\n",
    "                self.memory.generate_batches()\n",
    "            \n",
    "            # Convert arrays to torch tensors on GPU\n",
    "            rewards = torch.tensor(reward_arr, dtype=torch.float32, device=device)\n",
    "            values = torch.tensor(vals_arr, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones_arr, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Compute deltas for GAE: delta_t = r_t + gamma * V(t+1) * (1-done) - V(t)\n",
    "            deltas = rewards[:-1] + self.gamma * values[1:] * (1 - dones[:-1]) - values[:-1]\n",
    "            \n",
    "            # Compute advantage vector using the vectorized discounted cumulative sum.\n",
    "            advantage = torch.zeros_like(rewards, device=device)\n",
    "            advantage[:-1] = discount_cumsum(deltas, self.gamma * self.gae_lambda)\n",
    "            \n",
    "            # Loop over minibatches\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float, device=device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch], device=device)\n",
    "                actions = torch.tensor(action_arr[batch], device=device)\n",
    "    \n",
    "                # Forward pass through the actor and critic networks\n",
    "                dist = self.actor(states)\n",
    "                dist = Categorical(logits=dist)\n",
    "                critic_value = self.critic(states).squeeze()\n",
    "                \n",
    "                # Calculate probability ratio and losses\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1 - self.policy_clip, 1 + self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "    \n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value).pow(2).mean()\n",
    "    \n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                \n",
    "                # Update the networks\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                #torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1)\n",
    "                #torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1)\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "    \n",
    "        self.memory.clear_memory()             \n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    T = x.size(0)\n",
    "    discount_factors = discount ** torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "    # Clamp discount_factors to avoid zero values\n",
    "    discount_factors = torch.clamp(discount_factors, min=1e-10)\n",
    "    x_discounted = x * discount_factors\n",
    "    reversed_x = torch.flip(x_discounted, dims=[0])\n",
    "    cumsum_reversed = torch.cumsum(reversed_x, dim=0)\n",
    "    discounted_cumsum = torch.flip(cumsum_reversed, dims=[0])\n",
    "    return discounted_cumsum / discount_factors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67f3518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3712543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000000, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "board_rows = 7\n",
    "board_cols = 12\n",
    "num_reserved = 6\n",
    "test_reward = CombinedReward([\n",
    "    #PlacedCardInFoundationReward(weight=6),\n",
    "    WinReward(rows=board_rows, cols=board_cols),\n",
    "    ConstantReward(weight=0.5),\n",
    "    ScalingPlacedCardInFoundationReward(weight=1, rows=board_rows, cols=board_cols)\n",
    "    #PlayedLegalMoveReward(weight=1),\n",
    "    #PeriodicPlacedCardInFoundationReward(weight=4, reward_period=3),\n",
    "    #CreatedMovesReward(weight=1, num_reserved=num_reserved, foundation_count_dropoff=30)\n",
    "])\n",
    "print([reward.weight for reward in test_reward.rewards_list])\n",
    "env = CellitaireEnv(test_reward, rows=board_rows, cols=board_cols, num_reserved=num_reserved, max_moves=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e261d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001C25B80D380>\n",
      "NUM - PARAMS 113968779\n",
      "episode   100 | score  110.0 | recent avg  146.7 | avg moves 110.7 | avg saved  7.5 | learning steps      0 | done True\n",
      "episode   200 | score   50.5 | recent avg  155.3 | avg moves 105.5 | avg saved  7.6 | learning steps      0 | done True\n",
      "episode   300 | score  243.0 | recent avg  184.4 | avg moves 141.4 | avg saved  9.0 | learning steps      0 | done True\n",
      "episode   400 | score  100.0 | recent avg  123.1 | avg moves 103.4 | avg saved  6.3 | learning steps      0 | done True\n",
      "episode   500 | score   10.0 | recent avg   99.4 | avg moves  85.2 | avg saved  5.4 | learning steps      0 | done True\n",
      "episode   550 | score   56.5 | recent avg  110.7 | avg moves  97.7 | avg saved  6.0 | learning steps      1 | done True *\n",
      "episode   551 | score    5.0 | recent avg  110.7 | avg moves  97.8 | avg saved  6.0 | learning steps      1 | done True *\n",
      "episode   553 | score  471.5 | recent avg  115.2 | avg moves 103.3 | avg saved  6.1 | learning steps      1 | done False *\n",
      "episode   554 | score   24.0 | recent avg  115.4 | avg moves 103.5 | avg saved  6.2 | learning steps      1 | done True *\n",
      "episode   566 | score 1156.0 | recent avg  122.0 | avg moves 104.6 | avg saved  6.2 | learning steps      1 | done True *\n",
      "episode   569 | score    8.0 | recent avg  122.0 | avg moves 104.6 | avg saved  6.2 | learning steps      1 | done True *\n",
      "episode   572 | score  600.5 | recent avg  126.8 | avg moves 108.8 | avg saved  6.4 | learning steps      1 | done False *\n",
      "episode   573 | score   18.0 | recent avg  126.9 | avg moves 109.0 | avg saved  6.4 | learning steps      1 | done True *\n",
      "episode   574 | score  966.5 | recent avg  131.7 | avg moves 111.7 | avg saved  6.5 | learning steps      1 | done False *\n",
      "episode   583 | score  816.0 | recent avg  138.7 | avg moves 114.6 | avg saved  6.6 | learning steps      1 | done True *\n",
      "episode   585 | score  490.5 | recent avg  143.2 | avg moves 120.0 | avg saved  6.8 | learning steps      1 | done False *\n",
      "episode   587 | score  203.5 | recent avg  144.6 | avg moves 120.7 | avg saved  6.9 | learning steps      1 | done True *\n",
      "episode   588 | score   20.0 | recent avg  144.7 | avg moves 121.0 | avg saved  6.9 | learning steps      1 | done True *\n",
      "episode   589 | score   42.0 | recent avg  145.1 | avg moves 121.6 | avg saved  6.9 | learning steps      1 | done True *\n",
      "episode   590 | score  490.5 | recent avg  149.7 | avg moves 127.3 | avg saved  7.1 | learning steps      1 | done False *\n",
      "episode   629 | score 1439.0 | recent avg  155.3 | avg moves 124.0 | avg saved  6.9 | learning steps      1 | done True *\n",
      "episode   630 | score  531.5 | recent avg  160.6 | avg moves 129.8 | avg saved  7.2 | learning steps      1 | done False *\n",
      "episode   636 | score  630.0 | recent avg  161.1 | avg moves 127.4 | avg saved  7.2 | learning steps      1 | done True *\n",
      "episode   661 | score 1036.0 | recent avg  166.9 | avg moves 127.3 | avg saved  7.4 | learning steps      1 | done True *\n",
      "episode   710 | score  582.5 | recent avg  168.2 | avg moves 129.0 | avg saved  7.6 | learning steps      1 | done True *\n",
      "episode   721 | score   95.0 | recent avg  168.5 | avg moves 132.9 | avg saved  7.7 | learning steps      1 | done True *\n",
      "episode   722 | score   10.0 | recent avg  168.6 | avg moves 133.0 | avg saved  7.7 | learning steps      1 | done True *\n",
      "episode   723 | score  706.5 | recent avg  175.4 | avg moves 138.5 | avg saved  7.9 | learning steps      1 | done False *\n",
      "episode   823 | score    9.0 | recent avg   96.7 | avg moves  91.8 | avg saved  4.7 | learning steps      1 | done True\n",
      "episode   919 | score  378.5 | recent avg  175.4 | avg moves 136.1 | avg saved  7.8 | learning steps      1 | done False *\n",
      "episode   920 | score   15.0 | recent avg  175.5 | avg moves 136.2 | avg saved  7.8 | learning steps      1 | done True *\n",
      "episode   923 | score 1421.0 | recent avg  184.3 | avg moves 137.6 | avg saved  8.1 | learning steps      1 | done True *\n",
      "episode  1023 | score   26.0 | recent avg  125.6 | avg moves 108.2 | avg saved  5.5 | learning steps      1 | done True\n",
      "episode  1106 | score  952.0 | recent avg  192.6 | avg moves 139.8 | avg saved  8.3 | learning steps      2 | done True *\n",
      "episode  1206 | score    9.0 | recent avg  101.5 | avg moves  95.0 | avg saved  4.8 | learning steps      2 | done True\n",
      "episode  1273 | score  735.5 | recent avg  195.4 | avg moves 151.3 | avg saved  8.6 | learning steps      2 | done False *\n",
      "episode  1274 | score   12.5 | recent avg  195.4 | avg moves 151.2 | avg saved  8.6 | learning steps      2 | done True *\n",
      "episode  1276 | score  611.5 | recent avg  200.4 | avg moves 154.8 | avg saved  8.8 | learning steps      2 | done True *\n",
      "episode  1277 | score   10.0 | recent avg  200.5 | avg moves 154.9 | avg saved  8.8 | learning steps      2 | done True *\n",
      "episode  1278 | score  765.5 | recent avg  208.0 | avg moves 160.7 | avg saved  9.1 | learning steps      2 | done False *\n",
      "episode  1279 | score   15.5 | recent avg  208.1 | avg moves 160.8 | avg saved  9.1 | learning steps      2 | done True *\n",
      "episode  1281 | score 1080.5 | recent avg  211.7 | avg moves 161.0 | avg saved  9.2 | learning steps      2 | done False *\n",
      "episode  1282 | score   13.0 | recent avg  211.7 | avg moves 161.0 | avg saved  9.2 | learning steps      2 | done True *\n",
      "episode  1287 | score  453.5 | recent avg  212.4 | avg moves 162.0 | avg saved  9.2 | learning steps      2 | done False *\n",
      "episode  1292 | score  806.0 | recent avg  219.1 | avg moves 164.5 | avg saved  9.4 | learning steps      2 | done True *\n",
      "episode  1296 | score  999.0 | recent avg  224.5 | avg moves 163.5 | avg saved  9.7 | learning steps      2 | done True *\n",
      "episode  1396 | score   26.0 | recent avg  149.6 | avg moves 116.1 | avg saved  6.5 | learning steps      2 | done True\n",
      "episode  1496 | score   23.0 | recent avg  178.6 | avg moves 140.5 | avg saved  8.0 | learning steps      2 | done True\n",
      "episode  1596 | score    6.0 | recent avg   58.4 | avg moves  51.5 | avg saved  2.9 | learning steps      3 | done True\n",
      "episode  1696 | score    9.0 | recent avg   30.0 | avg moves  35.6 | avg saved  1.9 | learning steps      3 | done True\n",
      "episode  1796 | score   10.0 | recent avg   37.0 | avg moves  47.2 | avg saved  1.8 | learning steps      3 | done True\n",
      "episode  1896 | score   10.0 | recent avg   25.6 | avg moves  31.1 | avg saved  1.6 | learning steps      3 | done True\n",
      "episode  1996 | score    3.0 | recent avg   37.0 | avg moves  31.6 | avg saved  2.0 | learning steps      3 | done True\n",
      "episode  2096 | score    5.0 | recent avg   69.0 | avg moves  62.9 | avg saved  3.4 | learning steps      3 | done True\n",
      "episode  2196 | score   17.0 | recent avg   18.9 | avg moves  28.2 | avg saved  1.3 | learning steps      3 | done True\n",
      "episode  2296 | score   10.0 | recent avg   31.8 | avg moves  33.4 | avg saved  1.6 | learning steps      3 | done True\n",
      "episode  2396 | score  726.5 | recent avg   54.8 | avg moves  52.1 | avg saved  2.7 | learning steps      3 | done True\n",
      "episode  2496 | score   28.0 | recent avg   19.6 | avg moves  26.4 | avg saved  1.3 | learning steps      3 | done True\n",
      "episode  2596 | score    8.0 | recent avg   30.2 | avg moves  34.3 | avg saved  1.7 | learning steps      3 | done True\n",
      "episode  2696 | score    6.0 | recent avg   33.3 | avg moves  43.7 | avg saved  1.8 | learning steps      3 | done True\n",
      "episode  2796 | score    8.0 | recent avg   52.8 | avg moves  46.3 | avg saved  2.8 | learning steps      3 | done True\n",
      "episode  2896 | score    6.0 | recent avg   32.5 | avg moves  36.5 | avg saved  2.1 | learning steps      3 | done True\n",
      "episode  2996 | score   24.0 | recent avg   32.9 | avg moves  35.4 | avg saved  1.9 | learning steps      3 | done True\n",
      "episode  3096 | score    9.0 | recent avg   27.9 | avg moves  32.4 | avg saved  1.6 | learning steps      4 | done True\n",
      "episode  3196 | score    9.0 | recent avg   26.3 | avg moves  30.1 | avg saved  1.4 | learning steps      4 | done True\n",
      "episode  3296 | score    3.0 | recent avg   31.6 | avg moves  35.4 | avg saved  1.8 | learning steps      4 | done True\n",
      "episode  3396 | score    9.0 | recent avg   11.3 | avg moves  19.5 | avg saved  0.9 | learning steps      4 | done True\n",
      "episode  3496 | score    7.0 | recent avg   21.8 | avg moves  26.6 | avg saved  1.1 | learning steps      4 | done True\n",
      "episode  3596 | score   14.0 | recent avg   47.7 | avg moves  46.0 | avg saved  2.5 | learning steps      4 | done True\n",
      "episode  3696 | score   13.5 | recent avg   48.2 | avg moves  37.2 | avg saved  2.4 | learning steps      4 | done True\n",
      "episode  3796 | score   18.0 | recent avg   23.1 | avg moves  28.2 | avg saved  1.3 | learning steps      4 | done True\n",
      "episode  3896 | score   13.0 | recent avg   37.3 | avg moves  34.0 | avg saved  2.0 | learning steps      4 | done True\n",
      "episode  3996 | score    7.0 | recent avg   24.9 | avg moves  28.3 | avg saved  1.5 | learning steps      4 | done True\n",
      "episode  4096 | score   10.0 | recent avg   21.0 | avg moves  25.3 | avg saved  1.4 | learning steps      4 | done True\n",
      "episode  4196 | score   13.0 | recent avg   19.8 | avg moves  24.6 | avg saved  1.3 | learning steps      4 | done True\n",
      "episode  4296 | score    9.0 | recent avg   11.8 | avg moves  20.7 | avg saved  0.9 | learning steps      4 | done True\n",
      "episode  4396 | score    7.0 | recent avg   27.7 | avg moves  34.7 | avg saved  1.8 | learning steps      4 | done True\n",
      "episode  4496 | score   16.0 | recent avg   46.9 | avg moves  41.9 | avg saved  2.2 | learning steps      4 | done True\n",
      "episode  4596 | score    6.0 | recent avg   37.3 | avg moves  38.9 | avg saved  1.9 | learning steps      4 | done True\n",
      "episode  4696 | score    7.0 | recent avg   31.9 | avg moves  36.7 | avg saved  1.7 | learning steps      4 | done True\n",
      "episode  4796 | score   19.0 | recent avg   28.2 | avg moves  28.7 | avg saved  1.4 | learning steps      4 | done True\n",
      "episode  4896 | score   17.0 | recent avg   18.6 | avg moves  26.9 | avg saved  1.3 | learning steps      4 | done True\n",
      "episode  4996 | score    4.0 | recent avg   23.8 | avg moves  27.1 | avg saved  1.3 | learning steps      5 | done True\n",
      "episode  5096 | score   13.5 | recent avg   37.9 | avg moves  37.3 | avg saved  2.0 | learning steps      5 | done True\n",
      "episode  5196 | score    6.0 | recent avg   57.0 | avg moves  52.9 | avg saved  2.6 | learning steps      5 | done True\n",
      "episode  5296 | score    7.0 | recent avg   32.4 | avg moves  31.0 | avg saved  1.8 | learning steps      5 | done True\n",
      "episode  5396 | score   21.0 | recent avg   26.2 | avg moves  35.2 | avg saved  1.5 | learning steps      5 | done True\n",
      "episode  5496 | score   25.0 | recent avg   25.1 | avg moves  24.2 | avg saved  1.4 | learning steps      5 | done True\n",
      "episode  5596 | score    6.0 | recent avg   51.0 | avg moves  50.2 | avg saved  2.5 | learning steps      5 | done True\n",
      "episode  5696 | score    6.0 | recent avg   24.7 | avg moves  33.3 | avg saved  1.4 | learning steps      5 | done True\n",
      "episode  5796 | score   18.5 | recent avg   46.9 | avg moves  45.4 | avg saved  2.4 | learning steps      5 | done True\n",
      "episode  5896 | score  473.0 | recent avg   57.2 | avg moves  52.6 | avg saved  2.6 | learning steps      5 | done True\n",
      "episode  5996 | score    3.0 | recent avg   29.3 | avg moves  30.6 | avg saved  1.6 | learning steps      5 | done True\n",
      "episode  6096 | score   10.0 | recent avg   11.3 | avg moves  20.0 | avg saved  0.9 | learning steps      5 | done True\n",
      "episode  6196 | score    9.0 | recent avg   38.3 | avg moves  38.5 | avg saved  1.8 | learning steps      5 | done True\n",
      "episode  6296 | score    5.0 | recent avg   42.8 | avg moves  40.9 | avg saved  2.0 | learning steps      5 | done True\n",
      "episode  6396 | score   15.0 | recent avg   14.3 | avg moves  23.2 | avg saved  1.1 | learning steps      5 | done True\n",
      "episode  6496 | score   15.0 | recent avg   46.9 | avg moves  45.2 | avg saved  2.5 | learning steps      5 | done True\n",
      "episode  6596 | score   11.5 | recent avg   38.3 | avg moves  34.7 | avg saved  2.0 | learning steps      5 | done True\n",
      "episode  6696 | score   11.0 | recent avg   53.3 | avg moves  59.9 | avg saved  2.8 | learning steps      6 | done True\n",
      "episode  6796 | score    6.0 | recent avg   85.2 | avg moves  74.4 | avg saved  4.2 | learning steps      6 | done True\n",
      "episode  6896 | score   10.0 | recent avg   56.0 | avg moves  64.0 | avg saved  2.9 | learning steps      6 | done True\n",
      "episode  6996 | score    7.0 | recent avg   76.0 | avg moves  71.9 | avg saved  3.6 | learning steps      6 | done True\n",
      "episode  7096 | score   18.0 | recent avg   86.6 | avg moves  71.5 | avg saved  4.1 | learning steps      6 | done True\n",
      "episode  7196 | score   16.0 | recent avg   64.2 | avg moves  56.7 | avg saved  3.0 | learning steps      6 | done True\n",
      "episode  7296 | score    4.0 | recent avg   77.3 | avg moves  72.4 | avg saved  3.7 | learning steps      6 | done True\n",
      "episode  7396 | score   27.0 | recent avg   30.7 | avg moves  37.5 | avg saved  1.8 | learning steps      6 | done True\n",
      "episode  7496 | score   14.0 | recent avg   64.0 | avg moves  64.2 | avg saved  3.2 | learning steps      6 | done True\n",
      "episode  7596 | score    8.0 | recent avg   78.9 | avg moves  78.3 | avg saved  3.9 | learning steps      7 | done True\n",
      "episode  7696 | score   16.0 | recent avg   61.6 | avg moves  57.5 | avg saved  3.2 | learning steps      7 | done True\n",
      "episode  7796 | score   17.0 | recent avg   72.1 | avg moves  69.3 | avg saved  3.8 | learning steps      7 | done True\n",
      "episode  7896 | score  839.5 | recent avg  103.1 | avg moves 101.0 | avg saved  4.6 | learning steps      7 | done True\n",
      "episode  7996 | score   22.0 | recent avg   48.2 | avg moves  48.8 | avg saved  2.3 | learning steps      7 | done True\n",
      "episode  8096 | score   19.0 | recent avg   47.9 | avg moves  52.9 | avg saved  2.5 | learning steps      7 | done True\n",
      "episode  8196 | score   18.0 | recent avg   66.9 | avg moves  64.8 | avg saved  3.2 | learning steps      7 | done True\n",
      "episode  8296 | score    7.0 | recent avg   72.9 | avg moves  73.3 | avg saved  3.7 | learning steps      7 | done True\n",
      "episode  8396 | score    7.0 | recent avg   57.8 | avg moves  60.0 | avg saved  2.9 | learning steps      7 | done True\n",
      "episode  8496 | score   13.0 | recent avg   70.1 | avg moves  62.1 | avg saved  3.4 | learning steps      8 | done True\n",
      "episode  8596 | score    8.0 | recent avg   55.3 | avg moves  49.4 | avg saved  3.0 | learning steps      8 | done True\n",
      "episode  8696 | score   24.0 | recent avg   56.4 | avg moves  54.7 | avg saved  3.1 | learning steps      8 | done True\n",
      "episode  8796 | score  828.5 | recent avg   66.1 | avg moves  56.6 | avg saved  3.2 | learning steps      8 | done False\n",
      "episode  8896 | score  735.5 | recent avg   94.3 | avg moves  87.6 | avg saved  4.3 | learning steps      8 | done False\n",
      "episode  8996 | score    8.0 | recent avg   52.3 | avg moves  55.4 | avg saved  2.8 | learning steps      8 | done True\n",
      "episode  9096 | score    6.0 | recent avg   47.4 | avg moves  52.5 | avg saved  2.6 | learning steps      8 | done True\n",
      "episode  9196 | score   11.0 | recent avg   69.9 | avg moves  65.3 | avg saved  3.4 | learning steps      8 | done True\n",
      "episode  9296 | score   11.5 | recent avg  101.3 | avg moves  90.4 | avg saved  4.5 | learning steps      8 | done True\n",
      "episode  9396 | score    7.5 | recent avg   63.5 | avg moves  64.6 | avg saved  3.4 | learning steps      9 | done True\n",
      "episode  9496 | score    9.0 | recent avg   63.8 | avg moves  55.1 | avg saved  3.1 | learning steps      9 | done True\n",
      "episode  9596 | score   12.0 | recent avg   67.1 | avg moves  59.7 | avg saved  3.1 | learning steps      9 | done True\n",
      "episode  9696 | score   20.0 | recent avg   91.3 | avg moves  72.4 | avg saved  4.4 | learning steps      9 | done True\n",
      "episode  9796 | score   12.0 | recent avg   70.9 | avg moves  65.0 | avg saved  3.4 | learning steps      9 | done True\n",
      "episode  9896 | score   19.0 | recent avg  106.1 | avg moves  85.2 | avg saved  4.6 | learning steps      9 | done True\n",
      "episode  9996 | score   12.5 | recent avg   52.2 | avg moves  54.0 | avg saved  2.6 | learning steps      9 | done True\n",
      "episode 10096 | score   30.5 | recent avg   88.6 | avg moves  81.0 | avg saved  4.0 | learning steps      9 | done True\n",
      "episode 10196 | score   40.0 | recent avg   91.4 | avg moves  85.3 | avg saved  4.4 | learning steps      9 | done True\n",
      "episode 10296 | score   17.0 | recent avg   80.0 | avg moves  70.9 | avg saved  3.8 | learning steps     10 | done True\n",
      "episode 10396 | score    7.0 | recent avg   37.2 | avg moves  44.3 | avg saved  2.0 | learning steps     10 | done True\n",
      "episode 10496 | score   10.0 | recent avg   76.7 | avg moves  59.2 | avg saved  3.7 | learning steps     10 | done True\n",
      "episode 10596 | score   13.0 | recent avg   56.5 | avg moves  59.9 | avg saved  2.9 | learning steps     10 | done True\n",
      "episode 10696 | score   26.0 | recent avg   54.8 | avg moves  56.2 | avg saved  2.6 | learning steps     10 | done True\n",
      "episode 10796 | score    3.0 | recent avg   83.3 | avg moves  74.1 | avg saved  3.7 | learning steps     10 | done True\n",
      "episode 10896 | score    4.0 | recent avg   68.3 | avg moves  61.5 | avg saved  3.2 | learning steps     10 | done True\n",
      "episode 10996 | score    9.0 | recent avg   51.0 | avg moves  59.9 | avg saved  2.6 | learning steps     10 | done True\n",
      "episode 11096 | score   11.0 | recent avg  108.2 | avg moves  93.3 | avg saved  5.0 | learning steps     10 | done True\n",
      "episode 11196 | score   15.0 | recent avg   57.7 | avg moves  62.3 | avg saved  2.9 | learning steps     11 | done True\n",
      "episode 11296 | score   38.0 | recent avg   58.6 | avg moves  50.8 | avg saved  2.6 | learning steps     11 | done True\n",
      "episode 11396 | score   12.0 | recent avg   34.0 | avg moves  38.4 | avg saved  1.9 | learning steps     11 | done True\n",
      "episode 11496 | score   15.0 | recent avg   49.8 | avg moves  53.2 | avg saved  2.6 | learning steps     11 | done True\n",
      "episode 11596 | score   24.0 | recent avg   54.8 | avg moves  48.8 | avg saved  2.6 | learning steps     11 | done True\n",
      "episode 11696 | score  796.5 | recent avg   71.3 | avg moves  66.3 | avg saved  3.4 | learning steps     11 | done False\n",
      "episode 11796 | score   11.0 | recent avg   67.5 | avg moves  62.0 | avg saved  3.0 | learning steps     11 | done True\n",
      "episode 11896 | score    9.0 | recent avg   28.4 | avg moves  34.1 | avg saved  1.6 | learning steps     11 | done True\n",
      "episode 11996 | score    9.0 | recent avg   76.8 | avg moves  67.9 | avg saved  3.6 | learning steps     11 | done True\n",
      "episode 12096 | score    8.0 | recent avg   47.4 | avg moves  46.7 | avg saved  2.4 | learning steps     11 | done True\n",
      "episode 12196 | score    5.5 | recent avg   57.4 | avg moves  56.2 | avg saved  3.0 | learning steps     11 | done True\n",
      "episode 12296 | score 1146.0 | recent avg   78.8 | avg moves  67.2 | avg saved  3.5 | learning steps     12 | done True\n",
      "episode 12396 | score   13.0 | recent avg   51.3 | avg moves  51.6 | avg saved  2.7 | learning steps     12 | done True\n",
      "episode 12496 | score   67.0 | recent avg  102.7 | avg moves 103.2 | avg saved  5.0 | learning steps     12 | done True\n",
      "episode 12596 | score   99.0 | recent avg   76.8 | avg moves  70.3 | avg saved  3.7 | learning steps     12 | done True\n",
      "episode 12696 | score    8.0 | recent avg  119.9 | avg moves  97.3 | avg saved  5.5 | learning steps     12 | done True\n",
      "episode 12796 | score    7.0 | recent avg  111.9 | avg moves 109.0 | avg saved  5.3 | learning steps     12 | done True\n",
      "episode 12896 | score   10.0 | recent avg  107.7 | avg moves 102.2 | avg saved  4.9 | learning steps     12 | done True\n",
      "episode 12996 | score   12.0 | recent avg   77.4 | avg moves  68.3 | avg saved  3.8 | learning steps     13 | done True\n",
      "episode 13096 | score    9.0 | recent avg   63.1 | avg moves  62.1 | avg saved  3.0 | learning steps     13 | done True\n",
      "episode 13196 | score   27.0 | recent avg   55.2 | avg moves  57.6 | avg saved  3.0 | learning steps     13 | done True\n",
      "episode 13296 | score   27.0 | recent avg   58.7 | avg moves  61.9 | avg saved  3.0 | learning steps     13 | done True\n",
      "episode 13396 | score   35.0 | recent avg   51.1 | avg moves  48.3 | avg saved  2.6 | learning steps     13 | done True\n",
      "episode 13496 | score   20.5 | recent avg   48.3 | avg moves  59.1 | avg saved  2.6 | learning steps     13 | done True\n",
      "episode 13596 | score   12.0 | recent avg   41.3 | avg moves  46.8 | avg saved  2.5 | learning steps     13 | done True\n",
      "episode 13696 | score   11.0 | recent avg   38.3 | avg moves  44.6 | avg saved  2.1 | learning steps     13 | done True\n",
      "episode 13796 | score   10.0 | recent avg   52.6 | avg moves  52.4 | avg saved  2.6 | learning steps     13 | done True\n",
      "episode 13896 | score   11.0 | recent avg   57.9 | avg moves  58.2 | avg saved  2.8 | learning steps     13 | done True\n",
      "episode 13996 | score  916.5 | recent avg   87.7 | avg moves  84.6 | avg saved  4.0 | learning steps     13 | done True\n",
      "episode 14096 | score   14.0 | recent avg   54.1 | avg moves  58.9 | avg saved  2.8 | learning steps     14 | done True\n",
      "episode 14196 | score   14.0 | recent avg   39.7 | avg moves  41.7 | avg saved  2.2 | learning steps     14 | done True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m legal_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_legal_actions_as_int()\n\u001b[0;32m     53\u001b[0m action, prob, val \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(observation, legal_actions)\n\u001b[1;32m---> 54\u001b[0m observation_, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     55\u001b[0m n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     56\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mP:\\repos\\cellitaire-rl\\src\\cellitaire\\environment\\cellitaire_env.py:116\u001b[0m, in \u001b[0;36mCellitaireEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_moves \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 116\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[0;32m    117\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_legal_actions()) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    118\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_moves \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_moves \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_illegal_moves \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_illegal_moves\n",
      "File \u001b[1;32mP:\\repos\\cellitaire-rl\\src\\cellitaire\\environment\\cellitaire_env.py:90\u001b[0m, in \u001b[0;36mCellitaireEnv.get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 90\u001b[0m     board_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_board_state()\n\u001b[0;32m     91\u001b[0m     stockpile_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stockpile_state()\n\u001b[0;32m     92\u001b[0m     foundation_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_foundation_state()\n",
      "File \u001b[1;32mP:\\repos\\cellitaire-rl\\src\\cellitaire\\environment\\cellitaire_env.py:75\u001b[0m, in \u001b[0;36mCellitaireEnv.get_board_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_board_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     70\u001b[0m     board_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m     71\u001b[0m         [[slot\u001b[38;5;241m.\u001b[39mcard\u001b[38;5;241m.\u001b[39mcard_id \u001b[38;5;28;01mif\u001b[39;00m slot\u001b[38;5;241m.\u001b[39mcard \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m slot \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mslots],\n\u001b[0;32m     73\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m     74\u001b[0m     )\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m board_state\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "N = 60000\n",
    "batch_size = 3000\n",
    "n_epochs = 15\n",
    "alpha = 1e-6\n",
    "num_hidden_layers_actor=2\n",
    "hidden_dim_actor=4096\n",
    "num_hidden_layers_critic=4\n",
    "hidden_dim_critic=4096\n",
    "#embeddings_in_state_actor=1\n",
    "embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "env.reset()\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=(board_rows * board_cols * 4 + 6,), \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim,\n",
    "    num_hidden_layers_critic=num_hidden_layers_critic,\n",
    "    hidden_dim_critic=hidden_dim_critic,\n",
    ")\n",
    "\n",
    "n_games = 1\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "moves_history = []\n",
    "cards_saved_history = []\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "avg_moves = 0\n",
    "avg_cards_saved = 0\n",
    "n_steps = 0\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    observation, reward, done, truncated, info = env.reset()\n",
    "    observation = env.get_state()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while (not done) and (not truncated):\n",
    "        legal_actions = env.get_legal_actions_as_int()\n",
    "        action, prob, val = agent.choose_action(observation, legal_actions)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    moves_history.append(env.num_moves)\n",
    "    cards_saved_history.append(env.game.foundation.total_cards())\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    avg_moves = np.mean(moves_history[-100:])\n",
    "    avg_cards_saved = np.mean(cards_saved_history[-100:])\n",
    "    i += 1\n",
    "\n",
    "    if avg_score > best_score and n_steps > N:\n",
    "        best_score = avg_score\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done} *') \n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e08b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(score_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(moves_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(cards_saved_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cards_saved_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b04beb-bd28-4c5a-987e-9c8c28b033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d40bf7-3d64-496d-b842-340670553b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#N = 20\n",
    "#batch_size = 5\n",
    "#n_epochs = 4\n",
    "#alpha = 0.0003\n",
    "#embeddings_in_state_actor=1\n",
    "\n",
    "N = 10\n",
    "batch_size = 3\n",
    "n_epochs = 2\n",
    "alpha = 0.0003\n",
    "num_hidden_layers_actor=3\n",
    "hidden_dim_actor=2048\n",
    "embeddings_in_state_actor=1\n",
    "#embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "'''\n",
    "agent = Agent(\n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor\n",
    ")\n",
    "'''\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim\n",
    ")\n",
    "\n",
    "n_games = 150000\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "max_score = 0\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    (observation, _) = env.reset()\n",
    "    observation[0] += 5.0\n",
    "    observation[0] *= 5.2\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    while not done and not truncated:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_[0] += 5.0\n",
    "        observation_[0] *= 5.2\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    max_score = max(max_score, score)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done} *')\n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done}')\n",
    "    \n",
    "\n",
    "plt.plot(x, score_history)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fba1d-212d-4624-985d-82473afbdf4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b132a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b18d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd2b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
