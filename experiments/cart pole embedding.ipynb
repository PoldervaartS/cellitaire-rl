{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb804a91-6ebc-4b2c-a410-b0bb5ca11ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from cellitaire.environment.cellitaire_env import CellitaireEnv\n",
    "from cellitaire.environment.rewards.reward import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c16e4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "        \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3c5ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        alpha,\n",
    "        chkpt_dir='tmp/ppo', \n",
    "        num_embeddings=53, \n",
    "        embedding_dim=30, \n",
    "        embeddings_in_state=85,\n",
    "        num_hidden_layers=1,\n",
    "        hidden_dim=256\n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.embeddings_in_state = embeddings_in_state\n",
    "\n",
    "        # Create embedding layer only if embeddings_in_state > 0.\n",
    "        if embeddings_in_state > 0:\n",
    "            self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim).to(self.device)\n",
    "            input_layer_dim = input_dims[0] - embeddings_in_state + (embeddings_in_state * embedding_dim)\n",
    "        else:\n",
    "            self.embedding_layer = None\n",
    "            input_layer_dim = input_dims[0]\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_layer_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.actor.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.actor.append(nn.ReLU())\n",
    "        \n",
    "        self.actor.append(nn.Linear(hidden_dim, n_actions))\n",
    "        self.actor.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.to(self.device)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state):\n",
    "        if self.embeddings_in_state > 0:\n",
    "            embeddings = self.embedding_layer(state[:, :self.embeddings_in_state].int()).view(state.shape[0], 1, -1)\n",
    "            state = state.view(state.shape[0], 1, -1)[:, :, self.embeddings_in_state:]\n",
    "\n",
    "            \n",
    "            # Concatenate the remaining state features with the flattened embeddings.\n",
    "            new_state = torch.cat((state, embeddings), dim=2)\n",
    "        else:\n",
    "            new_state = state.view(state.shape[0], 1, -1)\n",
    "        \n",
    "        # Pass the processed state through the actor network.\n",
    "        logits = self.actor(new_state)\n",
    "        return logits\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89c790f-8870-4894-ac88-9a9cafbe684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dims, \n",
    "        alpha, \n",
    "        fc1_dims=256, \n",
    "        fc2_dims=256, \n",
    "        chkpt_dir='tmp/ppo'\n",
    "    ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(*input_dims, fc1_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc1_dims, fc2_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6338dcc-9c50-4744-8c64-cc5da74e5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        gamma=0.99, \n",
    "        alpha=0.0003, \n",
    "        gae_lambda=0.95,\n",
    "        policy_clip=0.2, \n",
    "        batch_size=64, \n",
    "        n_epochs=10,\n",
    "        num_hidden_layers_actor=1,\n",
    "        hidden_dim_actor=256,\n",
    "        embeddings_in_state_actor=85,\n",
    "        embedding_dim_actor=30\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            n_actions, \n",
    "            input_dims, \n",
    "            alpha, \n",
    "            num_hidden_layers=num_hidden_layers_actor, \n",
    "            hidden_dim=hidden_dim_actor,\n",
    "            embeddings_in_state=embeddings_in_state_actor,\n",
    "            embedding_dim=embedding_dim_actor\n",
    "        )\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        #print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        #print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "        dist = Categorical(dist)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        # Convert observation to tensor and send it to the actor's device.\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (batch, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # Convert legal_actions to a tensor on the same device.\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "\n",
    "        # Mask logits so that only legal actions remain.\n",
    "        # If logits has shape (n, 1, k), we index into the third dimension.\n",
    "        masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "\n",
    "        # Build a Categorical using the masked logits.\n",
    "        dist = Categorical(logits=masked_logits)\n",
    "\n",
    "        # Get the value estimate from the critic.\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        # This gives a relative index in the filtered (legal) logits.\n",
    "        relative_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map the relative index back to the original action space.\n",
    "        action = legal_actions[relative_index]\n",
    "\n",
    "        # Compute the log probability for the sampled action.\n",
    "        probs = torch.squeeze(dist.log_prob(relative_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    def choose_action(self, observation, legal_actions=None):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (n, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # If legal_actions is provided, mask the logits to include only those actions.\n",
    "        if legal_actions is not None:\n",
    "            legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "            masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "            dist = Categorical(logits=masked_logits)\n",
    "        else:\n",
    "            # Use all logits if no legal actions mask is provided.\n",
    "            full_logits = logits.squeeze(1)  # shape: (n, k)\n",
    "            dist = Categorical(logits=full_logits)\n",
    "\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        sampled_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map back to the original action if a mask was applied.\n",
    "        if legal_actions is not None:\n",
    "            action = legal_actions[sampled_index]\n",
    "        else:\n",
    "            action = sampled_index\n",
    "\n",
    "        # Get the log probability of the sampled action.\n",
    "        log_prob = torch.squeeze(dist.log_prob(sampled_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, log_prob, value\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = torch.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = torch.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float).to(self.actor.device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = torch.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                dist = Categorical(dist)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = torch.squeeze(critic_value)\n",
    "                \n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67f3518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3712543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1000000, 2, 0.01]\n"
     ]
    }
   ],
   "source": [
    "board_rows = 7\n",
    "board_cols = 12\n",
    "num_reserved = 6\n",
    "test_reward = CombinedReward([\n",
    "    PlacedCardInFoundationReward(weight=2),\n",
    "    WinReward(),\n",
    "    #ConstantReward(weight=-0.01),\n",
    "    #PlayedLegalMoveReward(weight=1),\n",
    "    PeriodicPlacedCardInFoundationReward(weight=2, reward_period=3),\n",
    "    CreatedMovesReward(weight=0.01, num_reserved=num_reserved, foundation_count_dropoff=30)\n",
    "])\n",
    "print([reward.weight for reward in test_reward.rewards_list])\n",
    "env = CellitaireEnv(test_reward, rows=board_rows, cols=board_cols, num_reserved=num_reserved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e261d71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode     0 | score   10.1 | recent avg   10.1 | avg moves  26.0 | avg saved  3.0 | done True *\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(observation, action, prob, val, reward, done)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 50\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn()\n\u001b[0;32m     51\u001b[0m     learn_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     52\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation_\n",
      "Cell \u001b[1;32mIn[5], line 186\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 186\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "batch_size = 3\n",
    "n_epochs = 2\n",
    "alpha = 0.0003\n",
    "num_hidden_layers_actor=2\n",
    "hidden_dim_actor=256\n",
    "#embeddings_in_state_actor=1\n",
    "embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=10\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=(board_rows * board_cols + 7,), \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim\n",
    ")\n",
    "\n",
    "n_games = 100000\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "moves_history = []\n",
    "cards_saved_history = []\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "avg_moves = 0\n",
    "avg_cards_saved = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    observation, reward, done, truncated, info = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while (not done) and (not truncated):\n",
    "        legal_actions = env.get_legal_actions_as_int()\n",
    "        action, prob, val = agent.choose_action(observation, legal_actions)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    moves_history.append(env.num_moves)\n",
    "    cards_saved_history.append(env.game.foundation.total_cards())\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    avg_moves = np.mean(moves_history[-100:])\n",
    "    avg_cards_saved = np.mean(cards_saved_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | done {done} *') \n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | done {done}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e08b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(score_history, dtype=torch.float).view(-1, 100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(moves_history, dtype=torch.float).view(-1, 100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(cards_saved_history, dtype=torch.float).view(-1, 100).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cards_saved_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b04beb-bd28-4c5a-987e-9c8c28b033c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00121022,  0.02562538,  0.01463803, -0.02002225], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d40bf7-3d64-496d-b842-340670553b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode     0 | score   37.0 | avg   37.0 | std   0.00 | max score  37.0 | learning steps     3 | done True *\n",
      "episode   100 | score   10.0 | avg   22.4 | std  11.91 | max score  73.0 | learning steps   228 | done True\n",
      "episode   200 | score   31.0 | avg   21.9 | std   9.94 | max score  73.0 | learning steps   446 | done True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(observation, action, prob, val, reward, done)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_steps \u001b[38;5;241m%\u001b[39m N \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m     agent\u001b[38;5;241m.\u001b[39mlearn()\n\u001b[0;32m     69\u001b[0m     learn_iters \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     70\u001b[0m observation \u001b[38;5;241m=\u001b[39m observation_\n",
      "Cell \u001b[1;32mIn[84], line 186\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 186\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#N = 20\n",
    "#batch_size = 5\n",
    "#n_epochs = 4\n",
    "#alpha = 0.0003\n",
    "#embeddings_in_state_actor=1\n",
    "\n",
    "N = 10\n",
    "batch_size = 3\n",
    "n_epochs = 2\n",
    "alpha = 0.0003\n",
    "num_hidden_layers_actor=3\n",
    "hidden_dim_actor=2048\n",
    "embeddings_in_state_actor=1\n",
    "#embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "'''\n",
    "agent = Agent(\n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor\n",
    ")\n",
    "'''\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim\n",
    ")\n",
    "\n",
    "n_games = 150000\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "max_score = 0\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    (observation, _) = env.reset()\n",
    "    observation[0] += 5.0\n",
    "    observation[0] *= 5.2\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    while not done and not truncated:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_[0] += 5.0\n",
    "        observation_[0] *= 5.2\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    max_score = max(max_score, score)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done} *')\n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done}')\n",
    "    \n",
    "\n",
    "plt.plot(x, score_history)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "603fba1d-212d-4624-985d-82473afbdf4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "612c7d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///P:/repos/cellitaire-rlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from cellitaire==0.0.1) (1.26.2)\n",
      "Building wheels for collected packages: cellitaire\n",
      "  Building editable for cellitaire (pyproject.toml): started\n",
      "  Building editable for cellitaire (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for cellitaire: filename=cellitaire-0.0.1-0.editable-py3-none-any.whl size=1308 sha256=b378391a3c216160d1234ca202eae929e697782b029366f7620acdb700ddbf9d\n",
      "  Stored in directory: C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x9pvpc0w\\wheels\\94\\28\\43\\7b210dfe894f4cf7a94090468ee7dfeadc8e241120461ab4b9\n",
      "Successfully built cellitaire\n",
      "Installing collected packages: cellitaire\n",
      "Successfully installed cellitaire-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\noe\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n",
      "WARNING: You are using pip version 22.0.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Noe\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b132a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (0.28.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [86 lines of output]\n",
      "  \n",
      "  \n",
      "  WARNING, No \"Setup\" File Exists, Running \"buildconfig/config.py\"\n",
      "  Using WINDOWS configuration...\n",
      "  \n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m2\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mexec\u001b[0m\u001b[1;31m(compile('''\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;31m# This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "      ...<31 lines>...\n",
      "      \u001b[1;31mexec(compile(setup_py_code, filename, \"exec\"))\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "      \u001b[1;31m''' % ('C:\\\\Users\\\\Noe\\\\AppData\\\\Local\\\\Temp\\\\pip-install-eoyw7vpr\\\\pygame_967dc80c3fe74667b8820019c1335dad\\\\setup.py',), \"<pip-setuptools-caller>\", \"exec\"))\u001b[0m\n",
      "      \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"<pip-setuptools-caller>\"\u001b[0m, line \u001b[35m34\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\setup.py\"\u001b[0m, line \u001b[35m400\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mbuildconfig.config.main\u001b[0m\u001b[1;31m(AUTO_CONFIG)\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config.py\"\u001b[0m, line \u001b[35m231\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      deps = CFG.main(**kwds, auto_config=auto)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m493\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      return setup_prebuilt_sdl2(prebuilt_dir)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m453\u001b[0m, in \u001b[35msetup_prebuilt_sdl2\u001b[0m\n",
      "      \u001b[31mDEPS.configure\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\config_win.py\"\u001b[0m, line \u001b[35m336\u001b[0m, in \u001b[35mconfigure\u001b[0m\n",
      "      from . import vstools\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\AppData\\Local\\Temp\\pip-install-eoyw7vpr\\pygame_967dc80c3fe74667b8820019c1335dad\\buildconfig\\vstools.py\"\u001b[0m, line \u001b[35m8\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mcompiler.initialize\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\"\u001b[0m, line \u001b[35m400\u001b[0m, in \u001b[35minitialize\u001b[0m\n",
      "      vc_env = query_vcvarsall(VERSION, plat_spec)\n",
      "    File \u001b[35m\"C:\\Users\\Noe\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\setuptools\\_distutils\\msvc9compiler.py\"\u001b[0m, line \u001b[35m280\u001b[0m, in \u001b[35mquery_vcvarsall\u001b[0m\n",
      "      raise DistutilsPlatformError(\"Unable to find vcvarsall.bat\")\n",
      "  \u001b[1;35mdistutils.errors.DistutilsPlatformError\u001b[0m: \u001b[35mUnable to find vcvarsall.bat\u001b[0m\n",
      "  Making dir :prebuilt_downloads:\n",
      "  Downloading... https://www.libsdl.org/release/SDL2-devel-2.0.22-VC.zip efa040633c4faf8b006c0c1e552456ca4e5a3a53\n",
      "  Unzipping :prebuilt_downloads\\SDL2-devel-2.0.22-VC.zip:\n",
      "  Downloading... https://www.libsdl.org/projects/SDL_image/release/SDL2_image-devel-2.0.5-VC.zip 137f86474691f4e12e76e07d58d5920c8d844d5b\n",
      "  Unzipping :prebuilt_downloads\\SDL2_image-devel-2.0.5-VC.zip:\n",
      "  Downloading... https://github.com/libsdl-org/SDL_ttf/releases/download/release-2.20.1/SDL2_ttf-devel-2.20.1-VC.zip 371606aceba450384428fd2852f73d2f6290b136\n",
      "  Unzipping :prebuilt_downloads\\SDL2_ttf-devel-2.20.1-VC.zip:\n",
      "  Downloading... https://github.com/libsdl-org/SDL_mixer/releases/download/release-2.6.2/SDL2_mixer-devel-2.6.2-VC.zip 000e3ea8a50261d46dbd200fb450b93c59ed4482\n",
      "  Unzipping :prebuilt_downloads\\SDL2_mixer-devel-2.6.2-VC.zip:\n",
      "  Downloading... https://github.com/pygame/pygame/releases/download/2.1.3.dev4/prebuilt-x64-pygame-2.1.4-20220319.zip 16b46596744ce9ef80e7e40fa72ddbafef1cf586\n",
      "  Unzipping :prebuilt_downloads\\prebuilt-x64-pygame-2.1.4-20220319.zip:\n",
      "  copying into .\\prebuilt-x64\n",
      "  Path for SDL: prebuilt-x64\\SDL2-2.0.22\n",
      "  ...Library directory for SDL: prebuilt-x64/SDL2-2.0.22/lib/x64\n",
      "  ...Include directory for SDL: prebuilt-x64/SDL2-2.0.22/include\n",
      "  Path for FONT: prebuilt-x64\\SDL2_ttf-2.20.1\n",
      "  ...Library directory for FONT: prebuilt-x64/SDL2_ttf-2.20.1/lib/x64\n",
      "  ...Include directory for FONT: prebuilt-x64/SDL2_ttf-2.20.1/include\n",
      "  Path for IMAGE: prebuilt-x64\\SDL2_image-2.0.5\n",
      "  ...Library directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/lib/x64\n",
      "  ...Include directory for IMAGE: prebuilt-x64/SDL2_image-2.0.5/include\n",
      "  Path for MIXER: prebuilt-x64\\SDL2_mixer-2.6.2\n",
      "  ...Library directory for MIXER: prebuilt-x64/SDL2_mixer-2.6.2/lib/x64\n",
      "  ...Include directory for MIXER: prebuilt-x64/SDL2_mixer-2.6.2/include\n",
      "  Path for PORTMIDI: prebuilt-x64\n",
      "  ...Library directory for PORTMIDI: prebuilt-x64/lib\n",
      "  ...Include directory for PORTMIDI: prebuilt-x64/include\n",
      "  DLL for SDL2: prebuilt-x64/SDL2-2.0.22/lib/x64/SDL2.dll\n",
      "  DLL for SDL2_ttf: prebuilt-x64/SDL2_ttf-2.20.1/lib/x64/SDL2_ttf.dll\n",
      "  DLL for SDL2_image: prebuilt-x64/SDL2_image-2.0.5/lib/x64/SDL2_image.dll\n",
      "  DLL for SDL2_mixer: prebuilt-x64/SDL2_mixer-2.6.2/lib/x64/SDL2_mixer.dll\n",
      "  DLL for portmidi: prebuilt-x64/lib/portmidi.dll\n",
      "  Path for FREETYPE: prebuilt-x64\n",
      "  ...Library directory for FREETYPE: prebuilt-x64/lib\n",
      "  ...Include directory for FREETYPE: prebuilt-x64/include\n",
      "  Path for PNG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  Path for JPEG not found.\n",
      "  ...Found include dir but no library dir in prebuilt-x64.\n",
      "  DLL for freetype: prebuilt-x64/lib/freetype.dll\n",
      "  DLL for png: prebuilt-x64/SDL2_image-2.0.5/lib/x64/libpng16-16.dll\n",
      "  \n",
      "  ---\n",
      "  For help with compilation see:\n",
      "      https://www.pygame.org/wiki/CompileWindows\n",
      "  To contribute to pygame development see:\n",
      "      https://www.pygame.org/contribute.html\n",
      "  ---\n",
      "  \n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (2.2.2)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\noe\\miniconda3\\envs\\cellitaire-rl\\lib\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Collecting pygame==2.1.3 (from gymnasium[classic-control])\n",
      "  Using cached pygame-2.1.3.tar.gz (12.8 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "%pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce4d362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygame\n",
      "  Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.6.1-cp313-cp313-win_amd64.whl (10.6 MB)\n",
      "   ---------------------------------------- 0.0/10.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 7.6/10.6 MB 42.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.6/10.6 MB 37.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b18d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%jupyter` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd2b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
