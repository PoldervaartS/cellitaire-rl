{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb804a91-6ebc-4b2c-a410-b0bb5ca11ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from cellitaire.environment.cellitaire_env import CellitaireEnv\n",
    "from cellitaire.environment.rewards.reward import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16e4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), np.array(self.vals), np.array(self.rewards), np.array(self.dones), batches\n",
    "        \n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.actions.append(action)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48832150-4a0e-4e32-ab19-1b324ff80055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        alpha,\n",
    "        chkpt_dir='tmp/ppo', \n",
    "        num_embeddings=53, \n",
    "        embedding_dim=30, \n",
    "        embeddings_in_state=85,\n",
    "        num_hidden_layers=1,\n",
    "        hidden_dim=256\n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.embeddings_in_state = embeddings_in_state\n",
    "\n",
    "        # Create embedding layer only if embeddings_in_state > 0.\n",
    "        if embeddings_in_state > 0:\n",
    "            self.embedding_layer = nn.Embedding(num_embeddings, embedding_dim).to(self.device)\n",
    "            input_layer_dim = input_dims[0] - embeddings_in_state + (embeddings_in_state * embedding_dim)\n",
    "        else:\n",
    "            self.embedding_layer = None\n",
    "            input_layer_dim = input_dims[0]\n",
    "\n",
    "        print(self.embedding_layer.parameters())\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "        \n",
    "        # First layer with layer normalization and activation.\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_layer_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Append hidden layers with layer normalization between linear and activation.\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.actor.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.actor.append(nn.LayerNorm(hidden_dim))\n",
    "            self.actor.append(nn.ReLU())\n",
    "        \n",
    "        # Final output layer.\n",
    "        self.actor.append(nn.Linear(hidden_dim, n_actions))\n",
    "        #self.actor.append(nn.Softmax(dim=-1))\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.to(self.device)\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def forward(self, state):\n",
    "        if self.embeddings_in_state > 0:\n",
    "            embeddings = self.embedding_layer(state[:, :self.embeddings_in_state].int()).view(state.shape[0], 1, -1)\n",
    "            s = state.view(state.shape[0], 1, -1)[:, :, self.embeddings_in_state:]\n",
    "            \n",
    "            # Concatenate the remaining state features with the flattened embeddings.\n",
    "            new_state = torch.cat((s, embeddings), dim=2)\n",
    "        else:\n",
    "            new_state = state.view(state.shape[0], 1, -1)\n",
    "        \n",
    "        x = new_state\n",
    "        if torch.isnan(x).any():\n",
    "            print(f\"NaN detected after embedding\")\n",
    "        logits = self.actor(new_state)\n",
    "        return logits\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89c790f-8870-4894-ac88-9a9cafbe684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dims, \n",
    "        alpha, \n",
    "        num_hidden_layers=1, \n",
    "        hidden_dim=256, \n",
    "        chkpt_dir='tmp/ppo'\n",
    "    ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(*input_dims, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.critic.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.critic.append(nn.ReLU())\n",
    "\n",
    "        self.critic.append(nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ae2fc7-893a-4c88-a9eb-95d2fbfa027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_actions, \n",
    "        input_dims, \n",
    "        gamma=0.99, \n",
    "        alpha=0.0003, \n",
    "        gae_lambda=0.95,\n",
    "        policy_clip=0.2, \n",
    "        batch_size=64, \n",
    "        n_epochs=10,\n",
    "        num_hidden_layers_actor=1,\n",
    "        hidden_dim_actor=256,\n",
    "        num_hidden_layers_critic=1,\n",
    "        hidden_dim_critic=256,\n",
    "        embeddings_in_state_actor=85,\n",
    "        embedding_dim_actor=30\n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            n_actions, \n",
    "            input_dims, \n",
    "            alpha, \n",
    "            num_hidden_layers=num_hidden_layers_actor, \n",
    "            hidden_dim=hidden_dim_actor,\n",
    "            embeddings_in_state=embeddings_in_state_actor,\n",
    "            embedding_dim=embedding_dim_actor\n",
    "        )\n",
    "        self.critic = CriticNetwork(\n",
    "            input_dims, \n",
    "            alpha,\n",
    "            num_hidden_layers=num_hidden_layers_critic,\n",
    "            hidden_dim=hidden_dim_critic\n",
    "        )\n",
    "        actor_param_count = sum(p.numel() for p in self.actor.parameters())\n",
    "        critic_param_count = sum(p.numel() for p in self.critic.parameters())\n",
    "        print(f'NUM - PARAMS {actor_param_count + critic_param_count}')\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        #print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        #print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "        \n",
    "        dist = self.actor(state)\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "        dist = Categorical(dist)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = torch.squeeze(dist.log_prob(action)).item()\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def choose_action(self, observation, legal_actions):\n",
    "        # Convert observation to tensor and send it to the actor's device.\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (batch, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # Convert legal_actions to a tensor on the same device.\n",
    "        legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "\n",
    "        # Mask logits so that only legal actions remain.\n",
    "        # If logits has shape (n, 1, k), we index into the third dimension.\n",
    "        masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "\n",
    "        # Build a Categorical using the masked logits.\n",
    "        dist = Categorical(logits=masked_logits)\n",
    "\n",
    "        # Get the value estimate from the critic.\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        # This gives a relative index in the filtered (legal) logits.\n",
    "        relative_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map the relative index back to the original action space.\n",
    "        action = legal_actions[relative_index]\n",
    "\n",
    "        # Compute the log probability for the sampled action.\n",
    "        probs = torch.squeeze(dist.log_prob(relative_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "    '''\n",
    "    \n",
    "    def choose_action(self, observation, legal_actions=None):\n",
    "        state = torch.tensor(np.array([observation]), dtype=torch.float).to(self.actor.device)\n",
    "\n",
    "        # Get logits from the actor; assumed shape: (n, 1, k)\n",
    "        logits = self.actor(state)\n",
    "\n",
    "        # If legal_actions is provided, mask the logits to include only those actions.\n",
    "        if legal_actions is not None:\n",
    "            legal_actions = torch.tensor(np.array(legal_actions)).to(self.actor.device)\n",
    "            masked_logits = logits[:, 0, legal_actions]  # shape: (n, len(legal_actions))\n",
    "            dist = Categorical(logits=masked_logits)\n",
    "        else:\n",
    "            # Use all logits if no legal actions mask is provided.\n",
    "            full_logits = logits.squeeze(1)  # shape: (n, k)\n",
    "            dist = Categorical(logits=full_logits)\n",
    "\n",
    "        value = self.critic(state)\n",
    "\n",
    "        # Sample an action index from the distribution.\n",
    "        sampled_index = dist.sample()  # shape: (n,)\n",
    "\n",
    "        # Map back to the original action if a mask was applied.\n",
    "        if legal_actions is not None:\n",
    "            action = legal_actions[sampled_index]\n",
    "        else:\n",
    "            action = sampled_index\n",
    "\n",
    "        # Get the log probability of the sampled action.\n",
    "        log_prob = torch.squeeze(dist.log_prob(sampled_index)).item()\n",
    "\n",
    "        # Squeeze and convert to Python scalars.\n",
    "        action = torch.squeeze(action).item()\n",
    "        value = torch.squeeze(value).item()\n",
    "\n",
    "        return action, log_prob, value\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        device = self.actor.device  # Assuming this is a CUDA device\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Retrieve batch data\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, batches = \\\n",
    "                self.memory.generate_batches()\n",
    "            \n",
    "            # Convert arrays to torch tensors on GPU\n",
    "            rewards = torch.tensor(reward_arr, dtype=torch.float32, device=device)\n",
    "            values = torch.tensor(vals_arr, dtype=torch.float32, device=device)\n",
    "            dones = torch.tensor(dones_arr, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Compute deltas for GAE: delta_t = r_t + gamma * V(t+1) * (1-done) - V(t)\n",
    "            deltas = rewards[:-1] + self.gamma * values[1:] * (1 - dones[:-1]) - values[:-1]\n",
    "            \n",
    "            # Compute advantage vector using the vectorized discounted cumulative sum.\n",
    "            advantage = torch.zeros_like(rewards, device=device)\n",
    "            advantage[:-1] = discount_cumsum(deltas, self.gamma * self.gae_lambda)\n",
    "            \n",
    "            # Loop over minibatches\n",
    "            for batch in batches:\n",
    "                states = torch.tensor(state_arr[batch], dtype=torch.float, device=device)\n",
    "                old_probs = torch.tensor(old_prob_arr[batch], device=device)\n",
    "                actions = torch.tensor(action_arr[batch], device=device)\n",
    "    \n",
    "                # Forward pass through the actor and critic networks\n",
    "                dist = self.actor(states)\n",
    "                dist = Categorical(logits=dist)\n",
    "                critic_value = self.critic(states).squeeze()\n",
    "                \n",
    "                # Calculate probability ratio and losses\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = torch.clamp(prob_ratio, 1 - self.policy_clip, 1 + self.policy_clip) * advantage[batch]\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "    \n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value).pow(2).mean()\n",
    "    \n",
    "                total_loss = actor_loss + 0.5 * critic_loss\n",
    "                \n",
    "                # Update the networks\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1)\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "    \n",
    "        self.memory.clear_memory()             \n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    T = x.size(0)\n",
    "    discount_factors = discount ** torch.arange(T, device=x.device, dtype=x.dtype)\n",
    "    # Clamp discount_factors to avoid zero values\n",
    "    discount_factors = torch.clamp(discount_factors, min=1e-10)\n",
    "    x_discounted = x * discount_factors\n",
    "    reversed_x = torch.flip(x_discounted, dims=[0])\n",
    "    cumsum_reversed = torch.cumsum(reversed_x, dim=0)\n",
    "    discounted_cumsum = torch.flip(cumsum_reversed, dims=[0])\n",
    "    return discounted_cumsum / discount_factors\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c67f3518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3712543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1000000, 0.5, 1]\n"
     ]
    }
   ],
   "source": [
    "board_rows = 7\n",
    "board_cols = 12\n",
    "num_reserved = 6\n",
    "test_reward = CombinedReward([\n",
    "    PlacedCardInFoundationReward(weight=6),\n",
    "    WinReward(),\n",
    "    ConstantReward(weight=0.5),\n",
    "    #PlayedLegalMoveReward(weight=1),\n",
    "    #PeriodicPlacedCardInFoundationReward(weight=4, reward_period=3),\n",
    "    CreatedMovesReward(weight=1, num_reserved=num_reserved, foundation_count_dropoff=30)\n",
    "])\n",
    "print([reward.weight for reward in test_reward.rewards_list])\n",
    "env = CellitaireEnv(test_reward, rows=board_rows, cols=board_cols, num_reserved=num_reserved, max_moves=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e261d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x00000205C52A3BC0>\n",
      "NUM - PARAMS 111912587\n",
      "episode     1 | score  556.5 | recent avg  556.5 | avg moves 451.0 | avg saved 23.0 | learning steps      0 | done True *\n",
      "episode     2 | score  567.0 | recent avg  561.8 | avg moves 438.5 | avg saved 27.0 | learning steps      0 | done True *\n",
      "episode   102 | score   68.5 | recent avg  165.7 | avg moves 128.0 | avg saved  8.7 | learning steps      6 | done True\n",
      "episode   202 | score   23.0 | recent avg  187.2 | avg moves 147.4 | avg saved  9.2 | learning steps     14 | done True\n",
      "episode   302 | score   50.0 | recent avg  182.8 | avg moves 142.5 | avg saved  9.3 | learning steps     21 | done True\n",
      "episode   402 | score    4.0 | recent avg  118.3 | avg moves  92.4 | avg saved  6.2 | learning steps     25 | done True\n",
      "episode   502 | score   16.0 | recent avg  163.7 | avg moves 126.5 | avg saved  8.6 | learning steps     32 | done True\n",
      "episode   602 | score    5.0 | recent avg  140.3 | avg moves 112.6 | avg saved  6.6 | learning steps     37 | done True\n",
      "episode   702 | score   49.5 | recent avg  189.5 | avg moves 147.9 | avg saved  9.8 | learning steps     45 | done True\n",
      "episode   802 | score   28.5 | recent avg  133.4 | avg moves 103.5 | avg saved  7.2 | learning steps     50 | done True\n",
      "episode   902 | score   56.0 | recent avg  182.7 | avg moves 142.7 | avg saved  9.4 | learning steps     57 | done True\n",
      "episode  1002 | score  168.0 | recent avg  154.6 | avg moves 122.5 | avg saved  7.8 | learning steps     63 | done True\n",
      "episode  1102 | score    5.0 | recent avg  119.4 | avg moves  94.4 | avg saved  6.0 | learning steps     68 | done True\n",
      "episode  1202 | score   39.0 | recent avg  112.0 | avg moves  88.9 | avg saved  5.6 | learning steps     72 | done True\n",
      "episode  1302 | score  148.0 | recent avg  133.4 | avg moves 105.3 | avg saved  6.9 | learning steps     78 | done True\n",
      "episode  1402 | score   21.0 | recent avg   94.4 | avg moves  78.0 | avg saved  4.4 | learning steps     82 | done True\n",
      "episode  1502 | score    9.5 | recent avg  161.1 | avg moves 132.6 | avg saved  7.3 | learning steps     88 | done True\n",
      "episode  1602 | score  701.5 | recent avg  166.9 | avg moves 138.2 | avg saved  7.3 | learning steps     95 | done False\n",
      "episode  1702 | score  620.5 | recent avg  163.1 | avg moves 132.3 | avg saved  8.0 | learning steps    102 | done True\n",
      "episode  1802 | score   61.0 | recent avg  128.7 | avg moves 103.9 | avg saved  6.1 | learning steps    107 | done True\n",
      "episode  1902 | score   16.0 | recent avg  141.2 | avg moves 115.1 | avg saved  6.8 | learning steps    113 | done True\n",
      "episode  2002 | score  574.5 | recent avg  143.0 | avg moves 115.8 | avg saved  6.8 | learning steps    118 | done True\n",
      "episode  2102 | score  685.5 | recent avg  182.8 | avg moves 149.8 | avg saved  8.4 | learning steps    126 | done True\n",
      "episode  2202 | score    3.0 | recent avg  128.0 | avg moves 107.4 | avg saved  5.8 | learning steps    131 | done True\n",
      "episode  2302 | score  720.5 | recent avg  189.7 | avg moves 155.5 | avg saved  9.2 | learning steps    139 | done False\n",
      "episode  2402 | score  660.5 | recent avg  179.7 | avg moves 150.3 | avg saved  7.9 | learning steps    147 | done False\n",
      "episode  2502 | score  690.5 | recent avg  125.0 | avg moves 101.3 | avg saved  6.0 | learning steps    152 | done False\n",
      "episode  2602 | score    8.5 | recent avg  138.1 | avg moves 115.1 | avg saved  6.1 | learning steps    157 | done True\n",
      "episode  2702 | score  720.5 | recent avg  101.5 | avg moves  83.8 | avg saved  4.8 | learning steps    162 | done False\n",
      "episode  2802 | score   35.0 | recent avg  172.3 | avg moves 140.8 | avg saved  8.5 | learning steps    169 | done True\n",
      "episode  2902 | score  105.5 | recent avg  139.1 | avg moves 114.1 | avg saved  6.8 | learning steps    174 | done True\n",
      "episode  3002 | score    9.0 | recent avg  172.2 | avg moves 139.7 | avg saved  8.4 | learning steps    181 | done True\n",
      "episode  3102 | score  665.0 | recent avg  152.2 | avg moves 127.0 | avg saved  6.9 | learning steps    188 | done True\n",
      "episode  3202 | score  333.5 | recent avg  145.0 | avg moves 119.8 | avg saved  6.8 | learning steps    194 | done True\n",
      "episode  3302 | score   32.0 | recent avg   84.8 | avg moves  72.5 | avg saved  3.7 | learning steps    197 | done True\n",
      "episode  3402 | score    9.5 | recent avg  127.1 | avg moves 101.2 | avg saved  6.4 | learning steps    202 | done True\n",
      "episode  3502 | score   20.0 | recent avg  102.0 | avg moves  86.6 | avg saved  4.7 | learning steps    207 | done True\n",
      "episode  3602 | score   38.0 | recent avg  103.1 | avg moves  88.0 | avg saved  4.5 | learning steps    211 | done True\n",
      "episode  3702 | score    9.0 | recent avg  111.1 | avg moves  92.0 | avg saved  5.4 | learning steps    216 | done True\n",
      "episode  3802 | score  641.0 | recent avg  146.4 | avg moves 121.9 | avg saved  6.9 | learning steps    222 | done True\n",
      "episode  3902 | score    8.5 | recent avg  136.3 | avg moves 115.5 | avg saved  6.4 | learning steps    228 | done True\n",
      "episode  4002 | score  495.0 | recent avg  146.7 | avg moves 123.8 | avg saved  6.4 | learning steps    234 | done True\n",
      "episode  4102 | score   15.0 | recent avg  123.0 | avg moves 101.6 | avg saved  6.0 | learning steps    239 | done True\n",
      "episode  4202 | score   11.0 | recent avg  130.2 | avg moves 109.4 | avg saved  5.8 | learning steps    244 | done True\n",
      "episode  4302 | score    3.0 | recent avg   90.2 | avg moves  75.9 | avg saved  4.2 | learning steps    248 | done True\n",
      "episode  4402 | score    9.0 | recent avg  116.2 | avg moves  97.7 | avg saved  5.3 | learning steps    253 | done True\n",
      "episode  4502 | score    3.0 | recent avg  124.3 | avg moves 102.1 | avg saved  6.1 | learning steps    258 | done True\n",
      "episode  4602 | score   12.0 | recent avg  118.1 | avg moves  98.6 | avg saved  5.4 | learning steps    263 | done True\n",
      "episode  4702 | score  140.0 | recent avg  114.1 | avg moves  94.8 | avg saved  5.5 | learning steps    268 | done True\n",
      "episode  4802 | score   14.0 | recent avg   88.3 | avg moves  74.2 | avg saved  4.1 | learning steps    271 | done True\n",
      "episode  4902 | score   13.0 | recent avg  108.0 | avg moves  90.7 | avg saved  5.0 | learning steps    276 | done True\n",
      "episode  5002 | score   37.0 | recent avg  158.0 | avg moves 133.6 | avg saved  7.2 | learning steps    283 | done True\n",
      "episode  5102 | score  713.5 | recent avg  125.2 | avg moves 105.1 | avg saved  5.6 | learning steps    288 | done False\n",
      "episode  5202 | score   36.0 | recent avg  132.3 | avg moves 109.9 | avg saved  6.2 | learning steps    293 | done True\n",
      "episode  5302 | score   20.5 | recent avg  163.7 | avg moves 134.9 | avg saved  7.5 | learning steps    300 | done True\n",
      "episode  5402 | score   31.0 | recent avg  162.5 | avg moves 137.0 | avg saved  7.2 | learning steps    307 | done True\n",
      "episode  5502 | score  609.5 | recent avg   99.1 | avg moves  82.9 | avg saved  4.6 | learning steps    311 | done True\n",
      "episode  5602 | score  102.0 | recent avg  113.1 | avg moves  93.3 | avg saved  5.4 | learning steps    316 | done True\n",
      "episode  5702 | score   41.5 | recent avg  122.7 | avg moves 103.9 | avg saved  5.5 | learning steps    321 | done True\n",
      "episode  5802 | score   34.0 | recent avg  102.7 | avg moves  86.0 | avg saved  4.7 | learning steps    325 | done True\n",
      "episode  5902 | score    8.5 | recent avg  139.0 | avg moves 119.1 | avg saved  5.9 | learning steps    331 | done True\n",
      "episode  6002 | score   12.0 | recent avg  120.8 | avg moves 103.0 | avg saved  5.3 | learning steps    336 | done True\n",
      "episode  6102 | score  754.5 | recent avg  118.4 | avg moves  98.3 | avg saved  5.3 | learning steps    341 | done False\n",
      "episode  6202 | score  690.5 | recent avg  121.3 | avg moves 102.0 | avg saved  5.5 | learning steps    346 | done False\n",
      "episode  6302 | score   18.0 | recent avg  162.9 | avg moves 134.2 | avg saved  7.9 | learning steps    353 | done True\n",
      "episode  6402 | score  638.0 | recent avg   93.9 | avg moves  77.9 | avg saved  4.6 | learning steps    357 | done True\n",
      "episode  6502 | score   12.0 | recent avg  109.9 | avg moves  92.1 | avg saved  4.8 | learning steps    362 | done True\n",
      "episode  6602 | score    3.0 | recent avg  104.4 | avg moves  88.3 | avg saved  4.6 | learning steps    366 | done True\n",
      "episode  6702 | score   74.5 | recent avg  131.8 | avg moves 111.0 | avg saved  6.1 | learning steps    372 | done True\n",
      "episode  6802 | score   62.0 | recent avg  137.1 | avg moves 112.2 | avg saved  6.9 | learning steps    377 | done True\n",
      "episode  6902 | score  114.0 | recent avg  164.1 | avg moves 135.0 | avg saved  7.7 | learning steps    384 | done True\n",
      "episode  7002 | score   32.5 | recent avg  134.3 | avg moves 109.4 | avg saved  6.4 | learning steps    389 | done True\n",
      "episode  7102 | score   20.0 | recent avg  116.7 | avg moves  97.9 | avg saved  5.1 | learning steps    394 | done True\n",
      "episode  7202 | score   25.0 | recent avg   84.5 | avg moves  72.2 | avg saved  3.7 | learning steps    398 | done True\n",
      "episode  7302 | score  553.0 | recent avg   93.3 | avg moves  78.4 | avg saved  4.3 | learning steps    402 | done True\n",
      "episode  7402 | score   14.0 | recent avg   96.5 | avg moves  80.0 | avg saved  4.7 | learning steps    406 | done True\n",
      "episode  7502 | score   13.0 | recent avg  128.6 | avg moves 106.7 | avg saved  6.2 | learning steps    411 | done True\n",
      "episode  7602 | score   22.0 | recent avg  114.9 | avg moves  95.8 | avg saved  5.5 | learning steps    416 | done True\n",
      "episode  7702 | score   18.0 | recent avg  137.9 | avg moves 113.5 | avg saved  6.3 | learning steps    422 | done True\n",
      "episode  7802 | score  729.0 | recent avg  121.6 | avg moves 100.5 | avg saved  5.7 | learning steps    427 | done True\n",
      "episode  7902 | score   17.0 | recent avg  119.0 | avg moves  98.7 | avg saved  5.4 | learning steps    432 | done True\n",
      "episode  8002 | score   64.0 | recent avg   87.9 | avg moves  73.8 | avg saved  4.1 | learning steps    435 | done True\n",
      "episode  8102 | score   48.5 | recent avg  125.2 | avg moves 105.5 | avg saved  5.4 | learning steps    441 | done True\n",
      "episode  8202 | score   11.0 | recent avg  139.3 | avg moves 116.0 | avg saved  6.3 | learning steps    446 | done True\n",
      "episode  8302 | score   50.0 | recent avg  123.9 | avg moves 104.5 | avg saved  5.6 | learning steps    452 | done True\n",
      "episode  8402 | score  618.5 | recent avg  115.7 | avg moves  97.4 | avg saved  5.3 | learning steps    457 | done True\n",
      "episode  8502 | score    7.0 | recent avg  150.9 | avg moves 126.2 | avg saved  6.9 | learning steps    463 | done True\n",
      "episode  8602 | score    6.0 | recent avg  111.4 | avg moves  91.3 | avg saved  5.5 | learning steps    467 | done True\n",
      "episode  8702 | score   21.0 | recent avg  144.7 | avg moves 119.1 | avg saved  6.9 | learning steps    473 | done True\n",
      "episode  8802 | score   15.0 | recent avg  162.2 | avg moves 133.3 | avg saved  7.9 | learning steps    480 | done True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m done) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m truncated):\n\u001b[0;32m     52\u001b[0m     legal_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_legal_actions_as_int()\n\u001b[1;32m---> 53\u001b[0m     action, prob, val \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(observation, legal_actions)\n\u001b[0;32m     54\u001b[0m     observation_, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     55\u001b[0m     n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[14], line 141\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, observation, legal_actions)\u001b[0m\n\u001b[0;32m    138\u001b[0m     action \u001b[38;5;241m=\u001b[39m sampled_index\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# Get the log probability of the sampled action.\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(dist\u001b[38;5;241m.\u001b[39mlog_prob(sampled_index))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Squeeze and convert to Python scalars.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqueeze(action)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\cellitaire-rl\\Lib\\site-packages\\torch\\distributions\\categorical.py:140\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[1;32m--> 140\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    141\u001b[0m value, log_pmf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits)\n\u001b[0;32m    142\u001b[0m value \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "N = 2000\n",
    "batch_size = 2000\n",
    "n_epochs = 1\n",
    "alpha = 1e-6\n",
    "num_hidden_layers_actor=2\n",
    "hidden_dim_actor=4096\n",
    "num_hidden_layers_critic=4\n",
    "hidden_dim_critic=4096\n",
    "#embeddings_in_state_actor=1\n",
    "embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "env.reset()\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=(board_rows * board_cols + 7,), \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim,\n",
    "    num_hidden_layers_critic=num_hidden_layers_critic,\n",
    "    hidden_dim_critic=hidden_dim_critic,\n",
    ")\n",
    "\n",
    "n_games = 1\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "moves_history = []\n",
    "cards_saved_history = []\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "avg_moves = 0\n",
    "avg_cards_saved = 0\n",
    "n_steps = 0\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    observation, reward, done, truncated, info = env.reset()\n",
    "    observation = env.get_state()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while (not done) and (not truncated):\n",
    "        legal_actions = env.get_legal_actions_as_int()\n",
    "        action, prob, val = agent.choose_action(observation, legal_actions)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    moves_history.append(env.num_moves)\n",
    "    cards_saved_history.append(env.game.foundation.total_cards())\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    avg_moves = np.mean(moves_history[-100:])\n",
    "    avg_cards_saved = np.mean(cards_saved_history[-100:])\n",
    "    i += 1\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done} *') \n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | recent avg {avg_score:>6.1f} | avg moves {avg_moves:>5.1f} | avg saved {avg_cards_saved:>4.1f} | learning steps {learn_iters:6} | done {done}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e08b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(score_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b28911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(moves_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(torch.tensor(cards_saved_history, dtype=torch.float).view(-1, 2000).mean(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d0b2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(cards_saved_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b04beb-bd28-4c5a-987e-9c8c28b033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset()\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d40bf7-3d64-496d-b842-340670553b26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#N = 20\n",
    "#batch_size = 5\n",
    "#n_epochs = 4\n",
    "#alpha = 0.0003\n",
    "#embeddings_in_state_actor=1\n",
    "\n",
    "N = 10\n",
    "batch_size = 3\n",
    "n_epochs = 2\n",
    "alpha = 0.0003\n",
    "num_hidden_layers_actor=3\n",
    "hidden_dim_actor=2048\n",
    "embeddings_in_state_actor=1\n",
    "#embeddings_in_state_actor = board_rows * board_cols + 1\n",
    "embedding_dim=30\n",
    "\n",
    "'''\n",
    "agent = Agent(\n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor\n",
    ")\n",
    "'''\n",
    "\n",
    "agent = Agent(\n",
    "    \n",
    "    n_actions=env.action_space.n, \n",
    "    input_dims=env.observation_space.shape, \n",
    "    batch_size=batch_size, \n",
    "    alpha=alpha, \n",
    "    n_epochs=n_epochs,\n",
    "    num_hidden_layers_actor=num_hidden_layers_actor,\n",
    "    hidden_dim_actor=hidden_dim_actor,\n",
    "    embeddings_in_state_actor=embeddings_in_state_actor,\n",
    "    embedding_dim_actor=embedding_dim\n",
    ")\n",
    "\n",
    "n_games = 150000\n",
    "\n",
    "best_score = -1000\n",
    "score_history = []\n",
    "max_score = 0\n",
    "episodes_without_best = 0\n",
    "\n",
    "learn_iters = 0\n",
    "avg_score = 0\n",
    "n_steps = 0\n",
    "\n",
    "for i in range(n_games):\n",
    "    (observation, _) = env.reset()\n",
    "    observation[0] += 5.0\n",
    "    observation[0] *= 5.2\n",
    "    done = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "    while not done and not truncated:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, truncated, info = env.step(action)\n",
    "        observation_[0] += 5.0\n",
    "        observation_[0] *= 5.2\n",
    "        n_steps += 1\n",
    "        score += reward\n",
    "        agent.remember(observation, action, prob, val, reward, done)\n",
    "        if n_steps % N == 0:\n",
    "            agent.learn()\n",
    "            learn_iters += 1\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    max_score = max(max_score, score)\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        #agent.save_models()\n",
    "        episodes_without_best = 0\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done} *')\n",
    "    else:\n",
    "        episodes_without_best += 1\n",
    "        \n",
    "    if episodes_without_best % 100 == 0 and episodes_without_best > 0:\n",
    "        recent_std = np.std(score_history[-100:])\n",
    "        print(f'episode {i:>5} | score {score:>6.1f} | avg {avg_score:>6.1f} | std {recent_std:>6.2f} | max score {max_score:>5.1f} | learning steps {learn_iters:>5} | done {done}')\n",
    "    \n",
    "\n",
    "plt.plot(x, score_history)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fba1d-212d-4624-985d-82473afbdf4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612c7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ../."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b132a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce4d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b18d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd2b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
